[{"content":"Introduction Welcome to the part 4 of the homelab series! In the previous parts, we built a server, deployed a suite of services, and configured our network. Now, it\u0026rsquo;s time to make it resilient and self-maintaining. A homelab isn\u0026rsquo;t just about setting things up; it\u0026rsquo;s about keeping them running reliably.\nThis guide will show you how to set up the three pillars of modern IT operations: Automated Backups, Automated Updates, and Proactive Alerting. By the end, you\u0026rsquo;ll have a homelab that runs itself, ensures your data is safe, stays up-to-date, and notifies you when something goes wrong.\nChapter 1: The Automated Backup Strategy (at 3 AM) A solid backup strategy is non-negotiable. I implemented a robust system inspired by the \u0026ldquo;3-2-1\u0026rdquo; rule, focusing on redundancy and an off-site copy. My strategy involves maintaining two copies of my data in two separate locations: one local backup on the server itself for fast recovery, and one automated, off-site backup to Google Drive to protect against a local disaster like a fire or hardware failure.\nThis script runs at 3 AM, creates a local backup, uploads it, and then notifies Discord.\nStep 1: Configure rclone for Google Drive First, you need a tool to communicate with Google Drive. We\u0026rsquo;ll use rclone.\nInstall rclone on your Debian server: sudo -v ; curl [https://rclone.org/install.sh](https://rclone.org/install.sh) | sudo bash Run the interactive setup: rclone config Follow the Prompts: n (New remote) * name\u0026gt;: gdrive (You can name it anything) storage\u0026gt;: Find and select drive (Google Drive). client_id\u0026gt; \u0026amp; client_secret\u0026gt;: Press Enter for both to leave blank. scope\u0026gt;: Choose 1 (Full access). Use auto config? y/n\u0026gt;: This is a critical step. Since we are on a headless server, type n and press Enter. Authorize Headless: rclone will give you a command to run on a machine with a web browser (like your main computer). On your main computer (where you have rclone installed), run the rclone authorize \u0026quot;drive\u0026quot; \u0026quot;...\u0026quot; command. This will open your browser, ask you to log in to Google, and grant permission. Your main computer\u0026rsquo;s terminal will then output a block of text (your config_token). Paste Token: Copy the token from your main computer and paste it back into your server\u0026rsquo;s rclone prompt. Finish the prompts, and your connection is complete. Step 2: Create the Backup Script Next, create a shell script to perform the backup.\nCreate the file and make it executable:\nnano ~/backup.sh chmod +x ~/backup.sh Paste in the following script. You must edit the first 7 variables to match your setup.\n#!/bin/bash # --- Configuration --- SOURCE_DIR=\u0026#34;/path/to/your/docker\u0026#34; # \u0026lt;-- Change to your Docker projects directory BACKUP_DIR=\u0026#34;/path/to/your/backups\u0026#34; # \u0026lt;-- Change to your backups folder FILENAME=\u0026#34;homelab-backup-$(date +%Y-%m-%d).tar.gz\u0026#34; LOCAL_RETENTION_DAYS=3 CLOUD_RETENTION_DAYS=3 RCLONE_REMOTE=\u0026#34;gdrive\u0026#34; # \u0026lt;-- Must match your rclone remote name RCLONE_DEST=\u0026#34;Homelab Backups\u0026#34; # \u0026lt;-- Folder name in Google Drive # --- \u0026#34;https://discordapp.com/api/webhooks/141949178941/6Tx6f1yjf26LztQ\u0026#34; --- DISCORD_WEBHOOK_URL=\u0026#34;YOUR_DISCORD_WEBHOOK_URL\u0026#34; # --- Notification Function --- send_notification() { MESSAGE=$1 curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#34;{\\\u0026#34;content\\\u0026#34;: \\\u0026#34;$MESSAGE\\\u0026#34;}\u0026#34; \u0026#34;$DISCORD_WEBHOOK_URL\u0026#34; } # --- Script Logic --- echo \u0026#34;--- Starting Homelab Backup: $(date) ---\u0026#34; send_notification \u0026#34;✅ Starting Homelab Backup...\u0026#34; # 1. Create local backup echo \u0026#34;Creating local backup...\u0026#34; tar -czf \u0026#34;${BACKUP_DIR}/${FILENAME}\u0026#34; -C \u0026#34;${SOURCE_DIR}\u0026#34; . echo \u0026#34;Local backup created at ${BACKUP_DIR}/${FILENAME}\u0026#34; # 2. Upload to Google Drive echo \u0026#34;Uploading backup to ${RCLONE_REMOTE}...\u0026#34; rclone copy \u0026#34;${BACKUP_DIR}/${FILENAME}\u0026#34; \u0026#34;${RCLONE_REMOTE}:${RCLONE_DEST}\u0026#34; echo \u0026#34;Upload complete.\u0026#34; # 3. Clean up local backups echo \u0026#34;Cleaning up local backups older than ${LOCAL_RETENTION_DAYS} days...\u0026#34; find \u0026#34;${BACKUP_DIR}\u0026#34; -type f -name \u0026#34;*.tar.gz\u0026#34; -mtime +${LOCAL_RETENTION_DAYS} -delete echo \u0026#34;Local cleanup complete.\u0026#34; # 4. Clean up cloud backups echo \u0026#34;Cleaning up cloud backups older than ${CLOUD_RETENTION_DAYS} days...\u0026#34; rclone delete \u0026#34;${RCLONE_REMOTE}:${RCLONE_DEST}\u0026#34; --min-age ${CLOUD_RETENTION_DAYS}d echo \u0026#34;Cloud cleanup complete.\u0026#34; echo \u0026#34;Backup process finished.\u0026#34; send_notification \u0026#34;🎉 Homelab backup and cloud upload completed successfully!\u0026#34; Step 3: Automate with Cron To run this script automatically, you must add it to the root user\u0026rsquo;s crontab. This is critical for giving the script permission to read all Docker files.\nOpen the root crontab editor: sudo crontab -e Add the following line to schedule the backup for 3:00 AM every morning: 0 3 * * * /path/to/your/backup.sh You will now get a fresh, onsite and off-site backup every night and a Discord message when it\u0026rsquo;s done. Chapter 2: Automated Updates with Watchtower (at 6 AM) Manually updating every Docker container is tedious. We can automate this by deploying Watchtower.\nStep 1: The Docker Compose File Create a docker-compose.yml for Watchtower. This configuration schedules it to run once a day at 6:00 AM, clean up old images, and send a Discord notification only if it finds an update.\nmkdir -p ~/docker/watchtower\ncd ~/docker/watchtower\nnano docker-compose.yml\nPaste in this configuration:\nservices: watchtower: image: containrrr/watchtower container_name: watchtower restart: unless-stopped volumes: - /var/run/docker.sock:/var/run/docker.sock environment: # Timezone setting TZ: America/Chicago # Discord notification settings WATCHTOWER_NOTIFICATIONS: shoutrrr WATCHTOWER_NOTIFICATION_URL: \u0026#34;discord://YOUR_DISCORD_WEBHOOK_ID_URL\u0026gt; # Notification settings WATCHTOWER_NOTIFICATIONS_LEVEL: info WATCHTOWER_NOTIFICATION_REPORT: \u0026#34;true\u0026#34; WATCHTOWER_NOTIFICATIONS_HOSTNAME: Homelab-Laptop # Update settings WATCHTOWER_CLEANUP: \u0026#34;true\u0026#34; WATCHTOWER_INCLUDE_STOPPED: \u0026#34;false\u0026#34; WATCHTOWER_INCLUDE_RESTARTING: \u0026#34;true\u0026#34; WATCHTOWER_SCHEDULE: \u0026#34;0 0 6 * * *\u0026#34; Note: The WATCHTOWER_NOTIFICATION_URL uses a special shoutrrr format for Discord, which looks like discord://token@webhook-id.\nNow, every morning at 6:00 AM, Watchtower will scan all running containers and update any that have a new image available.\nChapter 3: Proactive Alerting (24/7) The final piece of automation is proactive alerting. This setup ensures you are immediately notified via Discord if something goes wrong.\nStep 1: The Alerting Pipeline The pipeline we\u0026rsquo;ll build is: Prometheus (detects problems) -\u0026gt; Alertmanager (groups and routes alerts) -\u0026gt; Discord (notifies you).\nStep 2: Deploy Alertmanager First, deploy Alertmanager. It must be on the same npm_default network as Prometheus.\nmkdir -p ~/docker/alertmanager\ncd ~/docker/alertmanager\nCreate the alertmanager.yml configuration file:\nnano alertmanager.yml Paste in this configuration. It uses advanced routing to send critical alerts every 2 hours and warning alerts every 12 hours.\nglobal: resolve_timeout: 5m route: group_by: [\u0026#34;alertname\u0026#34;, \u0026#34;severity\u0026#34;] group_wait: 30s group_interval: 10m repeat_interbal: 12h receiver: \u0026#34;discord-notifications\u0026#34; routes: - receiver: \u0026#34;discord-notifications\u0026#34; matchers: - severity=\u0026#34;critical\u0026#34; repeat_interval: 2h - receiver: \u0026#34;discord-notifications\u0026#34; matchers: - severity=\u0026#34;warning\u0026#34; repeat_interval: 12h receivers: - name: \u0026#34;discord-notifications\u0026#34; discord_configs: - webhook_url: \u0026#34;YOUR_DISCORD_WEBHOOK_URL\u0026#34; send_resolved: true Now create the docker-compose.yml for Alertmanager:\nnano docker-compose.yml Paste in the following:\nservices: alertmanager: image: prom/alertmanager:latest container_name: alertmanager restart: unless-stopped volumes: - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml networks: - npm_default networks: npm_default: external: true Launch it: docker compose up -d\nStep 3: Configure Prometheus Finally, tell Prometheus to send alerts to Alertmanager and load your rules.\nCreate your rules file, ~/docker/monitoring/alert_rules.yml, with rules for \u0026ldquo;Instance Down,\u0026rdquo; \u0026ldquo;High CPU,\u0026rdquo; \u0026ldquo;Low Disk Space,\u0026rdquo; etc.\ncd ~/docker/monitoring nano alert_rules.yml Add the alert_rules.yml as a volume in your ~/docker/monitoring/docker-compose.yml.\nvolumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - ./alert_rules.yml:/etc/prometheus/alert_rules.yml - prometheus_data:/prometheus Add the alerting and rule_files blocks to your ~/docker/monitoring/prometheus.yml:\ngroups: -name: Critical System Alerts interval: 30s rules: - alert: InstanceDown expr: up == 0 for: 2m labels: severity: critical annotations: summary: \u0026#34;🔴 Instance {{ $labels.instance }} is DOWN\u0026#34; description: \u0026#34;Service {{ $labels.job }} has been unreachable for 2 minutes.\u0026#34; - alert: LaptopOnBattery expr: node_power_supply_online == 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;🔋 Server running on BATTERY\u0026#34; description: \u0026#34;Homelab has been unplugged for 5 minutes. Check power connection!\u0026#34; - alert: LowBatteryLevel expr: node_power_supply_capacity \u0026lt; 20 and node_power_supply_online == 0 for: 1m labels: severity: critical annotations: summary: \u0026#34;⚠️ CRITICAL: Battery at {{ $value }}%\u0026#34; description: \u0026#34;Battery below 20%. Server may shut down soon!\u0026#34; - alert: DiskAlmostFull expr: (node_filesystem_avail_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;} / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;}) * 100 \u0026lt; 10 for: 5m labels: severity: critical annotations: summary: \u0026#34;💾 Disk space critically low: {{ $value | humanize }}% remaining\u0026#34; description: \u0026#34;Root filesystem has less than 10% free space.\u0026#34; - alert: OutOfMemory expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 \u0026lt; 5 for: 2m labels: severity: critical annotations: summary: \u0026#34;🧠 Memory critically low: {{ $value | humanize }}% available\u0026#34; description: \u0026#34;Less than 5% memory available. System may become unresponsive.\u0026#34; - alert: CriticalCpuTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;coretemp\u0026#34;} \u0026gt; 95 for: 2m labels: severity: critical annotations: summary: \u0026#34;🔥 CRITICAL CPU Temperature: {{ $value }}°C\u0026#34; description: \u0026#34;CPU temperature exceeds 95°C. Thermal throttling or shutdown imminent!\u0026#34; - name: Warning System Alerts interval: 1m rules: - alert: HighCpuUsage expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[5m])) * 100) \u0026gt; 80 for: 5m labels: severity: warning annotations: summary: \u0026#34;⚡ High CPU usage: {{ $value | humanize }}%\u0026#34; description: \u0026#34;CPU usage above 80% for 5 minutes on {{ $labels.instance }}\u0026#34; - alert: HighSystemLoad expr: node_load5 / on(instance) count(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}) by (instance) \u0026gt; 1.5 for: 10m labels: severity: warning annotations: summary: \u0026#34;📊 High system load: {{ $value | humanize }}\u0026#34; description: \u0026#34;5-minute load average is 1.5x CPU cores for 10 minutes.\u0026#34; - alert: HighMemoryUsage expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 \u0026lt; 20 for: 5m labels: severity: warning annotations: summary: \u0026#34;🧠 High memory usage: {{ $value | humanize }}% available\u0026#34; description: \u0026#34;Less than 20% memory available.\u0026#34; - alert: HighCpuTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;coretemp\u0026#34;} \u0026gt; 85 for: 5m labels: severity: warning annotations: summary: \u0026#34;🌡️ High CPU temperature: {{ $value }}°C\u0026#34; description: \u0026#34;CPU temperature above 85°C. Consider improving cooling.\u0026#34; - alert: HighNvmeTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;nvme\u0026#34;} \u0026gt; 65 for: 10m labels: severity: warning annotations: summary: \u0026#34;💿 High NVMe temperature: {{ $value }}°C\u0026#34; description: \u0026#34;NVMe drive temperature above 65°C for 10 minutes.\u0026#34; - alert: DiskSpaceLow expr: (node_filesystem_avail_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;} / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;}) * 100 \u0026lt; 20 for: 10m labels: severity: warning annotations: summary: \u0026#34;💾 Disk space low: {{ $value | humanize }}% remaining\u0026#34; description: \u0026#34;Root filesystem has less than 20% free space.\u0026#34; - alert: HighSwapUsage expr: ((node_memory_SwapTotal_bytes - node_memory_SwapFree_bytes) / node_memory_SwapTotal_bytes * 100) \u0026gt; 50 for: 10m labels: severity: warning annotations: summary: \u0026#34;💱 High swap usage: {{ $value | humanize }}%\u0026#34; description: \u0026#34;Swap usage above 50%. System may be memory-constrained.\u0026#34; # Monitor your USB-C hub ethernet adapter (enx00) - alert: EthernetInterfaceDown expr: node_network_up{device=\u0026#34;enx00\u0026#34;} == 0 for: 2m labels: severity: warning annotations: summary: \u0026#34;🌐 USB-C Ethernet adapter is DISCONNECTED\u0026#34; description: \u0026#34;Your USB-C hub ethernet connection (enx00) is down. Check cable or hub.\u0026#34; - alert: HighNetworkErrors expr: rate(node_network_receive_errs_total{device=\u0026#34;enx00\u0026#34;}[5m]) \u0026gt; 10 or rate(node_network_transmit_errs_total{device=\u0026#34;enx00\u0026#34;}[5m]) \u0026gt; 10 for: 5m labels: severity: warning annotations: summary: \u0026#34;🌐 High network errors on USB-C ethernet\u0026#34; description: \u0026#34;Your ethernet adapter is experiencing high error rate. Check cable quality.\u0026#34; - name: Docker Container Alerts interval: 1m rules: # Simplified alert - just checks if container exporter is working - alert: ContainerMonitoringDown expr: absent(container_last_seen) for: 2m labels: severity: warning annotations: summary: \u0026#34;🐳 Container monitoring is down\u0026#34; description: \u0026#34;cAdvisor or container metrics are not available. Check if containers are being monitored.\u0026#34; - alert: ContainerRestarting expr: rate(container_start_time_seconds[5m]) \u0026gt; 0.01 for: 2m labels: severity: warning annotations: summary: \u0026#34;🐳 Container {{ $labels.name }} is restarting\u0026#34; description: \u0026#34;Container {{ $labels.name }} has restarted recently.\u0026#34; - alert: ContainerHighCpu expr: rate(container_cpu_usage_seconds_total{name!~\u0026#34;.*POD.*\u0026#34;,name!=\u0026#34;\u0026#34;}[5m]) * 100 \u0026gt; 80 for: 10m labels: severity: warning annotations: summary: \u0026#34;🐳 Container {{ $labels.name }} high CPU: {{ $value | humanize }}%\u0026#34; description: \u0026#34;Container CPU usage above 80% for 10 minutes.\u0026#34; Restart Prometheus to apply the changes:\ncd ~/docker/monitoring docker compose up -d --force-recreate prometheus Now, if any service fails or your server\u0026rsquo;s resources run low, you will get an instant notification in Discord.\nStep 3: The Critical Firewall Fix You may find your alerts are not sending. This is often due to a conflict between Docker and ufw.\nOpen the main ufw configuration file: sudo nano /etc/default/ufw Change DEFAULT_FORWARD_POLICY=\u0026quot;DROP\u0026quot; to DEFAULT_FORWARD_POLICY=\u0026quot;ACCEPT\u0026quot;. Reload the firewall: sudo ufw reload Restart your containers that need internet access: docker compose restart Now, if any service fails or your server\u0026rsquo;s resources run low, you will get an instant notification in Discord.\nConclusion Our homelab has now truly come to life. It\u0026rsquo;s no longer just a collection of services but a resilient, self-maintaining platform. With automated backups to Google Drive, daily updates via Watchtower, and proactive alerts with Prometheus and Alertmanager, our server can now run 24/7 with minimal manual intervention. We\u0026rsquo;ve built a solid, reliable, and intelligent system.\nBut there\u0026rsquo;s one critical piece still missing: end-to-end security for our local services.\nRight now, we\u0026rsquo;re accessing our dashboards at addresses like http://grafana.local, which browsers flag as \u0026ldquo;Not Secure.\u0026rdquo; What if we could use a real, public domain name for our internal services and get a valid HTTPS certificate, all without opening a single port on our router?\nIn the next part of this series, I\u0026rsquo;ll show you exactly how to do that. We\u0026rsquo;ll dive into an advanced but powerful setup using Cloudflare and Nginx Proxy Manager to bring trusted, zero-exposure SSL to everything we\u0026rsquo;ve built.\nStay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-4-automation-and-alerting/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to the part 4 of the homelab series! In the previous parts, we built a server, deployed a suite of services, and configured our network. Now, it\u0026rsquo;s time to make it resilient and self-maintaining. A homelab isn\u0026rsquo;t just about setting things up; it\u0026rsquo;s about keeping them running reliably.\u003c/p\u003e\n\u003cp\u003eThis guide will show you how to set up the three pillars of modern IT operations: \u003cstrong\u003eAutomated Backups\u003c/strong\u003e, \u003cstrong\u003eAutomated Updates\u003c/strong\u003e, and \u003cstrong\u003eProactive Alerting\u003c/strong\u003e. By the end, you\u0026rsquo;ll have a homelab that runs itself, ensures your data is safe, stays up-to-date, and notifies you when something goes wrong.\u003c/p\u003e","title":"Part 4: Automating a Homelab with Backups, Updates, and Alerts"},{"content":"My New Weekend Project: Building a Personal Ad-Blocking Server in the Cloud! Hey everyone, Prajwol here.\nLike a lot of you, I spend a good chunk of my day online. And lately, it\u0026rsquo;s felt like I\u0026rsquo;m in a constant battle with pop-ups, trackers, and auto-playing video ads. I\u0026rsquo;ve used browser extensions for years, but I wanted a more powerful solution—something that would protect my entire home network, including my phone and smart TV, without needing to install software everywhere.\nSo, I decided to take on a new project: building my very own ad-blocking DNS server in the cloud.\nI\u0026rsquo;d done something similar a while back with Linode, but this time I wanted to dive into the world of Amazon Web Services (AWS) and see if I could build a reliable, secure, and cost-effective setup from scratch. It turned into quite the adventure, involving a late-night session of launching a virtual server, wrestling with firewalls, and securing my own private domain with an SSL certificate.\nThe end result? It\u0026rsquo;s been fantastic. My web pages load noticeably faster, and the general online experience feels so much cleaner and less intrusive. Plus, knowing that I have full control over my own corner of the internet is incredibly satisfying. It\u0026rsquo;s a great feeling to see the query logs fill up with blocked requests for domains I\u0026rsquo;ve never even heard of!\nI documented every single step of my journey, from the first click in the AWS console to the final configuration on my home router. If you\u0026rsquo;re curious about how to build one for yourself, I\u0026rsquo;ve written up a complete, step-by-step guide.\nYou can check out the full project guide here! It was a challenging but really rewarding project. Let me know what you think!\nPublished: Friday, August 22, 2025\n","permalink":"http://localhost:1313/blog/adguard-aws/","summary":"\u003ch1 id=\"my-new-weekend-project-building-a-personal-ad-blocking-server-in-the-cloud\"\u003eMy New Weekend Project: Building a Personal Ad-Blocking Server in the Cloud!\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eLike a lot of you, I spend a good chunk of my day online. And lately, it\u0026rsquo;s felt like I\u0026rsquo;m in a constant battle with pop-ups, trackers, and auto-playing video ads. I\u0026rsquo;ve used browser extensions for years, but I wanted a more powerful solution—something that would protect my entire home network, including my phone and smart TV, without needing to install software everywhere.\u003c/p\u003e","title":"Building a Personal Ad-Blocking Server in the Cloud!"},{"content":"Introduction Welcome to Part 3 of my homelab series! In the previous parts, I built my server and deployed a suite of management and monitoring tools. Now, it\u0026rsquo;s time to build the brain of my network: a robust, redundant, and high-availability DNS system using AdGuard Home that works both at home and on the go.\nIn this detailed guide, I\u0026rsquo;ll walk you through how I deployed a total of three AdGuard Home instances, each with its own unique IP address. I set up a primary resolver on my homelab, a secondary failover resolver in the cloud for my mobile devices, and a tertiary resolver on a separate virtual network for local redundancy.\nChapter 1: The Local Workhorse (Primary DNS) I started by deploying my main, day-to-day DNS resolver on my homelab server.\nStep 1: Deploying AdGuard Home with Docker Compose First, I SSHed into my server, created a directory for the project, and a docker-compose.yml file to define the service.\nmkdir -p ~/docker/adguard-primary cd ~/docker/adguard-primary nano docker-compose.yml I pasted in the following configuration. This runs the AdGuard Home container, maps all the necessary ports for DNS and the web UI, and connects it to the shared npm_default network I set up in Part 2.\nservices: adguardhome: image: adguard/adguardhome:latest container_name: adguard-primary restart: unless-stopped ports: - \u0026#34;53:53/tcp\u0026#34; - \u0026#34;53:53/udp\u0026#34; - \u0026#34;8080:80/tcp\u0026#34; # Web UI - \u0026#34;853:853/tcp\u0026#34; # DNS-over-TLS volumes: - ./workdir:/opt/adguardhome/work - ./confdir:/opt/adguardhome/conf networks: - npm_default networks: npm_default: external: true I then launched the container by running:\ndocker compose up -d Step 2: Initial AdGuard Home Setup Wizard I navigated to http://\u0026lt;your-server-ip\u0026gt;:3000 in my web browser to start the setup wizard.\nI clicked \u0026ldquo;Get Started.\u0026rdquo;\nOn the \u0026ldquo;Admin Web Interface\u0026rdquo; screen, I changed the \u0026ldquo;Listen Interface\u0026rdquo; to All interfaces and the port to 80.\nOn the \u0026ldquo;DNS server\u0026rdquo; screen, I changed the \u0026ldquo;Listen Interface\u0026rdquo; to All interfaces and left the port as 53.\nI followed the prompts to create my admin username and password.\nOnce the setup was complete, I was redirected to my main dashboard, now available at http://\u0026lt;your-server-ip\u0026gt;:8080.\nStep 3: Configure My Home Router To make all my devices use AdGuard automatically, I logged into my home router\u0026rsquo;s admin panel, found the DHCP Server settings, and changed the Primary DNS Server to my homelab\u0026rsquo;s static IP address (e.g., 192.168.1.10).\nChapter 2: The Cloud Failover (Secondary DNS on Oracle Cloud) An off-site DNS server ensures I have ad-blocking on my mobile devices and acts as a backup.\nWhy I Chose Oracle Cloud After testing the free tiers of both AWS and Linode, I chose Oracle Cloud Infrastructure (OCI). In my experience, OCI\u0026rsquo;s \u0026ldquo;Always Free\u0026rdquo; tier is far more generous with its resources. It provides powerful Ampere A1 Compute instances with up to 4 CPU cores and 24 GB of RAM, plus 200 GB of storage and significant bandwidth, all for free. This was ideal for running my service 24/7 without the strict limitations or eventual costs associated with other providers.\nStep 1: Launching the Oracle Cloud VM Sign Up: I created my account on the Oracle Cloud website.\nCreate VM Instance: In the OCI console, I navigated to Compute \u0026gt; Instances and clicked \u0026ldquo;Create instance\u0026rdquo;.\nConfigure Instance:\nName: I gave it a name like AdGuard-Cloud.\nImage and Shape: I clicked \u0026ldquo;Edit\u0026rdquo;. For the image, I selected Ubuntu. For the shape, I selected \u0026ldquo;Ampere\u0026rdquo; and chose the VM.Standard.A1.Flex shape (it\u0026rsquo;s \u0026ldquo;Always Free-eligible\u0026rdquo;).\nNetworking: I used the default VCN and made sure \u0026ldquo;Assign a public IPv4 address\u0026rdquo; was checked.\nSSH Keys: I added my SSH public key.\nI clicked Create. Once the instance was running, I took note of its Public IP Address.\nStep 2: Configuring the Cloud Firewall For maximum security, I locked down the administrative ports to only my home IP address.\nFind My Public IP: I went to a site like whatismyip.com and copied my home\u0026rsquo;s public IP address.\nEdit Security List: I navigated to my instance\u0026rsquo;s details page, clicked the subnet link, then clicked the \u0026ldquo;Security List\u0026rdquo; link.\nI clicked \u0026ldquo;Add Ingress Rules\u0026rdquo; and added the following rules:\nFor SSH (Port 22): I set the Source to my home\u0026rsquo;s public IP, followed by /32 (e.g., 203.0.113.55/32). This is a critical security step.\nFor AdGuard Setup (Port 3000): I also set the Source to my home\u0026rsquo;s public IP with /32.\nFor AdGuard Web UI (Port 80/443): I set the Source to my home\u0026rsquo;s public IP with /32 as well.\nFor Public DNS (Port 53, 853, etc.): I set the Source to 0.0.0.0/0 (Anywhere) to allow all my devices to connect from any network.\nStep 3: Installing AdGuard Home \u0026amp; Configuring SSL Connect via SSH: I used the public IP and my SSH key to connect to the VM.\nRun Install Script: I chose to install AdGuard Home directly on the OS for this instance.\ncurl -s -S -L [https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh](https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh) | sh -s -- -v Get a Hostname: I went to No-IP.com, created a free hostname (e.g., my-cloud-dns.ddns.net), and pointed it to my cloud VM\u0026rsquo;s public IP.\nEnable Encryption: In my cloud AdGuard\u0026rsquo;s dashboard (Settings \u0026gt; Encryption settings), I enabled encryption, entered my No-IP hostname, and used the built-in function to request a Let\u0026rsquo;s Encrypt certificate.\nStep 4: Creating a Cloud Backup (Snapshot) A critical final step for any cloud service is creating a backup. Here is how I did it in OCI:\nIn the OCI Console, I navigated to the details page for my AdGuard-Cloud instance.\nUnder the \u0026ldquo;Resources\u0026rdquo; menu on the left, I clicked on \u0026ldquo;Boot volume\u0026rdquo;.\nOn the Boot Volume details page, under \u0026ldquo;Resources,\u0026rdquo; I clicked \u0026ldquo;Boot volume backups\u0026rdquo;.\nI clicked the \u0026ldquo;Create boot volume backup\u0026rdquo; button.\nI gave the backup a descriptive name (e.g., AdGuard-Cloud-Backup-YYYY-MM-DD) and clicked the create button. This creates a full snapshot of my server that I can use to restore it in minutes.\nStep 5: ### How to Use Your Cloud DNS on Mobile Devices The main benefit of the cloud server is having ad-blocking on the go. Here’s how I set it up on my mobile phone using secure, encrypted DNS.\nFor Android (Version 9+): Modern Android has a built-in feature called \u0026ldquo;Private DNS\u0026rdquo; that uses DNS-over-TLS (DoT), which is perfect for this.\nOpen Settings on your Android device.\nTap on \u0026ldquo;Network \u0026amp; internet\u0026rdquo; (this may be called \u0026ldquo;Connections\u0026rdquo; on some devices).\nFind and tap on \u0026ldquo;Private DNS\u0026rdquo;. You may need to look under an \u0026ldquo;Advanced\u0026rdquo; section.\nSelect the option labeled \u0026ldquo;Private DNS provider hostname\u0026rdquo;.\nIn the text box, enter the No-IP hostname you created for your Oracle Cloud server (e.g., my-cloud-dns.ddns.net).\nTap Save.\nYour phone will now send all its DNS queries through an encrypted tunnel to your personal AdGuard Home server in the cloud, giving you ad-blocking on both Wi-Fi and cellular data.\nFor iOS (iPhone/iPad): On iOS, the easiest way to set up encrypted DNS is by installing a configuration profile.\nOn your iPhone or iPad, open Safari.\nGo to a DNS profile generator site, like the one provided by AdGuard.\nWhen prompted, enter the DNS-over-HTTPS (DoH) address for your cloud server. It will be your No-IP hostname with /dns-query at the end (e.g., https://my-cloud-dns.ddns.net/dns-query).\nDownload the generated configuration profile.\nGo to your device\u0026rsquo;s Settings app. You will see a new \u0026ldquo;Profile Downloaded\u0026rdquo; item near the top. Tap on it.\nFollow the on-screen prompts to Install the profile. You may need to enter your device passcode.\nOnce installed, your iOS device will also route its DNS traffic through your secure cloud server.\nChapter 3: Ultimate Local Redundancy (Tertiary DNS with Macvlan) For an extra layer of redundancy within my homelab, I created a third AdGuard instance. By using an advanced Docker network mode called macvlan, this container gets its own unique IP address on my home network, making it a truly independent resolver.\nCreate Macvlan Network: First, I created the macvlan network, telling it which of my physical network cards to use (eth0 in my case).\ndocker network create -d macvlan \\ --subnet=192.168.1.0/24 \\ --gateway=192.168.1.1 \\ -o parent=eth0 homelab_net Deploy Tertiary Instance: I created a new folder (~/docker/adguard-tertiary) and this docker-compose.yml. Notice there are no ports since the container gets its own IP.\nservices: adguardhome2: image: adguard/adguardhome:latest container_name: adguardhome2 volumes: - \u0026#34;./work:/opt/adguardhome/work\u0026#34; - \u0026#34;./conf:/opt/adguardhome/conf\u0026#34; networks: homelab_net: ipv4_address: 192.168.1.11 # The new, unique IP for this container restart: unless-stopped networks: homelab_net: external: true Configure Router for Local Failover: To complete the local redundancy, I went back into my router\u0026rsquo;s DHCP settings.\nIn the Primary DNS field, I have the IP of my main homelab server (e.g., 192.168.1.10).\nIn the Secondary DNS field, I entered the unique IP address I assigned to my macvlan container (e.g., 192.168.1.11).\nNow, if my primary AdGuard container has an issue, all devices on my network will automatically fail over to the tertiary instance.\nChapter 4: Fine-Tuning and Integration Finally, I implemented some best practices on my primary AdGuard Home instance.\nUpstream DNS Servers: Under Settings \u0026gt; DNS Settings, I configured AdGuard to send requests to multiple resolvers in parallel for speed and reliability, using Cloudflare (1.1.1.1), Google (8.8.8.8), and Quad9 (9.9.9.9).\nEnable DNSSEC: In the same settings page, I enabled DNSSEC to verify the integrity of DNS responses.\nDNS Blocklists: I added several popular lists from the \u0026ldquo;Filters \u0026gt; DNS blocklists\u0026rdquo; page, including the AdGuard DNS filter and the OISD Blocklist, for robust protection.\nDNS Rewrites for Local Services: This is the key to a clean homelab experience. For each service, I performed a detailed two-step process:\nCreate the Proxy Host in Nginx Proxy Manager: I logged into my NPM admin panel, went to Hosts \u0026gt; Proxy Hosts, and clicked \u0026ldquo;Add Proxy Host\u0026rdquo;. For my Homer dashboard, I set the Forward Hostname to homer (the container name) and the Forward Port to 8080 (its internal port), using homer.local as the domain name.\nCreate the DNS Rewrite in AdGuard Home: I logged into my primary AdGuard dashboard, went to Filters \u0026gt; DNS Rewrites, and clicked \u0026ldquo;Add DNS rewrite\u0026rdquo;. I entered homer.local as the domain and the IP address of my Nginx Proxy Manager server as the answer.\nConclusion I\u0026rsquo;ve now built an incredibly robust, multi-layered DNS infrastructure. My home devices use the primary local server, which is backed up by a second, independent local server, and my mobile devices use a completely separate cloud instance for on-the-go protection. This provides a resilient, secure, and ad-free internet experience.\nIn the final part of this series, we\u0026rsquo;ll shift our focus from deploying services to maintaining them. I\u0026rsquo;ll show you how I set up a fully automated operations pipeline for my homelab, including daily off-site backups, automatic container updates with Watchtower, and proactive alerting with Prometheus. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-3-high-availability-dns/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to Part 3 of my homelab series! In the previous parts, I built my server and deployed a suite of management and monitoring tools. Now, it\u0026rsquo;s time to build the brain of my network: a robust, redundant, and high-availability DNS system using \u003cstrong\u003eAdGuard Home\u003c/strong\u003e that works both at home and on the go.\u003c/p\u003e\n\u003cp\u003eIn this detailed guide, I\u0026rsquo;ll walk you through how I deployed a total of \u003cstrong\u003ethree\u003c/strong\u003e AdGuard Home instances, each with its own unique IP address. I set up a primary resolver on my homelab, a secondary failover resolver in the cloud for my mobile devices, and a tertiary resolver on a separate virtual network for local redundancy.\u003c/p\u003e","title":"Part 3: A High-Availability DNS Network with AdGuard Home"},{"content":"New Project Alert: Running a Powerful AI Locally with Docker! Hey everyone, Prajwol here.\nI\u0026rsquo;ve always been fascinated by the incredible advancements in AI and large language models. While cloud-based models are powerful, I was really curious about what it would take to run a high-performance model right on my own machine. This gives you ultimate privacy, control, and the ability to experiment without limits.\nSo, for my latest project, I decided to dive in and get the DeepSeek-R1 model, a powerful AI, up and running locally using Docker.\nDocker is an amazing tool that lets you package up applications and all their dependencies into a neat little box called a container. This means you can run complex software without the headache of complicated installations or conflicts with other programs on your system. It was the perfect way to tame this powerful AI and get it running smoothly on my Ubuntu machine.\nThe process was a fantastic learning experience, covering everything from setting up Docker to pulling the model and interacting with the AI. It’s amazing to have that kind of power running on your own hardware.\nI’ve documented my entire process in a detailed, step-by-step guide. If you’re interested in local AI and want to see how you can run a powerful model yourself, be sure to check it out!\nYou can find the full project guide right here! Let me know what you think of this one!\nPublished: February, 2025\n","permalink":"http://localhost:1313/blog/running-deepseek-r1-on-docker-container-on-ubuntu/","summary":"\u003ch1 id=\"new-project-alert-running-a-powerful-ai-locally-with-docker\"\u003eNew Project Alert: Running a Powerful AI Locally with Docker!\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve always been fascinated by the incredible advancements in AI and large language models. While cloud-based models are powerful, I was really curious about what it would take to run a high-performance model right on my own machine. This gives you ultimate privacy, control, and the ability to experiment without limits.\u003c/p\u003e\n\u003cp\u003eSo, for my latest project, I decided to dive in and get the \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e model, a powerful AI, up and running locally using \u003cstrong\u003eDocker\u003c/strong\u003e.\u003c/p\u003e","title":"Running a Powerful AI Locally with Docker!"},{"content":"Introduction Welcome to Part 2 of my homelab series! In Part 1, we built a solid foundation by turning an old laptop into a hardened Debian server with Docker. Now that our server is running, we need to deploy services to manage, monitor, and easily access our projects.\nIn this guide, we\u0026rsquo;ll deploy three essential stacks. First, Nginx Proxy Manager (NPM) will act as our server\u0026rsquo;s front door and create a shared network for our containers. Second, we\u0026rsquo;ll set up a professional-grade monitoring stack with Prometheus and Grafana. Finally, we\u0026rsquo;ll deploy a Homer dashboard to create a beautiful and convenient launchpad for all our services.\n1. The Management Layer: Nginx Proxy Manager (NPM) 🌐 Before we can deploy our other services, we need a way to manage connections between them. NPM will act as our reverse proxy and, crucially, will create the shared Docker network that all our other services will connect to.\nA. Deploy Nginx Proxy Manager First, let\u0026rsquo;s create a directory and the docker-compose.yml file for NPM.\n# Create the directory mkdir -p ~/docker/npm cd ~/docker/npm # Create the docker-compose.yml nano docker-compose.yml Paste in the following configuration. This file defines the NPM service and creates a network named npm_default.\nservices: app: image: \u0026#39;jc21/nginx-proxy-manager:latest\u0026#39; container_name: npm-app-1 restart: unless-stopped ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;81:81\u0026#39; volumes: - ./data:/data - ./letsencrypt:/etc/letsencrypt networks: default: name: npm_default Launch it with\ndocker compose up -d You can now log in to the admin UI at http://\u0026lt;your-server-ip\u0026gt;:81.\n2. The Monitoring Stack 📊 With our shared network in place, we can now deploy our monitoring stack.\nPrometheus: Collects all the metrics.\nNode Exporter: Exposes the server\u0026rsquo;s hardware metrics.\ncAdvisor: Exposes Docker container metrics.\nGrafana: Visualizes all the data in beautiful dashboards.\nA. Create the Prometheus Configuration Prometheus needs a config file to know what to monitor.\n# Create the project directory mkdir -p ~/docker/monitoring cd ~/docker/monitoring # Create the prometheus.yml file nano prometheus.yml Paste in the following configuration:\nglobal: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;node-exporter\u0026#39; static_configs: - targets: [\u0026#39;node-exporter:9100\u0026#39;] - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] B. Deploy the Stack with Docker Compose Next, create the docker-compose.yml file in the same ~/docker/monitoring directory.\nnano docker-compose.yml This file defines all four monitoring services and tells them to connect to the npm_default network we created earlier.\nservices: prometheus: image: prom/prometheus:latest container_name: prometheus restart: unless-stopped ports: - \u0026#34;9090:9090\u0026#34; volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - prometheus_data:/prometheus networks: - default grafana: image: grafana/grafana:latest container_name: grafana restart: unless-stopped ports: - \u0026#34;3001:3000\u0026#34; volumes: - grafana_data:/var/lib/grafana networks: - default node-exporter: image: prom/node-exporter:latest container_name: node-exporter restart: unless-stopped volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - \u0026#39;--path.procfs=/host/proc\u0026#39; - \u0026#39;--path.sysfs=/host/sys\u0026#39; - \u0026#39;--path.rootfs=/rootfs\u0026#39; - \u0026#39;--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)\u0026#39; networks: - default cadvisor: image: gcr.io/cadvisor/cadvisor:latest container_name: cadvisor restart: unless-stopped ports: - \u0026#34;8081:8080\u0026#34; volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro networks: - default volumes: prometheus_data: grafana_data: networks: default: name: npm_default external: true Now, launch the stack:\ndocker compose up -d C. Configure Grafana Log in to Grafana at http://\u0026lt;your-server-ip\u0026gt;:3001 (default: admin/admin).\nAdd Data Source: Go to Connections \u0026gt; Data Sources, add a Prometheus source, and set the URL to http://prometheus:9090.\nImport Dashboards: Go to Dashboards \u0026gt; New \u0026gt; Import and add these dashboards by ID:\nNode Exporter Full (ID: 1860)\nDocker Host/Container Metrics (ID: 193)\n3. The Homer Launchpad Dashboard 🚀 Finally, let\u0026rsquo;s deploy Homer as our beautiful start page with custom icons.\nCreate Directories \u0026amp; Download Icons: First, create a directory for Homer and an assets subdirectory. Then, cd into the assets folder and download the icons.\nmkdir -p ~/docker/homer/assets cd ~/docker/homer/assets wget -O grafana.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/grafana.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/grafana.png) wget -O prometheus.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/prometheus.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/prometheus.png) wget -O cadvisor.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/cadvisor.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/cadvisor.png) wget -O npm.png [https://nginxproxymanager.com/icon.png](https://nginxproxymanager.com/icon.png) Create Configuration: Go back to the main homer directory and create the config.yml file.\ncd ~/docker/homer nano config.yml Paste in the following configuration. The logo: lines point to the icons we just downloaded.\n--- title: \u0026#34;Homelab Dashboard\u0026#34; subtitle: \u0026#34;Server Management\u0026#34; theme: \u0026#34;dark\u0026#34; services: - name: \u0026#34;Management\u0026#34; icon: \u0026#34;fas fa-server\u0026#34; items: - name: \u0026#34;Nginx Proxy Manager\u0026#34; logo: \u0026#34;assets/tools/npm.png\u0026#34; subtitle: \u0026#34;Reverse Proxy Admin\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:81\u0026#34; - name: \u0026#34;Monitoring\u0026#34; icon: \u0026#34;fas fa-chart-bar\u0026#34; items: - name: \u0026#34;Grafana\u0026#34; logo: \u0026#34;assets/tools/grafana.png\u0026#34; subtitle: \u0026#34;Metrics Dashboard\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:3001\u0026#34; - name: \u0026#34;Prometheus\u0026#34; logo: \u0026#34;assets/tools/prometheus.png\u0026#34; subtitle: \u0026#34;Metrics Database\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:9090\u0026#34; - name: \u0026#34;cAdvisor\u0026#34; logo: \u0026#34;assets/tools/cadvisor.png\u0026#34; subtitle: \u0026#34;Container Metrics\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:8081\u0026#34; Create Docker Compose File: Finally, create the docker-compose.yml file.\nnano docker-compose.yml This configuration connects Homer to our shared network.\nservices: homer: image: b4bz/homer container_name: homer volumes: - ./config.yml:/www/assets/config.yml - ./assets:/www/assets/tools ports: - \u0026#34;8090:8080\u0026#34; restart: unless-stopped networks: - npm_default networks: npm_default: external: true Launch: Run docker compose up -d. You can now access your new dashboard with custom icons at http://\u0026lt;your-server-ip\u0026gt;:8090.\nConclusion Our homelab now has a powerful management and monitoring foundation. Nginx Proxy Manager is ready to direct traffic, Grafana is visualizing our server\u0026rsquo;s health, and Homer provides a central launchpad.\nIn the next part of the series, we\u0026rsquo;ll deploy our core network service, AdGuard Home, and use NPM to create clean, memorable local domains for all the applications we set up today. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-2-monitoring-and-management/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to Part 2 of my homelab series! In \u003ca href=\"/projects/homelab-series-part-1-debian-docker-foundation/\"\u003ePart 1\u003c/a\u003e, we built a solid foundation by turning an old laptop into a hardened Debian server with Docker. Now that our server is running, we need to deploy services to manage, monitor, and easily access our projects.\u003c/p\u003e\n\u003cp\u003eIn this guide, we\u0026rsquo;ll deploy three essential stacks. First, \u003cstrong\u003eNginx Proxy Manager (NPM)\u003c/strong\u003e will act as our server\u0026rsquo;s front door and create a shared network for our containers. Second, we\u0026rsquo;ll set up a professional-grade monitoring stack with \u003cstrong\u003ePrometheus\u003c/strong\u003e and \u003cstrong\u003eGrafana\u003c/strong\u003e. Finally, we\u0026rsquo;ll deploy a \u003cstrong\u003eHomer\u003c/strong\u003e dashboard to create a beautiful and convenient launchpad for all our services.\u003c/p\u003e","title":"Part 2: Homelab Management \u0026 Monitoring"},{"content":"My First Cloud Ad-Blocker: A Look Back at AdGuard Home on Linode Hey everyone, Prajwol here.\nAs I continue to explore different cloud projects, I often think back to the ones that had the biggest impact on my day-to-day life. One of the very first projects that truly changed my internet experience was setting up my own ad-blocking DNS server using AdGuard Home on a Linode instance.\nMy goal was to find a simple, cost-effective way to block ads and trackers across my entire home network. I wanted a \u0026ldquo;set it and forget it\u0026rdquo; solution that would cover every device—from my phone to my smart TV—without needing to install an app on each one. Linode (now Akamai) was the perfect platform for this: straightforward, powerful, and great for hosting a lightweight service like AdGuard Home.\nThe process of spinning up a small server, running a single installation script, and then seeing the query logs light up with blocked requests was incredibly satisfying. It felt like I had taken back a real measure of control over my own network.\nThis project remains a fantastic entry point for anyone wanting to get started with self-hosting and network privacy. I\u0026rsquo;ve kept the original, detailed guide for anyone who wants to follow along.\nYou can find the full step-by-step project guide here! It’s a rewarding project that delivers tangible results almost immediately. Let me know if you give it a try!\nPublished: Friday, August 22, 2025\n","permalink":"http://localhost:1313/blog/adguard-home-on-cloud/","summary":"\u003ch1 id=\"my-first-cloud-ad-blocker-a-look-back-at-adguard-home-on-linode\"\u003eMy First Cloud Ad-Blocker: A Look Back at AdGuard Home on Linode\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eAs I continue to explore different cloud projects, I often think back to the ones that had the biggest impact on my day-to-day life. One of the very first projects that truly changed my internet experience was setting up my own ad-blocking DNS server using AdGuard Home on a Linode instance.\u003c/p\u003e\n\u003cp\u003eMy goal was to find a simple, cost-effective way to block ads and trackers across my entire home network. I wanted a \u0026ldquo;set it and forget it\u0026rdquo; solution that would cover every device—from my phone to my smart TV—without needing to install an app on each one. Linode (now Akamai) was the perfect platform for this: straightforward, powerful, and great for hosting a lightweight service like AdGuard Home.\u003c/p\u003e","title":"My First Cloud Ad-Blocker - A Look Back at AdGuard Home on Linode"},{"content":"Introduction Welcome to the first post in my new homelab series! I\u0026rsquo;ve always been fascinated by self-hosting and DevOps, and I believe the best way to learn is by doing. In this series, I\u0026rsquo;ll document my journey of turning an old, unused laptop into a powerful, efficient, and secure bare-metal server for hosting a variety of network services.\nThe goal for this first part is to lay a solid foundation. We\u0026rsquo;ll take an old laptop, install a minimal and stable Linux operating system, perform some initial security hardening, and set up Docker as our containerization engine. By the end of this post, we\u0026rsquo;ll have a perfect blank canvas ready for the exciting services we\u0026rsquo;ll deploy in the upcoming parts.\n1. Choosing the Hardware \u0026amp; OS Why an Old Laptop? Before diving in, why use an old laptop instead of a Raspberry Pi or a dedicated server? For a starter homelab, a laptop has three huge advantages:\nCost-Effective: It\u0026rsquo;s free if you have one lying around! Built-in UPS: The battery acts as a built-in Uninterruptible Power Supply (UPS), keeping the server running through short power outages. Low Power Consumption: Laptop hardware is designed to be power-efficient, which is great for a device that will be running 24/7. Why Debian 13 \u0026ldquo;Trixie\u0026rdquo;? For the operating system, I chose Debian. It\u0026rsquo;s renowned for its stability, security, and massive package repository. It’s the bedrock of many other distributions (like Ubuntu) and is perfect for a server because it\u0026rsquo;s lightweight and doesn\u0026rsquo;t include unnecessary software. We\u0026rsquo;ll be using the minimal \u0026ldquo;net-install\u0026rdquo; to ensure we only install what we absolutely need.\n2. Installation and Network Configuration The installation process is straightforward, but the network setup is key to a reliable server.\nMinimal Installation Create a Bootable USB: I downloaded the Debian 13 \u0026ldquo;netinst\u0026rdquo; ISO from the official website and used Rufus on Windows to create a bootable USB drive. Boot from USB: I plugged the USB into the laptop and booted from it (usually pressing F12, F2, or Esc during startup to select the USB device). Language, Location, and Keyboard: Selected English, United States, and the default keyboard layout. Network Setup: Connected the laptop to my home network (Ethernet preferred for stability). Hostname \u0026amp; Domain: Entered a short, memorable hostname for the server (e.g., homelab) and left the domain blank. User Accounts: Set a root password. Created a non-root regular user (this will be used for daily management). Partition Disks: Chose Guided – use entire disk with separate /home partition. This is simpler for a server setup. Software Selection: At the “Software selection” screen: Unchecked “Debian desktop environment” Checked “SSH server” and “standard system utilities” This ensures a clean command-line system that can be accessed remotely. GRUB Bootloader: Installed GRUB on the primary drive (so the system boots correctly). Finish Installation: Removed the USB drive when prompted and rebooted into the fresh Debian install. Setting a Static IP A server needs a permanent, unchanging IP address. The best way to do this is with DHCP Reservation on your router. This tells your router to always assign the same IP address to your server\u0026rsquo;s unique MAC address.\nFirst, find your laptop’s current IP address and network interface name by running:\nip a You’ll see output similar to:\n2: enp3s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\rinet 192.168.0.45/24 brd 192.168.0.255 scope global dynamic enp3s0\rvalid_lft 86396sec preferred_lft 86396sec In this example:\nInterface name: enp3s0 Current IP: 192.168.0.45 MAC address: shown under link/ether in the same section. With this info, log into your router’s admin panel, find the \u0026ldquo;DHCP Reservation\u0026rdquo; or \u0026ldquo;Static Leases\u0026rdquo; section, and assign a memorable IP address (e.g., 192.168.0.45) to your server’s MAC address.\nThis ensures the server always gets the same IP from your router, making it easy to find on your network.\nConnecting Remotely with SSH With a static IP set, all future management will be done remotely using an SSH client. For Windows, I highly recommend Solar-PuTTY. I created a new session, entered the server\u0026rsquo;s static IP address, my username, and password, and connected.\n3. Initial Server Hardening With a remote SSH session active, the first thing to do is secure the server and configure it for its headless role.\nUpdate the System First, let\u0026rsquo;s make sure all packages are up to date.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y Configure the Firewall ufw (Uncomplicated Firewall) is perfect for a simple setup. We\u0026rsquo;ll set it to deny all incoming traffic by default and only allow SSH connections.\n# Install UFW sudo apt install ufw -y # Allow SSH connections sudo ufw allow ssh # Enable the firewall sudo ufw enable Configure Lid-Close Action To ensure the laptop keeps running when the lid is closed, we edit the logind.conf file.\nsudo nano /etc/systemd/logind.conf Uncomment the line:\nHandleLidSwitch=ignore Save the file, then restart the service:\nsudo systemctl restart systemd-logind.service 4. Installing the Containerization Engine: Docker Instead of installing applications directly on our host, we\u0026rsquo;ll use Docker to keep the system clean and make management easier.\nInstall Docker Engine The official convenience script is the easiest way to get the latest version.\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Add User to Docker Group To run docker commands without sudo, add your user to the docker group. The $USER variable automatically uses the currently logged-in user.\nsudo usermod -aG docker $USER After this, log out and log back in for the change to take effect.\nInstall Docker Compose Docker Compose is essential for managing multi-container applications with a simple YAML file.\nsudo apt install docker-compose-plugin -y To verify the installation:\ndocker compose version Conclusion And that\u0026rsquo;s it for Part 1! We\u0026rsquo;ve successfully turned an old piece of hardware into a hardened, modern server running Debian and Docker with a reliable network configuration. We have a solid and secure foundation to build upon.\nIn the next part of the series, we\u0026rsquo;ll deploy our first critical service: a local, network-wide ad-blocking DNS resolver using AdGuard Home. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-1-debian-docker-foundation/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to the first post in my new homelab series! I\u0026rsquo;ve always been fascinated by self-hosting and DevOps, and I believe the best way to learn is by doing. In this series, I\u0026rsquo;ll document my journey of turning an old, unused laptop into a powerful, efficient, and secure bare-metal server for hosting a variety of network services.\u003c/p\u003e\n\u003cp\u003eThe goal for this first part is to lay a solid foundation. We\u0026rsquo;ll take an old laptop, install a minimal and stable Linux operating system, perform some initial security hardening, and set up Docker as our containerization engine. By the end of this post, we\u0026rsquo;ll have a perfect blank canvas ready for the exciting services we\u0026rsquo;ll deploy in the upcoming parts.\u003c/p\u003e","title":"Part 1: Reviving an Old Laptop with Debian \u0026 Docker"},{"content":"Your Personal Internet Guardian: How to Build a FREE Ad-Blocker in the Cloud! 🚀 Hey everyone! A while back, I wrote a guide on setting up AdGuard Home on Linode. The world of tech moves fast, and it\u0026rsquo;s time for an upgrade! Today, we\u0026rsquo;re going to build our own powerful, network-wide ad-blocker using Amazon Web Services (AWS), and we\u0026rsquo;ll make it secure with our own domain and SSL certificate.\nThink of this as building a digital gatekeeper for your internet. Before any ads, trackers, or malicious sites can reach your devices, our AdGuard Home server will slam the door shut. The best part? This works on your phone, laptop, smart TV—anything on your network—without installing a single app on them.\nThis guide is for everyone, from seasoned tech wizards to curious beginners. We\u0026rsquo;ll break down every step in simple terms, so grab a coffee, and let\u0026rsquo;s build something awesome!\n## Chapter 1: Building Our Home in the AWS Cloud ☁️ First, we need a server. We\u0026rsquo;ll use an Amazon EC2 instance, which is just a fancy name for a virtual computer that you rent.\nSign Up for AWS: If you don\u0026rsquo;t have an account, head to the AWS website and sign up. You\u0026rsquo;ll need a credit card for verification, but for this guide, we can often stay within the Free Tier.\nLaunch Your EC2 Instance:\nLog in to your AWS Console and search for EC2. Click \u0026ldquo;Launch instance\u0026rdquo;. Name: Give your server a cool name, like AdGuard-Server. Application and OS Images: In the search bar, type Debian and select the latest version (e.g., Debian 12). Make sure it\u0026rsquo;s marked \u0026ldquo;Free tier eligible\u0026rdquo;. Instance Type: Choose t2.micro. This is your free, trusty little server. Key Pair (for login): This is your digital key to the server\u0026rsquo;s front door. Click \u0026ldquo;Create a new key pair\u0026rdquo;, name it something like my-adguard-key, and download the .pem file. Keep this file secret and safe! Network settings (The Firewall): This is crucial. We need to tell our server which doors to open. Click \u0026ldquo;Edit\u0026rdquo;. Check the box for \u0026ldquo;Allow SSH traffic from\u0026rdquo; and select My IP. This lets you securely log in. Check \u0026ldquo;Allow HTTPS traffic from the internet\u0026rdquo; and \u0026ldquo;Allow HTTP traffic from the internet\u0026rdquo;. We\u0026rsquo;ll need these for our secure dashboard later. Launch It! Hit the \u0026ldquo;Launch instance\u0026rdquo; button and watch as your new cloud server comes to life.\nGive Your Server a Permanent Address (Elastic IP):\nBy default, your server\u0026rsquo;s public IP address will change every time it reboots. Let\u0026rsquo;s make it permanent! In the EC2 menu on the left, go to \u0026ldquo;Elastic IPs\u0026rdquo;. Click \u0026ldquo;Allocate Elastic IP address\u0026rdquo; and then \u0026ldquo;Allocate\u0026rdquo;. Select the new IP address from the list, click \u0026ldquo;Actions\u0026rdquo;, and then \u0026ldquo;Associate Elastic IP address\u0026rdquo;. Choose your AdGuard-Server instance from the list and click \u0026ldquo;Associate\u0026rdquo;. Your server now has a static IP address that will never change! Make a note of this new IP. ## Chapter 2: Opening the Doors (Configuring the Firewall) 🚪 Our server is running, but for maximum security, we want to ensure only you can access the administrative parts of it. We\u0026rsquo;ll open the public DNS ports to everyone, but lock down the management ports to your home IP address.\nFind Your Public IP Address: Open a new browser tab and go to a site like WhatIsMyIP.com. It will display your home\u0026rsquo;s public IP address. Copy this IP address (it will look something like 203.0.113.55).\nEdit the Firewall Rules: Go to your EC2 Instance details, click the \u0026ldquo;Security\u0026rdquo; tab, and click on the Security Group name.\nClick \u0026ldquo;Edit inbound rules\u0026rdquo; and \u0026ldquo;Add rule\u0026rdquo; for each of the following. This makes sure your DNS is publicly available but the setup panel is locked down to your IP only.\nRule for AdGuard Setup (Port 3000):\nType: Custom TCP Port range: 3000 Source: Paste your IP address here, and add /32 to the end (e.g., 203.0.113.55/32). The /32 tells AWS it\u0026rsquo;s a single, specific IP address. Rule for DNS (Port 53):\nType: Custom UDP and Custom TCP (you will add two separate rules for this port) Port range: 53 Source: Anywhere-IPv4 Rule for DNS-over-TLS (Port 853):\nType: Custom TCP Port range: 853 Source: Anywhere-IPv4 Click \u0026ldquo;Save rules\u0026rdquo;. Your firewall is now configured to allow public DNS requests while keeping your management panel secure.\n## Chapter 3: Installing AdGuard Home 🛡️ Now, let\u0026rsquo;s connect to our server and install the magic software.\nConnect via SSH: Open a terminal (PowerShell on Windows, Terminal on Mac/Linux) and use the key you downloaded to connect. Use your new Elastic IP address! # Replace the path and Elastic IP with your own ssh -i \u0026#34;path/to/my-adguard-key.pem\u0026#34; admin@YOUR_ELASTIC_IP Install AdGuard Home: Run this one simple command. It downloads and installs everything for you. curl -s -S -L [https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh](https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh) | sh -s -- -v Run the Setup Wizard: The script will give you a link, like http://YOUR_ELASTIC_IP:3000. Open this in your browser. Follow the on-screen steps to create your admin username and password. ## Chapter 4: Teaching Your Guardian Who to Trust and What to Block With AdGuard Home installed, the next step is to configure its core brain: the DNS servers it gets its answers from and the blocklists it uses to protect your network.\n1. Setting Up Upstream DNS Servers Think of \u0026ldquo;Upstream DNS Servers\u0026rdquo; as the giant, public phonebooks of the internet. When your AdGuard server doesn\u0026rsquo;t know an address (and it\u0026rsquo;s not on a blocklist), it asks one of these upstreams. It\u0026rsquo;s recommended to use a mix of the best encrypted DNS providers for security, privacy, and speed.\nIn the AdGuard dashboard, go to Settings -\u0026gt; DNS settings. In the \u0026ldquo;Upstream DNS servers\u0026rdquo; box, enter the following, one per line:\nhttps://dns.quad9.net/dns-query https://dns.google/dns-query https://dns.cloudflare.com/dns-query Quad9: Focuses heavily on security, blocking malicious domains. Google: Known for being very fast. Cloudflare: A great all-around choice with a strong focus on privacy. 2. Optimizing DNS Performance Still in the DNS settings page, scroll down to optimize how your server queries the upstreams.\nParallel requests: Select this option. This is the fastest and most resilient mode. It sends your DNS query to all three of your upstream servers at the same time and uses the answer from the very first one that responds. This ensures you always get the quickest possible result.\nEnable EDNS client subnet (ECS): Check this box. This is very important for services like Netflix, YouTube, and other content delivery networks (CDNs). It helps them give you content from a server that is geographically closest to you, resulting in faster speeds and a better experience.\n3. Enabling DNSSEC Right below the upstream servers, there\u0026rsquo;s a checkbox for \u0026ldquo;Enable DNSSEC\u0026rdquo;. You should check this box. DNSSEC is like a digital wax seal on a letter; it verifies that the DNS answers you\u0026rsquo;re getting are authentic and haven\u0026rsquo;t been tampered with. It\u0026rsquo;s a simple, one-click security boost.\n4. Choosing Your Blocklists This is the fun part—the actual ad-blocking! Go to Filters -\u0026gt; DNS blocklists. For a \u0026ldquo;Balanced \u0026amp; Powerful\u0026rdquo; setup that blocks aggressively without a high risk of breaking websites, enable the following lists:\nAdGuard DNS filter: A great, well-maintained baseline. OISD Blocklist Big: Widely considered one of the best all-in-one lists for blocking ads, trackers, and malware. HaGeZi\u0026rsquo;s Pro Blocklist: A fantastic list that adds another layer of aggressive blocking for privacy. HaGeZi\u0026rsquo;s Threat Intelligence Feed: A crucial security-only list that focuses on protecting against active threats like phishing and malware. This combination will give you robust protection against both annoyances and real dangers.\n## Chapter 5: Giving Your Server a Name (Free Domain with No-IP) 📛 An IP address is hard to remember. Let\u0026rsquo;s get a free, memorable name for our server.\nSign Up at No-IP: Go to No-IP.com, create a free account, and create a hostname (e.g., my-dns.ddns.net). Point it to Your Server: When creating the hostname, enter your server\u0026rsquo;s permanent Elastic IP address. Confirm your account via email. ## Chapter 6: Making It Secure with SSL/TLS 🔐 We\u0026rsquo;ll use Let\u0026rsquo;s Encrypt and Certbot to get a free SSL certificate, which lets us use secure https:// and encrypted DNS.\nInstall Certbot: In your SSH session, run these commands:\nsudo apt update sudo apt install certbot -y Get the Certificate: Run this command, replacing the email and domain with your own.\n# This command will temporarily stop any service on port 80, get the certificate, and then finish. sudo certbot certonly --standalone --agree-tos --email YOUR_EMAIL@example.com -d your-no-ip-hostname.ddns.net If it\u0026rsquo;s successful, it will tell you where your certificate files are saved (usually in /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/).\nConfigure AdGuard Home Encryption:\nGo to your AdGuard Home dashboard (Settings -\u0026gt; Encryption settings). Check \u0026ldquo;Enable encryption\u0026rdquo;. In the \u0026ldquo;Server name\u0026rdquo; field, enter your No-IP hostname. Under \u0026ldquo;Certificates\u0026rdquo;, choose \u0026ldquo;Set a certificates file path\u0026rdquo;. Certificate path: /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/fullchain.pem Private key path: /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/privkey.pem Click \u0026ldquo;Save configuration\u0026rdquo;. The page will reload on a secure https:// connection! ## Chapter 7: Automating SSL Renewal (Cron Job Magic) ✨ Let\u0026rsquo;s Encrypt certificates last for 90 days. We can tell our server to automatically renew them.\nOpen the Cron Editor: In SSH, run sudo crontab -e and choose nano as your editor. Add the Renewal Job: Add this line to the bottom of the file. It tells the server to try renewing the certificate every day at 2:30 AM. 30 2 * * * systemctl stop AdGuardHome.service \u0026amp;\u0026amp; certbot renew --quiet \u0026amp;\u0026amp; systemctl start AdGuardHome.service Save and exit (Ctrl+X, then Y, then Enter). Your server will now keep its certificate fresh forever! ## Chapter 8: Testing Your New Superpowers (DoH \u0026amp; DoT) 🧪 For a direct confirmation, I used these commands on my computer:\nDNS-over-HTTPS (DoH) Test: This test checks if the secure web endpoint for DNS is alive.\ncurl -v [https://your-no-ip-hostname.ddns.net/dns-query](https://your-no-ip-hostname.ddns.net/dns-query) I got a \u0026ldquo;405 Method Not Allowed\u0026rdquo; error, which sounds bad but is actually great news. It means I successfully connected to the server, which correctly told me I didn\u0026rsquo;t send a real query. The connection works!\nDNS-over-TLS (DoT) Test: This checks the dedicated secure port for DNS. I used a tool called kdig.\n# I had to install it first with: sudo apt install knot-dnsutils kdig @your-no-ip-hostname.ddns.net +tls-ca +tls-host=your-no-ip-hostname.ddns.net example.com The command returned a perfect DNS answer for example.com, confirming the secure tunnel was working.\n## Chapter 9: Protecting Your Kingdom (Router \u0026amp; Phone Setup) 🏰 Now, let\u0026rsquo;s point your devices to their new guardian.\nOn Your Home Router: Log in to your router\u0026rsquo;s admin page, find the DNS settings, and enter your server\u0026rsquo;s Elastic IP as the primary DNS server. Leave the secondary field blank! This forces all devices on your Wi-Fi to be protected. Then, restart your router. On Your Mobile Phone: Android: Go to Settings -\u0026gt; Network -\u0026gt; Private DNS. Choose \u0026ldquo;Private DNS provider hostname\u0026rdquo; and enter your No-IP hostname (my-dns.ddns.net). This gives you ad-blocking everywhere, even on cellular data! iOS: You can use a profile to configure DoH. A simple way is to use a site like AdGuard\u0026rsquo;s DNS profile generator, but enter your own server\u0026rsquo;s DoH address (https://my-dns.ddns.net/dns-query). ## Chapter 10: The Ultimate Safety Net (Creating a Snapshot) 📸 Finally, let\u0026rsquo;s back up our perfect setup.\nIn the EC2 Console, go to your instance details. Click the \u0026ldquo;Storage\u0026rdquo; tab and click the \u0026ldquo;Volume ID\u0026rdquo;. Click \u0026ldquo;Actions\u0026rdquo; -\u0026gt; \u0026ldquo;Create snapshot\u0026rdquo;. Give it a description, like AdGuard-Working-Setup-Backup. If you ever mess something up, you can use this snapshot to restore your server to this exact working state in minutes.\n## Bonus Chapter: Common Troubleshooting Tips If things aren\u0026rsquo;t working, here are a few common pitfalls to check:\nBrowser Overrides Everything: If one device isn\u0026rsquo;t blocking ads, check its browser settings! Modern browsers like Chrome have a \u0026ldquo;Secure DNS\u0026rdquo; feature that can bypass your custom setup. You may need to turn this off. Check Your Laptop\u0026rsquo;s DNS: Make sure your computer\u0026rsquo;s network settings are set to \u0026ldquo;Obtain DNS automatically\u0026rdquo; so it listens to the router. A manually set DNS on your PC will ignore the router\u0026rsquo;s settings. Beware of IPv6: If you run into trouble on one device, try disabling IPv6 in that device\u0026rsquo;s Wi-Fi adapter properties to force it to use your working IPv4 setup. ## It’s a Wrap! And there you have it! You\u0026rsquo;ve successfully built a personal, secure, ad-blocking DNS server in the cloud. You\u0026rsquo;ve learned about cloud computing, firewalls, DNS, SSL, and automation. Go enjoy a faster, cleaner, and more private internet experience.\n","permalink":"http://localhost:1313/projects/adguard-updated/","summary":"\u003ch1 id=\"your-personal-internet-guardian-how-to-build-a-free-ad-blocker-in-the-cloud-\"\u003eYour Personal Internet Guardian: How to Build a FREE Ad-Blocker in the Cloud! 🚀\u003c/h1\u003e\n\u003cp\u003eHey everyone! A while back, I wrote a guide on setting up AdGuard Home on Linode. The world of tech moves fast, and it\u0026rsquo;s time for an upgrade! Today, we\u0026rsquo;re going to build our own powerful, network-wide ad-blocker using \u003cstrong\u003eAmazon Web Services (AWS)\u003c/strong\u003e, and we\u0026rsquo;ll make it secure with our own domain and SSL certificate.\u003c/p\u003e\n\u003cp\u003eThink of this as building a digital gatekeeper for your internet. Before any ads, trackers, or malicious sites can reach your devices, our AdGuard Home server will slam the door shut. The best part? This works on your phone, laptop, smart TV—anything on your network—without installing a single app on them.\u003c/p\u003e","title":"How I Built My Own Ad-Blocking DNS Server in the Cloud (2025 Updated Edition!)"},{"content":"What\u0026rsquo;s the buzz about AdGuard Home? Imagine AdGuard Home as your personal internet guardian. This versatile tool blocks ads, trackers, and other online nuisances across all devices connected to your network. Whether you\u0026rsquo;re browsing on your phone, tablet, or computer, AdGuard Home has your back.\nIn today\u0026rsquo;s digital landscape, robust security measures are paramount. Protecting each device shields your family from accidental clicks and malicious attacks, ensuring peace of mind and a secure online environment.\nWhy on the Cloud? While setting up AdGuard Home on your home network is great, installing it on a cloud server like Linode takes things up a notch. Here\u0026rsquo;s why:\nOn-the-Go Protection: Your devices stay protected from ads and trackers, no matter where you are, you can even share it with your family. Centralized Control: Manage and customize your ad-blocking settings from a single dashboard. Enhanced Privacy: Keep your browsing data away from prying eyes. Ready to embark on this ad-free adventure? Let\u0026rsquo;s get started!\nSetting Up The Environment Step 1: Create a Linode Cloud Account Why choose Linode? Through NetworkChuck\u0026rsquo;s referral link, you receive a generous $100 cloud credit - a fantastic start!\nSign Up: Navigate to Linode\u0026rsquo;s signup page and register. Access the Dashboard: Log in and select \u0026lsquo;Linodes\u0026rsquo; from the left-side menu. Create a Linode: Click \u0026lsquo;Create Linode,\u0026rsquo; choose your preferred region, and select an operating system (Debian 11 is a solid choice). Choose a Plan: The Shared 1GB Nanode instance is sufficient for AdGuard Home. Label and Secure: Assign a label to your Linode and set a strong root password. Deploy: Click \u0026lsquo;Create Linode\u0026rsquo; and wait for it to initialize. Once your Linode is up and running, access it via the LISH Console or SSH. (use root as localhost login)\nStep 2: Installing AdGuard Home on Linode Yes, we\u0026rsquo;re already into setting up at this point.\nLog In: Access your Linode using SSH or the LISH Console with your root credentials. Update the system: sudo apt update \u0026amp;\u0026amp; apt upgrade -y Go ahead and copy this command to Install Adguard Home: curl -s -S -L https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh | sh -s -- -v AdGuard Home is installed and running. You can use CTRL+Shift+V to paste into the terminal.\nStep 3: Configure AdGuard Home Post-installation, you\u0026rsquo;ll see a list of IP addresses with port :3000. Access the Web Interface: Open your browser and navigate to the IP address followed by :3000. If you encounter a security warning, proceed by clicking \u0026ldquo;Continue to site.\u0026rdquo; Initial Setup: Click \u0026lsquo;Get Started\u0026rsquo; and follow the prompts. When uncertain, default settings are typically fine. Set Credentials: Set up the Username and Password. Step 4: Integrate AdGuard Home with Your Router After this, your AdGuard Home is running, but in order to use it on your devices you need to set up inside your home router for all your devices to be protected. For that, I can\u0026rsquo;t walk you through each and every router\u0026rsquo;s settings, but the steps are pretty similar.\nFind your router IP address, you should be able to find it on the back of your router (commonly 192.168.0.1 or 192.169.1.1) enter it into your browser. Login into your router using the credentials mentioned in the back of your router; the default is often admin for both username and password. I suggest you change your default password. Configure DNS Settings: Enable DHCP Server: Ensure your router\u0026rsquo;s DHCP server is active. Set DNS Addresses: Input your AdGuard Home server\u0026rsquo;s IP as the primary DNS (mine was 96.126.113.207). For secondary DNS, options like 1.1.1.1 (Cloudflare), 9.9.9.9 (Quad9), or 8.8.8.8 (Google) are reliable. Save and apply the changes. Fine-Tuning AdGuard Home If you\u0026rsquo;ve done everything till here you should be good, but for those who enjoy customizations, AdGuard Home offers a plethora of settings. Some of the customization I did are:\nSettings Go to Settings -\u0026gt; General Settings: You can enable Parental Control and Safe Search. You can also make your Statistics last longer than 24hrs which is default. Now on Settings -\u0026gt; DNS Settings By default, it uses DNS from quad9 which is pretty good but I suggest you add more. You can click on the list of known DNS providers, which you can choose from. I used: https://dns.quad9.net/dns-query https://dns.google/dns-query https://dns.cloudflare.com/dns-query Enable \u0026lsquo;Load Balancing\u0026rsquo; to distribute queries evenly. Scroll down to \u0026lsquo;DNS server configuration\u0026rsquo; and enable DNSSEC for enhanced security. Click on Save. Filters DNS blocklists Go to Filters -\u0026gt; DNS blocklists, here you can add a blocklist that people have created and use it to block even more things. By default, AdGuard uses the AdGuard DNS filter, and you can add more.\nClick on Add blocklist -\u0026gt; Choose from the list Don\u0026rsquo;t choose too many from the list cause it may slow your internet requests. These are the blocklists I added. And just like that you are blocking more and more things. DNS rewrites Go to Filters -\u0026gt; DNS rewrites, here you can add your own DNS entries, so I added AdGuard here.\nClick on Add DNS rewrite Type in domain adguardforme.local and your IP address for AdGuard Home. And save it. Now, when I want to go on the AdGuard Home dashboard I just type in adguardforme.local and I\u0026rsquo;m into AdGuard, I don\u0026rsquo;t have to remember the IP address.\nCustom filtering rules Go to Filters -\u0026gt; Custom filtering rules. For some reason when I use Facebook on mobile device stories and videos did not load up, so I added custom filtering rules.\n@@||graph.facebook.com^$important ","permalink":"http://localhost:1313/projects/adguard-home-on-cloud/","summary":"\u003ch1 id=\"whats-the-buzz-about-adguard-home\"\u003eWhat\u0026rsquo;s the buzz about AdGuard Home?\u003c/h1\u003e\n\u003cp\u003eImagine AdGuard Home as your personal internet guardian. This versatile tool blocks ads, trackers, and other online nuisances across all devices connected to your network. Whether you\u0026rsquo;re browsing on your phone, tablet, or computer, AdGuard Home has your back.\u003c/p\u003e\n\u003cp\u003eIn today\u0026rsquo;s digital landscape, robust security measures are paramount. Protecting each device shields your family from accidental clicks and malicious attacks, ensuring peace of mind and a secure online environment.\u003c/p\u003e","title":"Running Private Adguard Server on Cloud (Linode)"},{"content":"What\u0026rsquo;s a Docker Container? Before we dive into setting up DeepSeek-R1, let me explain what a Docker container is. Imagine you have a toy that works perfectly on your birthday but gets broken if you move it to another room. A Docker container is like a magic box that keeps your AI model (the toy) in perfect condition wherever you take it, whether it\u0026rsquo;s running as a background task, on a web server, or even in the cloud.\nDocker containers encapsulate everything required to run an application: the code, dependencies, and environment settings. This ensures consistency across different machines, which is super important for AI models that rely on precise configurations.\nSetting Up The Environment Step 1: Install Ubuntu on Windows (If You Haven\u0026rsquo;t Already) If you\u0026rsquo;re using Windows, the easiest way to get an Ubuntu environment is through the Microsoft Store. Here\u0026rsquo;s how:\nOpen the Microsoft Store and search for Ubuntu. Click Get and let it install. Once installed, open Ubuntu from the Start menu and follow the setup instructions. Update the system: sudo apt update \u0026amp;\u0026amp; sudo apt upgrade Now, you have an Ubuntu terminal running on Windows!\nStep 2: Install Docker (If You Haven\u0026rsquo;t Already) First, let\u0026rsquo;s check if you have Docker installed. Open a terminal and run:\ndocker --version If that returns a version number, congrats! If not, install Docker:\nsudo apt update \u0026amp;\u0026amp; sudo apt install docker.io -y sudo systemctl enable --now docker Step 3: Prerequisites for NVIDIA GPU Install NVIDIA Container Toolkit:\nConfiguring the production repository: curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list Update the package list: sudo apt-get update Install the NVIDIA Container Toolkit: sudo apt-get install -y nvidia-container-toolkit Running Ollama Inside Docker Run these commands(P.S. shoutout to NetworkChuck):\ndocker run -d \\ --gpus all \\ -v ollama:/root/.ollama \\ -p 11434:11434 \\ --security-opt=no-new-privileges \\ --cap-drop=ALL \\ --cap-add=SYS_NICE \\ --memory=8g \\ --memory-swap=8g \\ --cpus=4 \\ --read-only \\ --name ollama \\ ollama/ollama Running DeepSeek-R1 Locally Time to bring DeepSeek-R1 to life locally and containerized:\ndocker exec -it ollama ollama run deepseek-r1 or you can run other versions of deepseek-r1 just by typing in the version at the end after a colon(:)\ndocker exec -it ollama ollama run deepseek-r1:7b After this, play around with the AI, if you wanna exit just type:\n/bye Starting Deepseek-R1 To Start Deepseek-R1 from next time go to Ubuntu and type:\ndocker start ollama this will start ollama docker container; then type:\ndocker exec -it ollama ollama run deepseek-r1:7b ","permalink":"http://localhost:1313/projects/running-deepseek-r1-on-docker-container-on-ubuntu/","summary":"\u003ch1 id=\"whats-a-docker-container\"\u003eWhat\u0026rsquo;s a Docker Container?\u003c/h1\u003e\n\u003cp\u003eBefore we dive into setting up DeepSeek-R1, let me explain what a Docker container is. Imagine you have a toy that works perfectly on your birthday but gets broken if you move it to another room. A Docker container is like a magic box that keeps your AI model (the toy) in perfect condition wherever you take it, whether it\u0026rsquo;s running as a background task, on a web server, or even in the cloud.\u003c/p\u003e","title":"Dive into AI Fun: Running DeepSeek-R1 on a Docker Container on Ubuntu"},{"content":"Description I joined AbbVie initially as a contractor and quickly demonstrated the skills and dedication that led to my conversion to a full-time position. In my role, I stepped into a high-stakes production environment where precision and operational stability are paramount. My work centered on the optimization and maintenance of sophisticated, machine learning-based visual inspection systems. I was responsible for fine-tuning these models, analyzing their performance data, and troubleshooting complex technical issues across both hardware and software, including the POM and PCE systems.\nThis wasn\u0026rsquo;t just about keeping machines running; it was about enhancing them. By applying a systematic, data-driven approach, I contributed to a 30% reduction in product waste, a metric that translates directly to improved efficiency and sustainability. Working within strict GMPs and utilizing systems like SAP for material tracking, I learned to balance technical problem-solving with rigorous compliance, ensuring that every action contributed to the stability and reliability of mission-critical operations.\n","permalink":"http://localhost:1313/experience/abbvie/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eI joined AbbVie initially as a contractor and quickly demonstrated the skills and dedication that led to my conversion to a full-time position. In my role, I stepped into a high-stakes production environment where precision and operational stability are paramount. My work centered on the optimization and maintenance of sophisticated, machine learning-based visual inspection systems. I was responsible for fine-tuning these models, analyzing their performance data, and troubleshooting complex technical issues across both hardware and software, including the POM and PCE systems.\u003c/p\u003e","title":"Operator III"},{"content":"Description As my first professional role after moving to the United States, my position at FedEx was a crucial step in adapting my technical skills to a new corporate environment. My journey began as a contractor, where my performance and analytical skills in a fast-paced setting led to my transition to a full-time Associate role. At the device testing center, I was on the front lines of quality assurance for a wide array of consumer electronics. I conducted comprehensive, systematic testing on mobile devices, smartwatches, and routers, executing detailed test plans to identify hardware vulnerabilities, software bugs, and non-compliance with network standards.\nMy responsibilities included meticulously documenting my findings, reproducing bugs to assist developers, and providing clear, actionable reports to engineering teams. This collaborative process was crucial in accelerating the repair cycle and ensuring that products met the highest standards of quality and security before reaching the market. The role sharpened my analytical skills and gave me a deep appreciation for the importance of rigorous testing in the software development lifecycle.\n","permalink":"http://localhost:1313/experience/fedex/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs my first professional role after moving to the United States, my position at FedEx was a crucial step in adapting my technical skills to a new corporate environment. My journey began as a contractor, where my performance and analytical skills in a fast-paced setting led to my transition to a full-time Associate role. At the device testing center, I was on the front lines of quality assurance for a wide array of consumer electronics. I conducted comprehensive, systematic testing on mobile devices, smartwatches, and routers, executing detailed test plans to identify hardware vulnerabilities, software bugs, and non-compliance with network standards.\u003c/p\u003e","title":"Product Testing Associate"},{"content":"Description As the IT Support Specialist for a bustling international college with over 2,500 students and staff, I was at the heart of the campus\u0026rsquo;s technical operations. My role was dynamic and comprehensive, involving end-to-end technical support across a diverse, multi-building campus. I managed the entire user lifecycle, from onboarding new accounts to ensuring smooth system setups across Windows, Linux, and Mac environments. I was the primary point of contact for all technical challenges, resolving Tier 1 and 2 support tickets with a 90% SLA adherence and troubleshooting complex OS issues to minimize downtime.\nMy tenure was marked by significant growth and adaptation. I led the complete technical setup of eight new computer labs, managing everything from hardware deployment and network cabling to software installation and configuration. When the COVID-19 pandemic hit, I was instrumental in transitioning the campus to a hybrid learning model, my first professional experience navigating such a large-scale shift. This required rapidly scaling our remote support capabilities and ensuring both students and faculty could operate effectively from anywhere.\nA cornerstone project of my time was the complete technical overhaul of the newly acquired Kumari Film Hall. I was deeply involved in the project to transform the old cinema into modern lecture halls, which included designing and deploying the entire network infrastructure, setting up AV systems, and ensuring seamless integration with the main campus network.\nTo support these expanding operations, I took the lead in deploying a new UVDesk help desk ticketing system on a CentOS server and embraced automation, utilizing tools like OK Goldy to streamline user creation in Google Workspace. These initiatives standardized processes, improved efficiency, and allowed our team to successfully manage the college\u0026rsquo;s ambitious growth.\n","permalink":"http://localhost:1313/experience/islingtoncollege/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs the IT Support Specialist for a bustling international college with over 2,500 students and staff, I was at the heart of the campus\u0026rsquo;s technical operations. My role was dynamic and comprehensive, involving end-to-end technical support across a diverse, multi-building campus. I managed the entire user lifecycle, from onboarding new accounts to ensuring smooth system setups across Windows, Linux, and Mac environments. I was the primary point of contact for all technical challenges, resolving Tier 1 and 2 support tickets with a 90% SLA adherence and troubleshooting complex OS issues to minimize downtime.\u003c/p\u003e","title":"IT Support Specialist"},{"content":"Description Building on my foundational experience, my internship at BlackBox Technologies immersed me in a more complex, project-based environment. I was an integral part of a development team tasked with building a web-based attendance system from the ground up. This role provided me with invaluable hands-on, full-stack experience. I contributed to the backend by assisting senior engineers with the development of business logic in .NET, giving me insight into server-side architecture. Simultaneously, I was responsible for building responsive, user-facing components for the front-end using HTML, CSS, and JavaScript.\nThis experience was a deep dive into the software development lifecycle. I learned how to translate business requirements into technical specifications, participated in code reviews, and understood the synergy between front-end and back-end systems. Working in close collaboration with the engineering team on a single, focused product was an excellent opportunity to apply my skills to a real-world project and solidify my understanding of creating robust, scalable web applications.\n","permalink":"http://localhost:1313/experience/blackbox/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eBuilding on my foundational experience, my internship at BlackBox Technologies immersed me in a more complex, project-based environment. I was an integral part of a development team tasked with building a web-based attendance system from the ground up. This role provided me with invaluable hands-on, full-stack experience. I contributed to the backend by assisting senior engineers with the development of business logic in .NET, giving me insight into server-side architecture. Simultaneously, I was responsible for building responsive, user-facing components for the front-end using HTML, CSS, and JavaScript.\u003c/p\u003e","title":"Web Development Intern"},{"content":"Description My journey into professional software development began at Radiant Infotech, my first internship and job in the tech industry. This role was a pivotal transition from academic theory to real-world application. I was entrusted with supporting the full lifecycle of client websites, which provided an immersive learning experience. My primary responsibility was to develop responsive, pixel-perfect front-end interfaces using HTML, CSS, and Bootstrap, translating design files into functional web components. A key part of this process was using Adobe Photoshop to prepare and optimize web graphics, ensuring both aesthetic quality and optimal performance.\nBeyond the initial development, my role extended to managing website content through various CMS platforms and performing rigorous debugging to ensure cross-browser compatibility and a seamless user experience. This foundational internship was crucial in building my confidence and skills in modern web development, teaching me how to collaborate effectively within a team to deliver high-quality digital products for clients.\n","permalink":"http://localhost:1313/experience/radiantinfotech/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eMy journey into professional software development began at Radiant Infotech, my first internship and job in the tech industry. This role was a pivotal transition from academic theory to real-world application. I was entrusted with supporting the full lifecycle of client websites, which provided an immersive learning experience. My primary responsibility was to develop responsive, pixel-perfect front-end interfaces using HTML, CSS, and Bootstrap, translating design files into functional web components. A key part of this process was using Adobe Photoshop to prepare and optimize web graphics, ensuring both aesthetic quality and optimal performance.\u003c/p\u003e","title":"Web Development Intern"},{"content":"Introduction Welcome to my personal portfolio website. I respect your privacy and am committed to protecting it. This policy outlines what information is collected when you visit my site and how that information is used.\nInformation Collection and Use I collect information in two ways: information you provide directly and anonymous data collected by analytics services.\nPersonal Data You Provide When you request a copy of my resume, you are asked to voluntarily provide your email address.\nHow it\u0026rsquo;s collected: This information is collected via an embedded Google Form. Why it\u0026rsquo;s collected: It is used for the sole purpose of sending the requested resume document to you through an automated process managed by Google Apps Script. How it\u0026rsquo;s used: Your email will not be used for marketing purposes, sold, or shared with any third parties. Anonymous Usage Data To improve the user experience and analyze traffic, this website uses the following third-party services:\nUmami (Self-Hosted): This website uses a self-hosted instance of Umami for privacy-focused web analytics. Umami collects anonymous usage data such as page views, referrers, and geographic regions to help me understand website traffic. This service does not use cookies, does not collect any personally identifiable information, and all data is stored on a private server under my control.\nCloudflare Web Analytics: This service collects anonymous traffic data such as page views and country of origin. It does not use cookies or collect personally identifiable information. You can view their privacy policy here.\nService Providers This website relies on the following third-party service providers to function:\nGoogle Workspace (Forms, Sheets, Apps Script): Used to manage and automate resume requests. Cloudflare: Used as a Content Delivery Network (CDN) to improve website performance, as a security firewall to protect against malicious attacks, and for collecting anonymous web analytics. Vercel: Used to host the self-hosted Umami analytics application. Netlify \u0026amp; GitHub: Used for hosting and deploying the website. Changes to This Privacy Policy I may update this Privacy Policy from time to time. I will notify you of any changes by posting the new Privacy Policy on this page. You are advised to review this page periodically for any changes.\nContact Me If you have any questions about this Privacy Policy, please contact me at: prajwolad18@gmail.com\n","permalink":"http://localhost:1313/privacy-policy/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWelcome to my personal portfolio website. I respect your privacy and am committed to protecting it. This policy outlines what information is collected when you visit my site and how that information is used.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"information-collection-and-use\"\u003eInformation Collection and Use\u003c/h2\u003e\n\u003cp\u003eI collect information in two ways: information you provide directly and anonymous data collected by analytics services.\u003c/p\u003e\n\u003ch3 id=\"personal-data-you-provide\"\u003ePersonal Data You Provide\u003c/h3\u003e\n\u003cp\u003eWhen you request a copy of my resume, you are asked to voluntarily provide your email address.\u003c/p\u003e","title":"Privacy Policy"},{"content":"Introduction Welcome to the part 4 of the homelab series! In the previous parts, we built a server, deployed a suite of services, and configured our network. Now, it\u0026rsquo;s time to make it resilient and self-maintaining. A homelab isn\u0026rsquo;t just about setting things up; it\u0026rsquo;s about keeping them running reliably.\nThis guide will show you how to set up the three pillars of modern IT operations: Automated Backups, Automated Updates, and Proactive Alerting. By the end, you\u0026rsquo;ll have a homelab that runs itself, ensures your data is safe, stays up-to-date, and notifies you when something goes wrong.\nChapter 1: The Automated Backup Strategy (at 3 AM) A solid backup strategy is non-negotiable. I implemented a robust system inspired by the \u0026ldquo;3-2-1\u0026rdquo; rule, focusing on redundancy and an off-site copy. My strategy involves maintaining two copies of my data in two separate locations: one local backup on the server itself for fast recovery, and one automated, off-site backup to Google Drive to protect against a local disaster like a fire or hardware failure.\nThis script runs at 3 AM, creates a local backup, uploads it, and then notifies Discord.\nStep 1: Configure rclone for Google Drive First, you need a tool to communicate with Google Drive. We\u0026rsquo;ll use rclone.\nInstall rclone on your Debian server: sudo -v ; curl [https://rclone.org/install.sh](https://rclone.org/install.sh) | sudo bash Run the interactive setup: rclone config Follow the Prompts: n (New remote) * name\u0026gt;: gdrive (You can name it anything) storage\u0026gt;: Find and select drive (Google Drive). client_id\u0026gt; \u0026amp; client_secret\u0026gt;: Press Enter for both to leave blank. scope\u0026gt;: Choose 1 (Full access). Use auto config? y/n\u0026gt;: This is a critical step. Since we are on a headless server, type n and press Enter. Authorize Headless: rclone will give you a command to run on a machine with a web browser (like your main computer). On your main computer (where you have rclone installed), run the rclone authorize \u0026quot;drive\u0026quot; \u0026quot;...\u0026quot; command. This will open your browser, ask you to log in to Google, and grant permission. Your main computer\u0026rsquo;s terminal will then output a block of text (your config_token). Paste Token: Copy the token from your main computer and paste it back into your server\u0026rsquo;s rclone prompt. Finish the prompts, and your connection is complete. Step 2: Create the Backup Script Next, create a shell script to perform the backup.\nCreate the file and make it executable:\nnano ~/backup.sh chmod +x ~/backup.sh Paste in the following script. You must edit the first 7 variables to match your setup.\n#!/bin/bash # --- Configuration --- SOURCE_DIR=\u0026#34;/path/to/your/docker\u0026#34; # \u0026lt;-- Change to your Docker projects directory BACKUP_DIR=\u0026#34;/path/to/your/backups\u0026#34; # \u0026lt;-- Change to your backups folder FILENAME=\u0026#34;homelab-backup-$(date +%Y-%m-%d).tar.gz\u0026#34; LOCAL_RETENTION_DAYS=3 CLOUD_RETENTION_DAYS=3 RCLONE_REMOTE=\u0026#34;gdrive\u0026#34; # \u0026lt;-- Must match your rclone remote name RCLONE_DEST=\u0026#34;Homelab Backups\u0026#34; # \u0026lt;-- Folder name in Google Drive # --- \u0026#34;https://discordapp.com/api/webhooks/141949178941/6Tx6f1yjf26LztQ\u0026#34; --- DISCORD_WEBHOOK_URL=\u0026#34;YOUR_DISCORD_WEBHOOK_URL\u0026#34; # --- Notification Function --- send_notification() { MESSAGE=$1 curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#34;{\\\u0026#34;content\\\u0026#34;: \\\u0026#34;$MESSAGE\\\u0026#34;}\u0026#34; \u0026#34;$DISCORD_WEBHOOK_URL\u0026#34; } # --- Script Logic --- echo \u0026#34;--- Starting Homelab Backup: $(date) ---\u0026#34; send_notification \u0026#34;✅ Starting Homelab Backup...\u0026#34; # 1. Create local backup echo \u0026#34;Creating local backup...\u0026#34; tar -czf \u0026#34;${BACKUP_DIR}/${FILENAME}\u0026#34; -C \u0026#34;${SOURCE_DIR}\u0026#34; . echo \u0026#34;Local backup created at ${BACKUP_DIR}/${FILENAME}\u0026#34; # 2. Upload to Google Drive echo \u0026#34;Uploading backup to ${RCLONE_REMOTE}...\u0026#34; rclone copy \u0026#34;${BACKUP_DIR}/${FILENAME}\u0026#34; \u0026#34;${RCLONE_REMOTE}:${RCLONE_DEST}\u0026#34; echo \u0026#34;Upload complete.\u0026#34; # 3. Clean up local backups echo \u0026#34;Cleaning up local backups older than ${LOCAL_RETENTION_DAYS} days...\u0026#34; find \u0026#34;${BACKUP_DIR}\u0026#34; -type f -name \u0026#34;*.tar.gz\u0026#34; -mtime +${LOCAL_RETENTION_DAYS} -delete echo \u0026#34;Local cleanup complete.\u0026#34; # 4. Clean up cloud backups echo \u0026#34;Cleaning up cloud backups older than ${CLOUD_RETENTION_DAYS} days...\u0026#34; rclone delete \u0026#34;${RCLONE_REMOTE}:${RCLONE_DEST}\u0026#34; --min-age ${CLOUD_RETENTION_DAYS}d echo \u0026#34;Cloud cleanup complete.\u0026#34; echo \u0026#34;Backup process finished.\u0026#34; send_notification \u0026#34;🎉 Homelab backup and cloud upload completed successfully!\u0026#34; Step 3: Automate with Cron To run this script automatically, you must add it to the root user\u0026rsquo;s crontab. This is critical for giving the script permission to read all Docker files.\nOpen the root crontab editor: sudo crontab -e Add the following line to schedule the backup for 3:00 AM every morning: 0 3 * * * /path/to/your/backup.sh You will now get a fresh, onsite and off-site backup every night and a Discord message when it\u0026rsquo;s done. Chapter 2: Automated Updates with Watchtower (at 6 AM) Manually updating every Docker container is tedious. We can automate this by deploying Watchtower.\nStep 1: The Docker Compose File Create a docker-compose.yml for Watchtower. This configuration schedules it to run once a day at 6:00 AM, clean up old images, and send a Discord notification only if it finds an update.\nmkdir -p ~/docker/watchtower\ncd ~/docker/watchtower\nnano docker-compose.yml\nPaste in this configuration:\nservices: watchtower: image: containrrr/watchtower container_name: watchtower restart: unless-stopped volumes: - /var/run/docker.sock:/var/run/docker.sock environment: # Timezone setting TZ: America/Chicago # Discord notification settings WATCHTOWER_NOTIFICATIONS: shoutrrr WATCHTOWER_NOTIFICATION_URL: \u0026#34;discord://YOUR_DISCORD_WEBHOOK_ID_URL\u0026gt; # Notification settings WATCHTOWER_NOTIFICATIONS_LEVEL: info WATCHTOWER_NOTIFICATION_REPORT: \u0026#34;true\u0026#34; WATCHTOWER_NOTIFICATIONS_HOSTNAME: Homelab-Laptop # Update settings WATCHTOWER_CLEANUP: \u0026#34;true\u0026#34; WATCHTOWER_INCLUDE_STOPPED: \u0026#34;false\u0026#34; WATCHTOWER_INCLUDE_RESTARTING: \u0026#34;true\u0026#34; WATCHTOWER_SCHEDULE: \u0026#34;0 0 6 * * *\u0026#34; Note: The WATCHTOWER_NOTIFICATION_URL uses a special shoutrrr format for Discord, which looks like discord://token@webhook-id.\nNow, every morning at 6:00 AM, Watchtower will scan all running containers and update any that have a new image available.\nChapter 3: Proactive Alerting (24/7) The final piece of automation is proactive alerting. This setup ensures you are immediately notified via Discord if something goes wrong.\nStep 1: The Alerting Pipeline The pipeline we\u0026rsquo;ll build is: Prometheus (detects problems) -\u0026gt; Alertmanager (groups and routes alerts) -\u0026gt; Discord (notifies you).\nStep 2: Deploy Alertmanager First, deploy Alertmanager. It must be on the same npm_default network as Prometheus.\nmkdir -p ~/docker/alertmanager\ncd ~/docker/alertmanager\nCreate the alertmanager.yml configuration file:\nnano alertmanager.yml Paste in this configuration. It uses advanced routing to send critical alerts every 2 hours and warning alerts every 12 hours.\nglobal: resolve_timeout: 5m route: group_by: [\u0026#34;alertname\u0026#34;, \u0026#34;severity\u0026#34;] group_wait: 30s group_interval: 10m repeat_interbal: 12h receiver: \u0026#34;discord-notifications\u0026#34; routes: - receiver: \u0026#34;discord-notifications\u0026#34; matchers: - severity=\u0026#34;critical\u0026#34; repeat_interval: 2h - receiver: \u0026#34;discord-notifications\u0026#34; matchers: - severity=\u0026#34;warning\u0026#34; repeat_interval: 12h receivers: - name: \u0026#34;discord-notifications\u0026#34; discord_configs: - webhook_url: \u0026#34;YOUR_DISCORD_WEBHOOK_URL\u0026#34; send_resolved: true Now create the docker-compose.yml for Alertmanager:\nnano docker-compose.yml Paste in the following:\nservices: alertmanager: image: prom/alertmanager:latest container_name: alertmanager restart: unless-stopped volumes: - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml networks: - npm_default networks: npm_default: external: true Launch it: docker compose up -d\nStep 3: Configure Prometheus Finally, tell Prometheus to send alerts to Alertmanager and load your rules.\nCreate your rules file, ~/docker/monitoring/alert_rules.yml, with rules for \u0026ldquo;Instance Down,\u0026rdquo; \u0026ldquo;High CPU,\u0026rdquo; \u0026ldquo;Low Disk Space,\u0026rdquo; etc.\ncd ~/docker/monitoring nano alert_rules.yml Add the alert_rules.yml as a volume in your ~/docker/monitoring/docker-compose.yml.\nvolumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - ./alert_rules.yml:/etc/prometheus/alert_rules.yml - prometheus_data:/prometheus Add the alerting and rule_files blocks to your ~/docker/monitoring/prometheus.yml:\ngroups: -name: Critical System Alerts interval: 30s rules: - alert: InstanceDown expr: up == 0 for: 2m labels: severity: critical annotations: summary: \u0026#34;🔴 Instance {{ $labels.instance }} is DOWN\u0026#34; description: \u0026#34;Service {{ $labels.job }} has been unreachable for 2 minutes.\u0026#34; - alert: LaptopOnBattery expr: node_power_supply_online == 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;🔋 Server running on BATTERY\u0026#34; description: \u0026#34;Homelab has been unplugged for 5 minutes. Check power connection!\u0026#34; - alert: LowBatteryLevel expr: node_power_supply_capacity \u0026lt; 20 and node_power_supply_online == 0 for: 1m labels: severity: critical annotations: summary: \u0026#34;⚠️ CRITICAL: Battery at {{ $value }}%\u0026#34; description: \u0026#34;Battery below 20%. Server may shut down soon!\u0026#34; - alert: DiskAlmostFull expr: (node_filesystem_avail_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;} / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;}) * 100 \u0026lt; 10 for: 5m labels: severity: critical annotations: summary: \u0026#34;💾 Disk space critically low: {{ $value | humanize }}% remaining\u0026#34; description: \u0026#34;Root filesystem has less than 10% free space.\u0026#34; - alert: OutOfMemory expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 \u0026lt; 5 for: 2m labels: severity: critical annotations: summary: \u0026#34;🧠 Memory critically low: {{ $value | humanize }}% available\u0026#34; description: \u0026#34;Less than 5% memory available. System may become unresponsive.\u0026#34; - alert: CriticalCpuTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;coretemp\u0026#34;} \u0026gt; 95 for: 2m labels: severity: critical annotations: summary: \u0026#34;🔥 CRITICAL CPU Temperature: {{ $value }}°C\u0026#34; description: \u0026#34;CPU temperature exceeds 95°C. Thermal throttling or shutdown imminent!\u0026#34; - name: Warning System Alerts interval: 1m rules: - alert: HighCpuUsage expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[5m])) * 100) \u0026gt; 80 for: 5m labels: severity: warning annotations: summary: \u0026#34;⚡ High CPU usage: {{ $value | humanize }}%\u0026#34; description: \u0026#34;CPU usage above 80% for 5 minutes on {{ $labels.instance }}\u0026#34; - alert: HighSystemLoad expr: node_load5 / on(instance) count(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}) by (instance) \u0026gt; 1.5 for: 10m labels: severity: warning annotations: summary: \u0026#34;📊 High system load: {{ $value | humanize }}\u0026#34; description: \u0026#34;5-minute load average is 1.5x CPU cores for 10 minutes.\u0026#34; - alert: HighMemoryUsage expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 \u0026lt; 20 for: 5m labels: severity: warning annotations: summary: \u0026#34;🧠 High memory usage: {{ $value | humanize }}% available\u0026#34; description: \u0026#34;Less than 20% memory available.\u0026#34; - alert: HighCpuTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;coretemp\u0026#34;} \u0026gt; 85 for: 5m labels: severity: warning annotations: summary: \u0026#34;🌡️ High CPU temperature: {{ $value }}°C\u0026#34; description: \u0026#34;CPU temperature above 85°C. Consider improving cooling.\u0026#34; - alert: HighNvmeTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;nvme\u0026#34;} \u0026gt; 65 for: 10m labels: severity: warning annotations: summary: \u0026#34;💿 High NVMe temperature: {{ $value }}°C\u0026#34; description: \u0026#34;NVMe drive temperature above 65°C for 10 minutes.\u0026#34; - alert: DiskSpaceLow expr: (node_filesystem_avail_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;} / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;}) * 100 \u0026lt; 20 for: 10m labels: severity: warning annotations: summary: \u0026#34;💾 Disk space low: {{ $value | humanize }}% remaining\u0026#34; description: \u0026#34;Root filesystem has less than 20% free space.\u0026#34; - alert: HighSwapUsage expr: ((node_memory_SwapTotal_bytes - node_memory_SwapFree_bytes) / node_memory_SwapTotal_bytes * 100) \u0026gt; 50 for: 10m labels: severity: warning annotations: summary: \u0026#34;💱 High swap usage: {{ $value | humanize }}%\u0026#34; description: \u0026#34;Swap usage above 50%. System may be memory-constrained.\u0026#34; # Monitor your USB-C hub ethernet adapter (enx00) - alert: EthernetInterfaceDown expr: node_network_up{device=\u0026#34;enx00\u0026#34;} == 0 for: 2m labels: severity: warning annotations: summary: \u0026#34;🌐 USB-C Ethernet adapter is DISCONNECTED\u0026#34; description: \u0026#34;Your USB-C hub ethernet connection (enx00) is down. Check cable or hub.\u0026#34; - alert: HighNetworkErrors expr: rate(node_network_receive_errs_total{device=\u0026#34;enx00\u0026#34;}[5m]) \u0026gt; 10 or rate(node_network_transmit_errs_total{device=\u0026#34;enx00\u0026#34;}[5m]) \u0026gt; 10 for: 5m labels: severity: warning annotations: summary: \u0026#34;🌐 High network errors on USB-C ethernet\u0026#34; description: \u0026#34;Your ethernet adapter is experiencing high error rate. Check cable quality.\u0026#34; - name: Docker Container Alerts interval: 1m rules: # Simplified alert - just checks if container exporter is working - alert: ContainerMonitoringDown expr: absent(container_last_seen) for: 2m labels: severity: warning annotations: summary: \u0026#34;🐳 Container monitoring is down\u0026#34; description: \u0026#34;cAdvisor or container metrics are not available. Check if containers are being monitored.\u0026#34; - alert: ContainerRestarting expr: rate(container_start_time_seconds[5m]) \u0026gt; 0.01 for: 2m labels: severity: warning annotations: summary: \u0026#34;🐳 Container {{ $labels.name }} is restarting\u0026#34; description: \u0026#34;Container {{ $labels.name }} has restarted recently.\u0026#34; - alert: ContainerHighCpu expr: rate(container_cpu_usage_seconds_total{name!~\u0026#34;.*POD.*\u0026#34;,name!=\u0026#34;\u0026#34;}[5m]) * 100 \u0026gt; 80 for: 10m labels: severity: warning annotations: summary: \u0026#34;🐳 Container {{ $labels.name }} high CPU: {{ $value | humanize }}%\u0026#34; description: \u0026#34;Container CPU usage above 80% for 10 minutes.\u0026#34; Restart Prometheus to apply the changes:\ncd ~/docker/monitoring docker compose up -d --force-recreate prometheus Now, if any service fails or your server\u0026rsquo;s resources run low, you will get an instant notification in Discord.\nStep 3: The Critical Firewall Fix You may find your alerts are not sending. This is often due to a conflict between Docker and ufw.\nOpen the main ufw configuration file:\nsudo nano /etc/default/ufw Change DEFAULT_FORWARD_POLICY=\u0026quot;DROP\u0026quot; to DEFAULT_FORWARD_POLICY=\u0026quot;ACCEPT\u0026quot;.\nReload the firewall:\nsudo ufw reload Restart your containers that need internet access: docker compose restart Now, if any service fails or your server\u0026rsquo;s resources run low, you will get an instant notification in Discord.\nConclusion Our homelab has now truly come to life. It\u0026rsquo;s no longer just a collection of services but a resilient, self-maintaining platform. With automated backups to Google Drive, daily updates via Watchtower, and proactive alerts with Prometheus and Alertmanager, our server can now run 24/7 with minimal manual intervention. We\u0026rsquo;ve built a solid, reliable, and intelligent system.\nBut there\u0026rsquo;s one critical piece still missing: end-to-end security for our local services.\nRight now, we\u0026rsquo;re accessing our dashboards at addresses like http://grafana.local, which browsers flag as \u0026ldquo;Not Secure.\u0026rdquo; What if we could use a real, public domain name for our internal services and get a valid HTTPS certificate, all without opening a single port on our router?\nIn the next part of this series, I\u0026rsquo;ll show you exactly how to do that. We\u0026rsquo;ll dive into an advanced but powerful setup using Cloudflare and Nginx Proxy Manager to bring trusted, zero-exposure SSL to everything we\u0026rsquo;ve built.\nStay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-4-automation-and-alerting/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to the part 4 of the homelab series! In the previous parts, we built a server, deployed a suite of services, and configured our network. Now, it\u0026rsquo;s time to make it resilient and self-maintaining. A homelab isn\u0026rsquo;t just about setting things up; it\u0026rsquo;s about keeping them running reliably.\u003c/p\u003e\n\u003cp\u003eThis guide will show you how to set up the three pillars of modern IT operations: \u003cstrong\u003eAutomated Backups\u003c/strong\u003e, \u003cstrong\u003eAutomated Updates\u003c/strong\u003e, and \u003cstrong\u003eProactive Alerting\u003c/strong\u003e. By the end, you\u0026rsquo;ll have a homelab that runs itself, ensures your data is safe, stays up-to-date, and notifies you when something goes wrong.\u003c/p\u003e","title":"Part 4: Automating a Homelab with Backups, Updates, and Alerts"},{"content":"My New Weekend Project: Building a Personal Ad-Blocking Server in the Cloud! Hey everyone, Prajwol here.\nLike a lot of you, I spend a good chunk of my day online. And lately, it\u0026rsquo;s felt like I\u0026rsquo;m in a constant battle with pop-ups, trackers, and auto-playing video ads. I\u0026rsquo;ve used browser extensions for years, but I wanted a more powerful solution—something that would protect my entire home network, including my phone and smart TV, without needing to install software everywhere.\nSo, I decided to take on a new project: building my very own ad-blocking DNS server in the cloud.\nI\u0026rsquo;d done something similar a while back with Linode, but this time I wanted to dive into the world of Amazon Web Services (AWS) and see if I could build a reliable, secure, and cost-effective setup from scratch. It turned into quite the adventure, involving a late-night session of launching a virtual server, wrestling with firewalls, and securing my own private domain with an SSL certificate.\nThe end result? It\u0026rsquo;s been fantastic. My web pages load noticeably faster, and the general online experience feels so much cleaner and less intrusive. Plus, knowing that I have full control over my own corner of the internet is incredibly satisfying. It\u0026rsquo;s a great feeling to see the query logs fill up with blocked requests for domains I\u0026rsquo;ve never even heard of!\nI documented every single step of my journey, from the first click in the AWS console to the final configuration on my home router. If you\u0026rsquo;re curious about how to build one for yourself, I\u0026rsquo;ve written up a complete, step-by-step guide.\nYou can check out the full project guide here! It was a challenging but really rewarding project. Let me know what you think!\nPublished: Friday, August 22, 2025\n","permalink":"http://localhost:1313/blog/adguard-aws/","summary":"\u003ch1 id=\"my-new-weekend-project-building-a-personal-ad-blocking-server-in-the-cloud\"\u003eMy New Weekend Project: Building a Personal Ad-Blocking Server in the Cloud!\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eLike a lot of you, I spend a good chunk of my day online. And lately, it\u0026rsquo;s felt like I\u0026rsquo;m in a constant battle with pop-ups, trackers, and auto-playing video ads. I\u0026rsquo;ve used browser extensions for years, but I wanted a more powerful solution—something that would protect my entire home network, including my phone and smart TV, without needing to install software everywhere.\u003c/p\u003e","title":"Building a Personal Ad-Blocking Server in the Cloud!"},{"content":"Introduction Welcome to Part 3 of my homelab series! In the previous parts, I built my server and deployed a suite of management and monitoring tools. Now, it\u0026rsquo;s time to build the brain of my network: a robust, redundant, and high-availability DNS system using AdGuard Home that works both at home and on the go.\nIn this detailed guide, I\u0026rsquo;ll walk you through how I deployed a total of three AdGuard Home instances, each with its own unique IP address. I set up a primary resolver on my homelab, a secondary failover resolver in the cloud for my mobile devices, and a tertiary resolver on a separate virtual network for local redundancy.\nChapter 1: The Local Workhorse (Primary DNS) I started by deploying my main, day-to-day DNS resolver on my homelab server.\nStep 1: Deploying AdGuard Home with Docker Compose First, I SSHed into my server, created a directory for the project, and a docker-compose.yml file to define the service.\nmkdir -p ~/docker/adguard-primary cd ~/docker/adguard-primary nano docker-compose.yml I pasted in the following configuration. This runs the AdGuard Home container, maps all the necessary ports for DNS and the web UI, and connects it to the shared npm_default network I set up in Part 2.\nservices: adguardhome: image: adguard/adguardhome:latest container_name: adguard-primary restart: unless-stopped ports: - \u0026#34;53:53/tcp\u0026#34; - \u0026#34;53:53/udp\u0026#34; - \u0026#34;8080:80/tcp\u0026#34; # Web UI - \u0026#34;853:853/tcp\u0026#34; # DNS-over-TLS volumes: - ./workdir:/opt/adguardhome/work - ./confdir:/opt/adguardhome/conf networks: - npm_default networks: npm_default: external: true I then launched the container by running:\ndocker compose up -d Step 2: Initial AdGuard Home Setup Wizard I navigated to http://\u0026lt;your-server-ip\u0026gt;:3000 in my web browser to start the setup wizard.\nI clicked \u0026ldquo;Get Started.\u0026rdquo;\nOn the \u0026ldquo;Admin Web Interface\u0026rdquo; screen, I changed the \u0026ldquo;Listen Interface\u0026rdquo; to All interfaces and the port to 80.\nOn the \u0026ldquo;DNS server\u0026rdquo; screen, I changed the \u0026ldquo;Listen Interface\u0026rdquo; to All interfaces and left the port as 53.\nI followed the prompts to create my admin username and password.\nOnce the setup was complete, I was redirected to my main dashboard, now available at http://\u0026lt;your-server-ip\u0026gt;:8080.\nStep 3: Configure My Home Router To make all my devices use AdGuard automatically, I logged into my home router\u0026rsquo;s admin panel, found the DHCP Server settings, and changed the Primary DNS Server to my homelab\u0026rsquo;s static IP address (e.g., 192.168.1.10).\nChapter 2: The Cloud Failover (Secondary DNS on Oracle Cloud) An off-site DNS server ensures I have ad-blocking on my mobile devices and acts as a backup.\nWhy I Chose Oracle Cloud After testing the free tiers of both AWS and Linode, I chose Oracle Cloud Infrastructure (OCI). In my experience, OCI\u0026rsquo;s \u0026ldquo;Always Free\u0026rdquo; tier is far more generous with its resources. It provides powerful Ampere A1 Compute instances with up to 4 CPU cores and 24 GB of RAM, plus 200 GB of storage and significant bandwidth, all for free. This was ideal for running my service 24/7 without the strict limitations or eventual costs associated with other providers.\nStep 1: Launching the Oracle Cloud VM Sign Up: I created my account on the Oracle Cloud website.\nCreate VM Instance: In the OCI console, I navigated to Compute \u0026gt; Instances and clicked \u0026ldquo;Create instance\u0026rdquo;.\nConfigure Instance:\nName: I gave it a name like AdGuard-Cloud.\nImage and Shape: I clicked \u0026ldquo;Edit\u0026rdquo;. For the image, I selected Ubuntu. For the shape, I selected \u0026ldquo;Ampere\u0026rdquo; and chose the VM.Standard.A1.Flex shape (it\u0026rsquo;s \u0026ldquo;Always Free-eligible\u0026rdquo;).\nNetworking: I used the default VCN and made sure \u0026ldquo;Assign a public IPv4 address\u0026rdquo; was checked.\nSSH Keys: I added my SSH public key.\nI clicked Create. Once the instance was running, I took note of its Public IP Address.\nStep 2: Configuring the Cloud Firewall For maximum security, I locked down the administrative ports to only my home IP address.\nFind My Public IP: I went to a site like whatismyip.com and copied my home\u0026rsquo;s public IP address.\nEdit Security List: I navigated to my instance\u0026rsquo;s details page, clicked the subnet link, then clicked the \u0026ldquo;Security List\u0026rdquo; link.\nI clicked \u0026ldquo;Add Ingress Rules\u0026rdquo; and added the following rules:\nFor SSH (Port 22): I set the Source to my home\u0026rsquo;s public IP, followed by /32 (e.g., 203.0.113.55/32). This is a critical security step.\nFor AdGuard Setup (Port 3000): I also set the Source to my home\u0026rsquo;s public IP with /32.\nFor AdGuard Web UI (Port 80/443): I set the Source to my home\u0026rsquo;s public IP with /32 as well.\nFor Public DNS (Port 53, 853, etc.): I set the Source to 0.0.0.0/0 (Anywhere) to allow all my devices to connect from any network.\nStep 3: Installing AdGuard Home \u0026amp; Configuring SSL Connect via SSH: I used the public IP and my SSH key to connect to the VM.\nRun Install Script: I chose to install AdGuard Home directly on the OS for this instance.\ncurl -s -S -L [https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh](https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh) | sh -s -- -v Get a Hostname: I went to No-IP.com, created a free hostname (e.g., my-cloud-dns.ddns.net), and pointed it to my cloud VM\u0026rsquo;s public IP.\nEnable Encryption: In my cloud AdGuard\u0026rsquo;s dashboard (Settings \u0026gt; Encryption settings), I enabled encryption, entered my No-IP hostname, and used the built-in function to request a Let\u0026rsquo;s Encrypt certificate.\nStep 4: Creating a Cloud Backup (Snapshot) A critical final step for any cloud service is creating a backup. Here is how I did it in OCI:\nIn the OCI Console, I navigated to the details page for my AdGuard-Cloud instance.\nUnder the \u0026ldquo;Resources\u0026rdquo; menu on the left, I clicked on \u0026ldquo;Boot volume\u0026rdquo;.\nOn the Boot Volume details page, under \u0026ldquo;Resources,\u0026rdquo; I clicked \u0026ldquo;Boot volume backups\u0026rdquo;.\nI clicked the \u0026ldquo;Create boot volume backup\u0026rdquo; button.\nI gave the backup a descriptive name (e.g., AdGuard-Cloud-Backup-YYYY-MM-DD) and clicked the create button. This creates a full snapshot of my server that I can use to restore it in minutes.\nStep 5: ### How to Use Your Cloud DNS on Mobile Devices The main benefit of the cloud server is having ad-blocking on the go. Here’s how I set it up on my mobile phone using secure, encrypted DNS.\nFor Android (Version 9+): Modern Android has a built-in feature called \u0026ldquo;Private DNS\u0026rdquo; that uses DNS-over-TLS (DoT), which is perfect for this.\nOpen Settings on your Android device.\nTap on \u0026ldquo;Network \u0026amp; internet\u0026rdquo; (this may be called \u0026ldquo;Connections\u0026rdquo; on some devices).\nFind and tap on \u0026ldquo;Private DNS\u0026rdquo;. You may need to look under an \u0026ldquo;Advanced\u0026rdquo; section.\nSelect the option labeled \u0026ldquo;Private DNS provider hostname\u0026rdquo;.\nIn the text box, enter the No-IP hostname you created for your Oracle Cloud server (e.g., my-cloud-dns.ddns.net).\nTap Save.\nYour phone will now send all its DNS queries through an encrypted tunnel to your personal AdGuard Home server in the cloud, giving you ad-blocking on both Wi-Fi and cellular data.\nFor iOS (iPhone/iPad): On iOS, the easiest way to set up encrypted DNS is by installing a configuration profile.\nOn your iPhone or iPad, open Safari.\nGo to a DNS profile generator site, like the one provided by AdGuard.\nWhen prompted, enter the DNS-over-HTTPS (DoH) address for your cloud server. It will be your No-IP hostname with /dns-query at the end (e.g., https://my-cloud-dns.ddns.net/dns-query).\nDownload the generated configuration profile.\nGo to your device\u0026rsquo;s Settings app. You will see a new \u0026ldquo;Profile Downloaded\u0026rdquo; item near the top. Tap on it.\nFollow the on-screen prompts to Install the profile. You may need to enter your device passcode.\nOnce installed, your iOS device will also route its DNS traffic through your secure cloud server.\nChapter 3: Ultimate Local Redundancy (Tertiary DNS with Macvlan) For an extra layer of redundancy within my homelab, I created a third AdGuard instance. By using an advanced Docker network mode called macvlan, this container gets its own unique IP address on my home network, making it a truly independent resolver.\nCreate Macvlan Network: First, I created the macvlan network, telling it which of my physical network cards to use (eth0 in my case).\ndocker network create -d macvlan \\ --subnet=192.168.1.0/24 \\ --gateway=192.168.1.1 \\ -o parent=eth0 homelab_net Deploy Tertiary Instance: I created a new folder (~/docker/adguard-tertiary) and this docker-compose.yml. Notice there are no ports since the container gets its own IP.\nservices: adguardhome2: image: adguard/adguardhome:latest container_name: adguardhome2 volumes: - \u0026#34;./work:/opt/adguardhome/work\u0026#34; - \u0026#34;./conf:/opt/adguardhome/conf\u0026#34; networks: homelab_net: ipv4_address: 192.168.1.11 # The new, unique IP for this container restart: unless-stopped networks: homelab_net: external: true Configure Router for Local Failover: To complete the local redundancy, I went back into my router\u0026rsquo;s DHCP settings.\nIn the Primary DNS field, I have the IP of my main homelab server (e.g., 192.168.1.10).\nIn the Secondary DNS field, I entered the unique IP address I assigned to my macvlan container (e.g., 192.168.1.11).\nNow, if my primary AdGuard container has an issue, all devices on my network will automatically fail over to the tertiary instance.\nChapter 4: Fine-Tuning and Integration Finally, I implemented some best practices on my primary AdGuard Home instance.\nUpstream DNS Servers: Under Settings \u0026gt; DNS Settings, I configured AdGuard to send requests to multiple resolvers in parallel for speed and reliability, using Cloudflare (1.1.1.1), Google (8.8.8.8), and Quad9 (9.9.9.9).\nEnable DNSSEC: In the same settings page, I enabled DNSSEC to verify the integrity of DNS responses.\nDNS Blocklists: I added several popular lists from the \u0026ldquo;Filters \u0026gt; DNS blocklists\u0026rdquo; page, including the AdGuard DNS filter and the OISD Blocklist, for robust protection.\nDNS Rewrites for Local Services: This is the key to a clean homelab experience. For each service, I performed a detailed two-step process:\nCreate the Proxy Host in Nginx Proxy Manager: I logged into my NPM admin panel, went to Hosts \u0026gt; Proxy Hosts, and clicked \u0026ldquo;Add Proxy Host\u0026rdquo;. For my Homer dashboard, I set the Forward Hostname to homer (the container name) and the Forward Port to 8080 (its internal port), using homer.local as the domain name.\nCreate the DNS Rewrite in AdGuard Home: I logged into my primary AdGuard dashboard, went to Filters \u0026gt; DNS Rewrites, and clicked \u0026ldquo;Add DNS rewrite\u0026rdquo;. I entered homer.local as the domain and the IP address of my Nginx Proxy Manager server as the answer.\nConclusion I\u0026rsquo;ve now built an incredibly robust, multi-layered DNS infrastructure. My home devices use the primary local server, which is backed up by a second, independent local server, and my mobile devices use a completely separate cloud instance for on-the-go protection. This provides a resilient, secure, and ad-free internet experience.\nIn the final part of this series, we\u0026rsquo;ll shift our focus from deploying services to maintaining them. I\u0026rsquo;ll show you how I set up a fully automated operations pipeline for my homelab, including daily off-site backups, automatic container updates with Watchtower, and proactive alerting with Prometheus. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-3-high-availability-dns/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to Part 3 of my homelab series! In the previous parts, I built my server and deployed a suite of management and monitoring tools. Now, it\u0026rsquo;s time to build the brain of my network: a robust, redundant, and high-availability DNS system using \u003cstrong\u003eAdGuard Home\u003c/strong\u003e that works both at home and on the go.\u003c/p\u003e\n\u003cp\u003eIn this detailed guide, I\u0026rsquo;ll walk you through how I deployed a total of \u003cstrong\u003ethree\u003c/strong\u003e AdGuard Home instances, each with its own unique IP address. I set up a primary resolver on my homelab, a secondary failover resolver in the cloud for my mobile devices, and a tertiary resolver on a separate virtual network for local redundancy.\u003c/p\u003e","title":"Part 3: A High-Availability DNS Network with AdGuard Home"},{"content":"New Project Alert: Running a Powerful AI Locally with Docker! Hey everyone, Prajwol here.\nI\u0026rsquo;ve always been fascinated by the incredible advancements in AI and large language models. While cloud-based models are powerful, I was really curious about what it would take to run a high-performance model right on my own machine. This gives you ultimate privacy, control, and the ability to experiment without limits.\nSo, for my latest project, I decided to dive in and get the DeepSeek-R1 model, a powerful AI, up and running locally using Docker.\nDocker is an amazing tool that lets you package up applications and all their dependencies into a neat little box called a container. This means you can run complex software without the headache of complicated installations or conflicts with other programs on your system. It was the perfect way to tame this powerful AI and get it running smoothly on my Ubuntu machine.\nThe process was a fantastic learning experience, covering everything from setting up Docker to pulling the model and interacting with the AI. It’s amazing to have that kind of power running on your own hardware.\nI’ve documented my entire process in a detailed, step-by-step guide. If you’re interested in local AI and want to see how you can run a powerful model yourself, be sure to check it out!\nYou can find the full project guide right here! Let me know what you think of this one!\nPublished: February, 2025\n","permalink":"http://localhost:1313/blog/running-deepseek-r1-on-docker-container-on-ubuntu/","summary":"\u003ch1 id=\"new-project-alert-running-a-powerful-ai-locally-with-docker\"\u003eNew Project Alert: Running a Powerful AI Locally with Docker!\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve always been fascinated by the incredible advancements in AI and large language models. While cloud-based models are powerful, I was really curious about what it would take to run a high-performance model right on my own machine. This gives you ultimate privacy, control, and the ability to experiment without limits.\u003c/p\u003e\n\u003cp\u003eSo, for my latest project, I decided to dive in and get the \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e model, a powerful AI, up and running locally using \u003cstrong\u003eDocker\u003c/strong\u003e.\u003c/p\u003e","title":"Running a Powerful AI Locally with Docker!"},{"content":"Introduction Welcome to Part 2 of my homelab series! In Part 1, we built a solid foundation by turning an old laptop into a hardened Debian server with Docker. Now that our server is running, we need to deploy services to manage, monitor, and easily access our projects.\nIn this guide, we\u0026rsquo;ll deploy three essential stacks. First, Nginx Proxy Manager (NPM) will act as our server\u0026rsquo;s front door and create a shared network for our containers. Second, we\u0026rsquo;ll set up a professional-grade monitoring stack with Prometheus and Grafana. Finally, we\u0026rsquo;ll deploy a Homer dashboard to create a beautiful and convenient launchpad for all our services.\n1. The Management Layer: Nginx Proxy Manager (NPM) 🌐 Before we can deploy our other services, we need a way to manage connections between them. NPM will act as our reverse proxy and, crucially, will create the shared Docker network that all our other services will connect to.\nA. Deploy Nginx Proxy Manager First, let\u0026rsquo;s create a directory and the docker-compose.yml file for NPM.\n# Create the directory mkdir -p ~/docker/npm cd ~/docker/npm # Create the docker-compose.yml nano docker-compose.yml Paste in the following configuration. This file defines the NPM service and creates a network named npm_default.\nservices: app: image: \u0026#39;jc21/nginx-proxy-manager:latest\u0026#39; container_name: npm-app-1 restart: unless-stopped ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;81:81\u0026#39; volumes: - ./data:/data - ./letsencrypt:/etc/letsencrypt networks: default: name: npm_default Launch it with\ndocker compose up -d You can now log in to the admin UI at http://\u0026lt;your-server-ip\u0026gt;:81.\n2. The Monitoring Stack 📊 With our shared network in place, we can now deploy our monitoring stack.\nPrometheus: Collects all the metrics.\nNode Exporter: Exposes the server\u0026rsquo;s hardware metrics.\ncAdvisor: Exposes Docker container metrics.\nGrafana: Visualizes all the data in beautiful dashboards.\nA. Create the Prometheus Configuration Prometheus needs a config file to know what to monitor.\n# Create the project directory mkdir -p ~/docker/monitoring cd ~/docker/monitoring # Create the prometheus.yml file nano prometheus.yml Paste in the following configuration:\nglobal: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;node-exporter\u0026#39; static_configs: - targets: [\u0026#39;node-exporter:9100\u0026#39;] - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] B. Deploy the Stack with Docker Compose Next, create the docker-compose.yml file in the same ~/docker/monitoring directory.\nnano docker-compose.yml This file defines all four monitoring services and tells them to connect to the npm_default network we created earlier.\nservices: prometheus: image: prom/prometheus:latest container_name: prometheus restart: unless-stopped ports: - \u0026#34;9090:9090\u0026#34; volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - prometheus_data:/prometheus networks: - default grafana: image: grafana/grafana:latest container_name: grafana restart: unless-stopped ports: - \u0026#34;3001:3000\u0026#34; volumes: - grafana_data:/var/lib/grafana networks: - default node-exporter: image: prom/node-exporter:latest container_name: node-exporter restart: unless-stopped volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - \u0026#39;--path.procfs=/host/proc\u0026#39; - \u0026#39;--path.sysfs=/host/sys\u0026#39; - \u0026#39;--path.rootfs=/rootfs\u0026#39; - \u0026#39;--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)\u0026#39; networks: - default cadvisor: image: gcr.io/cadvisor/cadvisor:latest container_name: cadvisor restart: unless-stopped ports: - \u0026#34;8081:8080\u0026#34; volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro networks: - default volumes: prometheus_data: grafana_data: networks: default: name: npm_default external: true Now, launch the stack:\ndocker compose up -d C. Configure Grafana Log in to Grafana at http://\u0026lt;your-server-ip\u0026gt;:3001 (default: admin/admin).\nAdd Data Source: Go to Connections \u0026gt; Data Sources, add a Prometheus source, and set the URL to http://prometheus:9090.\nImport Dashboards: Go to Dashboards \u0026gt; New \u0026gt; Import and add these dashboards by ID:\nNode Exporter Full (ID: 1860)\nDocker Host/Container Metrics (ID: 193)\n3. The Homer Launchpad Dashboard 🚀 Finally, let\u0026rsquo;s deploy Homer as our beautiful start page with custom icons.\nCreate Directories \u0026amp; Download Icons: First, create a directory for Homer and an assets subdirectory. Then, cd into the assets folder and download the icons.\nmkdir -p ~/docker/homer/assets cd ~/docker/homer/assets wget -O grafana.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/grafana.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/grafana.png) wget -O prometheus.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/prometheus.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/prometheus.png) wget -O cadvisor.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/cadvisor.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/cadvisor.png) wget -O npm.png [https://nginxproxymanager.com/icon.png](https://nginxproxymanager.com/icon.png) Create Configuration: Go back to the main homer directory and create the config.yml file.\ncd ~/docker/homer nano config.yml Paste in the following configuration. The logo: lines point to the icons we just downloaded.\n--- title: \u0026#34;Homelab Dashboard\u0026#34; subtitle: \u0026#34;Server Management\u0026#34; theme: \u0026#34;dark\u0026#34; services: - name: \u0026#34;Management\u0026#34; icon: \u0026#34;fas fa-server\u0026#34; items: - name: \u0026#34;Nginx Proxy Manager\u0026#34; logo: \u0026#34;assets/tools/npm.png\u0026#34; subtitle: \u0026#34;Reverse Proxy Admin\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:81\u0026#34; - name: \u0026#34;Monitoring\u0026#34; icon: \u0026#34;fas fa-chart-bar\u0026#34; items: - name: \u0026#34;Grafana\u0026#34; logo: \u0026#34;assets/tools/grafana.png\u0026#34; subtitle: \u0026#34;Metrics Dashboard\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:3001\u0026#34; - name: \u0026#34;Prometheus\u0026#34; logo: \u0026#34;assets/tools/prometheus.png\u0026#34; subtitle: \u0026#34;Metrics Database\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:9090\u0026#34; - name: \u0026#34;cAdvisor\u0026#34; logo: \u0026#34;assets/tools/cadvisor.png\u0026#34; subtitle: \u0026#34;Container Metrics\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:8081\u0026#34; Create Docker Compose File: Finally, create the docker-compose.yml file.\nnano docker-compose.yml This configuration connects Homer to our shared network.\nservices: homer: image: b4bz/homer container_name: homer volumes: - ./config.yml:/www/assets/config.yml - ./assets:/www/assets/tools ports: - \u0026#34;8090:8080\u0026#34; restart: unless-stopped networks: - npm_default networks: npm_default: external: true Launch: Run docker compose up -d. You can now access your new dashboard with custom icons at http://\u0026lt;your-server-ip\u0026gt;:8090.\nConclusion Our homelab now has a powerful management and monitoring foundation. Nginx Proxy Manager is ready to direct traffic, Grafana is visualizing our server\u0026rsquo;s health, and Homer provides a central launchpad.\nIn the next part of the series, we\u0026rsquo;ll deploy our core network service, AdGuard Home, and use NPM to create clean, memorable local domains for all the applications we set up today. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-2-monitoring-and-management/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to Part 2 of my homelab series! In \u003ca href=\"/projects/homelab-series-part-1-debian-docker-foundation/\"\u003ePart 1\u003c/a\u003e, we built a solid foundation by turning an old laptop into a hardened Debian server with Docker. Now that our server is running, we need to deploy services to manage, monitor, and easily access our projects.\u003c/p\u003e\n\u003cp\u003eIn this guide, we\u0026rsquo;ll deploy three essential stacks. First, \u003cstrong\u003eNginx Proxy Manager (NPM)\u003c/strong\u003e will act as our server\u0026rsquo;s front door and create a shared network for our containers. Second, we\u0026rsquo;ll set up a professional-grade monitoring stack with \u003cstrong\u003ePrometheus\u003c/strong\u003e and \u003cstrong\u003eGrafana\u003c/strong\u003e. Finally, we\u0026rsquo;ll deploy a \u003cstrong\u003eHomer\u003c/strong\u003e dashboard to create a beautiful and convenient launchpad for all our services.\u003c/p\u003e","title":"Part 2: Homelab Management \u0026 Monitoring"},{"content":"My First Cloud Ad-Blocker: A Look Back at AdGuard Home on Linode Hey everyone, Prajwol here.\nAs I continue to explore different cloud projects, I often think back to the ones that had the biggest impact on my day-to-day life. One of the very first projects that truly changed my internet experience was setting up my own ad-blocking DNS server using AdGuard Home on a Linode instance.\nMy goal was to find a simple, cost-effective way to block ads and trackers across my entire home network. I wanted a \u0026ldquo;set it and forget it\u0026rdquo; solution that would cover every device—from my phone to my smart TV—without needing to install an app on each one. Linode (now Akamai) was the perfect platform for this: straightforward, powerful, and great for hosting a lightweight service like AdGuard Home.\nThe process of spinning up a small server, running a single installation script, and then seeing the query logs light up with blocked requests was incredibly satisfying. It felt like I had taken back a real measure of control over my own network.\nThis project remains a fantastic entry point for anyone wanting to get started with self-hosting and network privacy. I\u0026rsquo;ve kept the original, detailed guide for anyone who wants to follow along.\nYou can find the full step-by-step project guide here! It’s a rewarding project that delivers tangible results almost immediately. Let me know if you give it a try!\nPublished: Friday, August 22, 2025\n","permalink":"http://localhost:1313/blog/adguard-home-on-cloud/","summary":"\u003ch1 id=\"my-first-cloud-ad-blocker-a-look-back-at-adguard-home-on-linode\"\u003eMy First Cloud Ad-Blocker: A Look Back at AdGuard Home on Linode\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eAs I continue to explore different cloud projects, I often think back to the ones that had the biggest impact on my day-to-day life. One of the very first projects that truly changed my internet experience was setting up my own ad-blocking DNS server using AdGuard Home on a Linode instance.\u003c/p\u003e\n\u003cp\u003eMy goal was to find a simple, cost-effective way to block ads and trackers across my entire home network. I wanted a \u0026ldquo;set it and forget it\u0026rdquo; solution that would cover every device—from my phone to my smart TV—without needing to install an app on each one. Linode (now Akamai) was the perfect platform for this: straightforward, powerful, and great for hosting a lightweight service like AdGuard Home.\u003c/p\u003e","title":"My First Cloud Ad-Blocker - A Look Back at AdGuard Home on Linode"},{"content":"Introduction Welcome to the first post in my new homelab series! I\u0026rsquo;ve always been fascinated by self-hosting and DevOps, and I believe the best way to learn is by doing. In this series, I\u0026rsquo;ll document my journey of turning an old, unused laptop into a powerful, efficient, and secure bare-metal server for hosting a variety of network services.\nThe goal for this first part is to lay a solid foundation. We\u0026rsquo;ll take an old laptop, install a minimal and stable Linux operating system, perform some initial security hardening, and set up Docker as our containerization engine. By the end of this post, we\u0026rsquo;ll have a perfect blank canvas ready for the exciting services we\u0026rsquo;ll deploy in the upcoming parts.\n1. Choosing the Hardware \u0026amp; OS Why an Old Laptop? Before diving in, why use an old laptop instead of a Raspberry Pi or a dedicated server? For a starter homelab, a laptop has three huge advantages:\nCost-Effective: It\u0026rsquo;s free if you have one lying around! Built-in UPS: The battery acts as a built-in Uninterruptible Power Supply (UPS), keeping the server running through short power outages. Low Power Consumption: Laptop hardware is designed to be power-efficient, which is great for a device that will be running 24/7. Why Debian 13 \u0026ldquo;Trixie\u0026rdquo;? For the operating system, I chose Debian. It\u0026rsquo;s renowned for its stability, security, and massive package repository. It’s the bedrock of many other distributions (like Ubuntu) and is perfect for a server because it\u0026rsquo;s lightweight and doesn\u0026rsquo;t include unnecessary software. We\u0026rsquo;ll be using the minimal \u0026ldquo;net-install\u0026rdquo; to ensure we only install what we absolutely need.\n2. Installation and Network Configuration The installation process is straightforward, but the network setup is key to a reliable server.\nMinimal Installation Create a Bootable USB: I downloaded the Debian 13 \u0026ldquo;netinst\u0026rdquo; ISO from the official website and used Rufus on Windows to create a bootable USB drive. Boot from USB: I plugged the USB into the laptop and booted from it (usually pressing F12, F2, or Esc during startup to select the USB device). Language, Location, and Keyboard: Selected English, United States, and the default keyboard layout. Network Setup: Connected the laptop to my home network (Ethernet preferred for stability). Hostname \u0026amp; Domain: Entered a short, memorable hostname for the server (e.g., homelab) and left the domain blank. User Accounts: Set a root password. Created a non-root regular user (this will be used for daily management). Partition Disks: Chose Guided – use entire disk with separate /home partition. This is simpler for a server setup. Software Selection: At the “Software selection” screen: Unchecked “Debian desktop environment” Checked “SSH server” and “standard system utilities” This ensures a clean command-line system that can be accessed remotely. GRUB Bootloader: Installed GRUB on the primary drive (so the system boots correctly). Finish Installation: Removed the USB drive when prompted and rebooted into the fresh Debian install. Setting a Static IP A server needs a permanent, unchanging IP address. The best way to do this is with DHCP Reservation on your router. This tells your router to always assign the same IP address to your server\u0026rsquo;s unique MAC address.\nFirst, find your laptop’s current IP address and network interface name by running:\nip a You’ll see output similar to:\n2: enp3s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\rinet 192.168.0.45/24 brd 192.168.0.255 scope global dynamic enp3s0\rvalid_lft 86396sec preferred_lft 86396sec In this example:\nInterface name: enp3s0 Current IP: 192.168.0.45 MAC address: shown under link/ether in the same section. With this info, log into your router’s admin panel, find the \u0026ldquo;DHCP Reservation\u0026rdquo; or \u0026ldquo;Static Leases\u0026rdquo; section, and assign a memorable IP address (e.g., 192.168.0.45) to your server’s MAC address.\nThis ensures the server always gets the same IP from your router, making it easy to find on your network.\nConnecting Remotely with SSH With a static IP set, all future management will be done remotely using an SSH client. For Windows, I highly recommend Solar-PuTTY. I created a new session, entered the server\u0026rsquo;s static IP address, my username, and password, and connected.\n3. Initial Server Hardening With a remote SSH session active, the first thing to do is secure the server and configure it for its headless role.\nUpdate the System First, let\u0026rsquo;s make sure all packages are up to date.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y Configure the Firewall ufw (Uncomplicated Firewall) is perfect for a simple setup. We\u0026rsquo;ll set it to deny all incoming traffic by default and only allow SSH connections.\n# Install UFW sudo apt install ufw -y # Allow SSH connections sudo ufw allow ssh # Enable the firewall sudo ufw enable Configure Lid-Close Action To ensure the laptop keeps running when the lid is closed, we edit the logind.conf file.\nsudo nano /etc/systemd/logind.conf Uncomment the line:\nHandleLidSwitch=ignore Save the file, then restart the service:\nsudo systemctl restart systemd-logind.service 4. Installing the Containerization Engine: Docker Instead of installing applications directly on our host, we\u0026rsquo;ll use Docker to keep the system clean and make management easier.\nInstall Docker Engine The official convenience script is the easiest way to get the latest version.\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Add User to Docker Group To run docker commands without sudo, add your user to the docker group. The $USER variable automatically uses the currently logged-in user.\nsudo usermod -aG docker $USER After this, log out and log back in for the change to take effect.\nInstall Docker Compose Docker Compose is essential for managing multi-container applications with a simple YAML file.\nsudo apt install docker-compose-plugin -y To verify the installation:\ndocker compose version Conclusion And that\u0026rsquo;s it for Part 1! We\u0026rsquo;ve successfully turned an old piece of hardware into a hardened, modern server running Debian and Docker with a reliable network configuration. We have a solid and secure foundation to build upon.\nIn the next part of the series, we\u0026rsquo;ll deploy our first critical service: a local, network-wide ad-blocking DNS resolver using AdGuard Home. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-1-debian-docker-foundation/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to the first post in my new homelab series! I\u0026rsquo;ve always been fascinated by self-hosting and DevOps, and I believe the best way to learn is by doing. In this series, I\u0026rsquo;ll document my journey of turning an old, unused laptop into a powerful, efficient, and secure bare-metal server for hosting a variety of network services.\u003c/p\u003e\n\u003cp\u003eThe goal for this first part is to lay a solid foundation. We\u0026rsquo;ll take an old laptop, install a minimal and stable Linux operating system, perform some initial security hardening, and set up Docker as our containerization engine. By the end of this post, we\u0026rsquo;ll have a perfect blank canvas ready for the exciting services we\u0026rsquo;ll deploy in the upcoming parts.\u003c/p\u003e","title":"Part 1: Reviving an Old Laptop with Debian \u0026 Docker"},{"content":"Your Personal Internet Guardian: How to Build a FREE Ad-Blocker in the Cloud! 🚀 Hey everyone! A while back, I wrote a guide on setting up AdGuard Home on Linode. The world of tech moves fast, and it\u0026rsquo;s time for an upgrade! Today, we\u0026rsquo;re going to build our own powerful, network-wide ad-blocker using Amazon Web Services (AWS), and we\u0026rsquo;ll make it secure with our own domain and SSL certificate.\nThink of this as building a digital gatekeeper for your internet. Before any ads, trackers, or malicious sites can reach your devices, our AdGuard Home server will slam the door shut. The best part? This works on your phone, laptop, smart TV—anything on your network—without installing a single app on them.\nThis guide is for everyone, from seasoned tech wizards to curious beginners. We\u0026rsquo;ll break down every step in simple terms, so grab a coffee, and let\u0026rsquo;s build something awesome!\n## Chapter 1: Building Our Home in the AWS Cloud ☁️ First, we need a server. We\u0026rsquo;ll use an Amazon EC2 instance, which is just a fancy name for a virtual computer that you rent.\nSign Up for AWS: If you don\u0026rsquo;t have an account, head to the AWS website and sign up. You\u0026rsquo;ll need a credit card for verification, but for this guide, we can often stay within the Free Tier.\nLaunch Your EC2 Instance:\nLog in to your AWS Console and search for EC2. Click \u0026ldquo;Launch instance\u0026rdquo;. Name: Give your server a cool name, like AdGuard-Server. Application and OS Images: In the search bar, type Debian and select the latest version (e.g., Debian 12). Make sure it\u0026rsquo;s marked \u0026ldquo;Free tier eligible\u0026rdquo;. Instance Type: Choose t2.micro. This is your free, trusty little server. Key Pair (for login): This is your digital key to the server\u0026rsquo;s front door. Click \u0026ldquo;Create a new key pair\u0026rdquo;, name it something like my-adguard-key, and download the .pem file. Keep this file secret and safe! Network settings (The Firewall): This is crucial. We need to tell our server which doors to open. Click \u0026ldquo;Edit\u0026rdquo;. Check the box for \u0026ldquo;Allow SSH traffic from\u0026rdquo; and select My IP. This lets you securely log in. Check \u0026ldquo;Allow HTTPS traffic from the internet\u0026rdquo; and \u0026ldquo;Allow HTTP traffic from the internet\u0026rdquo;. We\u0026rsquo;ll need these for our secure dashboard later. Launch It! Hit the \u0026ldquo;Launch instance\u0026rdquo; button and watch as your new cloud server comes to life.\nGive Your Server a Permanent Address (Elastic IP):\nBy default, your server\u0026rsquo;s public IP address will change every time it reboots. Let\u0026rsquo;s make it permanent! In the EC2 menu on the left, go to \u0026ldquo;Elastic IPs\u0026rdquo;. Click \u0026ldquo;Allocate Elastic IP address\u0026rdquo; and then \u0026ldquo;Allocate\u0026rdquo;. Select the new IP address from the list, click \u0026ldquo;Actions\u0026rdquo;, and then \u0026ldquo;Associate Elastic IP address\u0026rdquo;. Choose your AdGuard-Server instance from the list and click \u0026ldquo;Associate\u0026rdquo;. Your server now has a static IP address that will never change! Make a note of this new IP. ## Chapter 2: Opening the Doors (Configuring the Firewall) 🚪 Our server is running, but for maximum security, we want to ensure only you can access the administrative parts of it. We\u0026rsquo;ll open the public DNS ports to everyone, but lock down the management ports to your home IP address.\nFind Your Public IP Address: Open a new browser tab and go to a site like WhatIsMyIP.com. It will display your home\u0026rsquo;s public IP address. Copy this IP address (it will look something like 203.0.113.55).\nEdit the Firewall Rules: Go to your EC2 Instance details, click the \u0026ldquo;Security\u0026rdquo; tab, and click on the Security Group name.\nClick \u0026ldquo;Edit inbound rules\u0026rdquo; and \u0026ldquo;Add rule\u0026rdquo; for each of the following. This makes sure your DNS is publicly available but the setup panel is locked down to your IP only.\nRule for AdGuard Setup (Port 3000):\nType: Custom TCP Port range: 3000 Source: Paste your IP address here, and add /32 to the end (e.g., 203.0.113.55/32). The /32 tells AWS it\u0026rsquo;s a single, specific IP address. Rule for DNS (Port 53):\nType: Custom UDP and Custom TCP (you will add two separate rules for this port) Port range: 53 Source: Anywhere-IPv4 Rule for DNS-over-TLS (Port 853):\nType: Custom TCP Port range: 853 Source: Anywhere-IPv4 Click \u0026ldquo;Save rules\u0026rdquo;. Your firewall is now configured to allow public DNS requests while keeping your management panel secure.\n## Chapter 3: Installing AdGuard Home 🛡️ Now, let\u0026rsquo;s connect to our server and install the magic software.\nConnect via SSH: Open a terminal (PowerShell on Windows, Terminal on Mac/Linux) and use the key you downloaded to connect. Use your new Elastic IP address! # Replace the path and Elastic IP with your own ssh -i \u0026#34;path/to/my-adguard-key.pem\u0026#34; admin@YOUR_ELASTIC_IP Install AdGuard Home: Run this one simple command. It downloads and installs everything for you. curl -s -S -L [https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh](https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh) | sh -s -- -v Run the Setup Wizard: The script will give you a link, like http://YOUR_ELASTIC_IP:3000. Open this in your browser. Follow the on-screen steps to create your admin username and password. ## Chapter 4: Teaching Your Guardian Who to Trust and What to Block With AdGuard Home installed, the next step is to configure its core brain: the DNS servers it gets its answers from and the blocklists it uses to protect your network.\n1. Setting Up Upstream DNS Servers Think of \u0026ldquo;Upstream DNS Servers\u0026rdquo; as the giant, public phonebooks of the internet. When your AdGuard server doesn\u0026rsquo;t know an address (and it\u0026rsquo;s not on a blocklist), it asks one of these upstreams. It\u0026rsquo;s recommended to use a mix of the best encrypted DNS providers for security, privacy, and speed.\nIn the AdGuard dashboard, go to Settings -\u0026gt; DNS settings. In the \u0026ldquo;Upstream DNS servers\u0026rdquo; box, enter the following, one per line:\nhttps://dns.quad9.net/dns-query https://dns.google/dns-query https://dns.cloudflare.com/dns-query Quad9: Focuses heavily on security, blocking malicious domains. Google: Known for being very fast. Cloudflare: A great all-around choice with a strong focus on privacy. 2. Optimizing DNS Performance Still in the DNS settings page, scroll down to optimize how your server queries the upstreams.\nParallel requests: Select this option. This is the fastest and most resilient mode. It sends your DNS query to all three of your upstream servers at the same time and uses the answer from the very first one that responds. This ensures you always get the quickest possible result.\nEnable EDNS client subnet (ECS): Check this box. This is very important for services like Netflix, YouTube, and other content delivery networks (CDNs). It helps them give you content from a server that is geographically closest to you, resulting in faster speeds and a better experience.\n3. Enabling DNSSEC Right below the upstream servers, there\u0026rsquo;s a checkbox for \u0026ldquo;Enable DNSSEC\u0026rdquo;. You should check this box. DNSSEC is like a digital wax seal on a letter; it verifies that the DNS answers you\u0026rsquo;re getting are authentic and haven\u0026rsquo;t been tampered with. It\u0026rsquo;s a simple, one-click security boost.\n4. Choosing Your Blocklists This is the fun part—the actual ad-blocking! Go to Filters -\u0026gt; DNS blocklists. For a \u0026ldquo;Balanced \u0026amp; Powerful\u0026rdquo; setup that blocks aggressively without a high risk of breaking websites, enable the following lists:\nAdGuard DNS filter: A great, well-maintained baseline. OISD Blocklist Big: Widely considered one of the best all-in-one lists for blocking ads, trackers, and malware. HaGeZi\u0026rsquo;s Pro Blocklist: A fantastic list that adds another layer of aggressive blocking for privacy. HaGeZi\u0026rsquo;s Threat Intelligence Feed: A crucial security-only list that focuses on protecting against active threats like phishing and malware. This combination will give you robust protection against both annoyances and real dangers.\n## Chapter 5: Giving Your Server a Name (Free Domain with No-IP) 📛 An IP address is hard to remember. Let\u0026rsquo;s get a free, memorable name for our server.\nSign Up at No-IP: Go to No-IP.com, create a free account, and create a hostname (e.g., my-dns.ddns.net). Point it to Your Server: When creating the hostname, enter your server\u0026rsquo;s permanent Elastic IP address. Confirm your account via email. ## Chapter 6: Making It Secure with SSL/TLS 🔐 We\u0026rsquo;ll use Let\u0026rsquo;s Encrypt and Certbot to get a free SSL certificate, which lets us use secure https:// and encrypted DNS.\nInstall Certbot: In your SSH session, run these commands:\nsudo apt update sudo apt install certbot -y Get the Certificate: Run this command, replacing the email and domain with your own.\n# This command will temporarily stop any service on port 80, get the certificate, and then finish. sudo certbot certonly --standalone --agree-tos --email YOUR_EMAIL@example.com -d your-no-ip-hostname.ddns.net If it\u0026rsquo;s successful, it will tell you where your certificate files are saved (usually in /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/).\nConfigure AdGuard Home Encryption:\nGo to your AdGuard Home dashboard (Settings -\u0026gt; Encryption settings). Check \u0026ldquo;Enable encryption\u0026rdquo;. In the \u0026ldquo;Server name\u0026rdquo; field, enter your No-IP hostname. Under \u0026ldquo;Certificates\u0026rdquo;, choose \u0026ldquo;Set a certificates file path\u0026rdquo;. Certificate path: /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/fullchain.pem Private key path: /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/privkey.pem Click \u0026ldquo;Save configuration\u0026rdquo;. The page will reload on a secure https:// connection! ## Chapter 7: Automating SSL Renewal (Cron Job Magic) ✨ Let\u0026rsquo;s Encrypt certificates last for 90 days. We can tell our server to automatically renew them.\nOpen the Cron Editor: In SSH, run sudo crontab -e and choose nano as your editor. Add the Renewal Job: Add this line to the bottom of the file. It tells the server to try renewing the certificate every day at 2:30 AM. 30 2 * * * systemctl stop AdGuardHome.service \u0026amp;\u0026amp; certbot renew --quiet \u0026amp;\u0026amp; systemctl start AdGuardHome.service Save and exit (Ctrl+X, then Y, then Enter). Your server will now keep its certificate fresh forever! ## Chapter 8: Testing Your New Superpowers (DoH \u0026amp; DoT) 🧪 For a direct confirmation, I used these commands on my computer:\nDNS-over-HTTPS (DoH) Test: This test checks if the secure web endpoint for DNS is alive.\ncurl -v [https://your-no-ip-hostname.ddns.net/dns-query](https://your-no-ip-hostname.ddns.net/dns-query) I got a \u0026ldquo;405 Method Not Allowed\u0026rdquo; error, which sounds bad but is actually great news. It means I successfully connected to the server, which correctly told me I didn\u0026rsquo;t send a real query. The connection works!\nDNS-over-TLS (DoT) Test: This checks the dedicated secure port for DNS. I used a tool called kdig.\n# I had to install it first with: sudo apt install knot-dnsutils kdig @your-no-ip-hostname.ddns.net +tls-ca +tls-host=your-no-ip-hostname.ddns.net example.com The command returned a perfect DNS answer for example.com, confirming the secure tunnel was working.\n## Chapter 9: Protecting Your Kingdom (Router \u0026amp; Phone Setup) 🏰 Now, let\u0026rsquo;s point your devices to their new guardian.\nOn Your Home Router: Log in to your router\u0026rsquo;s admin page, find the DNS settings, and enter your server\u0026rsquo;s Elastic IP as the primary DNS server. Leave the secondary field blank! This forces all devices on your Wi-Fi to be protected. Then, restart your router. On Your Mobile Phone: Android: Go to Settings -\u0026gt; Network -\u0026gt; Private DNS. Choose \u0026ldquo;Private DNS provider hostname\u0026rdquo; and enter your No-IP hostname (my-dns.ddns.net). This gives you ad-blocking everywhere, even on cellular data! iOS: You can use a profile to configure DoH. A simple way is to use a site like AdGuard\u0026rsquo;s DNS profile generator, but enter your own server\u0026rsquo;s DoH address (https://my-dns.ddns.net/dns-query). ## Chapter 10: The Ultimate Safety Net (Creating a Snapshot) 📸 Finally, let\u0026rsquo;s back up our perfect setup.\nIn the EC2 Console, go to your instance details. Click the \u0026ldquo;Storage\u0026rdquo; tab and click the \u0026ldquo;Volume ID\u0026rdquo;. Click \u0026ldquo;Actions\u0026rdquo; -\u0026gt; \u0026ldquo;Create snapshot\u0026rdquo;. Give it a description, like AdGuard-Working-Setup-Backup. If you ever mess something up, you can use this snapshot to restore your server to this exact working state in minutes.\n## Bonus Chapter: Common Troubleshooting Tips If things aren\u0026rsquo;t working, here are a few common pitfalls to check:\nBrowser Overrides Everything: If one device isn\u0026rsquo;t blocking ads, check its browser settings! Modern browsers like Chrome have a \u0026ldquo;Secure DNS\u0026rdquo; feature that can bypass your custom setup. You may need to turn this off. Check Your Laptop\u0026rsquo;s DNS: Make sure your computer\u0026rsquo;s network settings are set to \u0026ldquo;Obtain DNS automatically\u0026rdquo; so it listens to the router. A manually set DNS on your PC will ignore the router\u0026rsquo;s settings. Beware of IPv6: If you run into trouble on one device, try disabling IPv6 in that device\u0026rsquo;s Wi-Fi adapter properties to force it to use your working IPv4 setup. ## It’s a Wrap! And there you have it! You\u0026rsquo;ve successfully built a personal, secure, ad-blocking DNS server in the cloud. You\u0026rsquo;ve learned about cloud computing, firewalls, DNS, SSL, and automation. Go enjoy a faster, cleaner, and more private internet experience.\n","permalink":"http://localhost:1313/projects/adguard-updated/","summary":"\u003ch1 id=\"your-personal-internet-guardian-how-to-build-a-free-ad-blocker-in-the-cloud-\"\u003eYour Personal Internet Guardian: How to Build a FREE Ad-Blocker in the Cloud! 🚀\u003c/h1\u003e\n\u003cp\u003eHey everyone! A while back, I wrote a guide on setting up AdGuard Home on Linode. The world of tech moves fast, and it\u0026rsquo;s time for an upgrade! Today, we\u0026rsquo;re going to build our own powerful, network-wide ad-blocker using \u003cstrong\u003eAmazon Web Services (AWS)\u003c/strong\u003e, and we\u0026rsquo;ll make it secure with our own domain and SSL certificate.\u003c/p\u003e\n\u003cp\u003eThink of this as building a digital gatekeeper for your internet. Before any ads, trackers, or malicious sites can reach your devices, our AdGuard Home server will slam the door shut. The best part? This works on your phone, laptop, smart TV—anything on your network—without installing a single app on them.\u003c/p\u003e","title":"How I Built My Own Ad-Blocking DNS Server in the Cloud (2025 Updated Edition!)"},{"content":"What\u0026rsquo;s the buzz about AdGuard Home? Imagine AdGuard Home as your personal internet guardian. This versatile tool blocks ads, trackers, and other online nuisances across all devices connected to your network. Whether you\u0026rsquo;re browsing on your phone, tablet, or computer, AdGuard Home has your back.\nIn today\u0026rsquo;s digital landscape, robust security measures are paramount. Protecting each device shields your family from accidental clicks and malicious attacks, ensuring peace of mind and a secure online environment.\nWhy on the Cloud? While setting up AdGuard Home on your home network is great, installing it on a cloud server like Linode takes things up a notch. Here\u0026rsquo;s why:\nOn-the-Go Protection: Your devices stay protected from ads and trackers, no matter where you are, you can even share it with your family. Centralized Control: Manage and customize your ad-blocking settings from a single dashboard. Enhanced Privacy: Keep your browsing data away from prying eyes. Ready to embark on this ad-free adventure? Let\u0026rsquo;s get started!\nSetting Up The Environment Step 1: Create a Linode Cloud Account Why choose Linode? Through NetworkChuck\u0026rsquo;s referral link, you receive a generous $100 cloud credit - a fantastic start!\nSign Up: Navigate to Linode\u0026rsquo;s signup page and register. Access the Dashboard: Log in and select \u0026lsquo;Linodes\u0026rsquo; from the left-side menu. Create a Linode: Click \u0026lsquo;Create Linode,\u0026rsquo; choose your preferred region, and select an operating system (Debian 11 is a solid choice). Choose a Plan: The Shared 1GB Nanode instance is sufficient for AdGuard Home. Label and Secure: Assign a label to your Linode and set a strong root password. Deploy: Click \u0026lsquo;Create Linode\u0026rsquo; and wait for it to initialize. Once your Linode is up and running, access it via the LISH Console or SSH. (use root as localhost login)\nStep 2: Installing AdGuard Home on Linode Yes, we\u0026rsquo;re already into setting up at this point.\nLog In: Access your Linode using SSH or the LISH Console with your root credentials. Update the system: sudo apt update \u0026amp;\u0026amp; apt upgrade -y Go ahead and copy this command to Install Adguard Home: curl -s -S -L https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh | sh -s -- -v AdGuard Home is installed and running. You can use CTRL+Shift+V to paste into the terminal.\nStep 3: Configure AdGuard Home Post-installation, you\u0026rsquo;ll see a list of IP addresses with port :3000. Access the Web Interface: Open your browser and navigate to the IP address followed by :3000. If you encounter a security warning, proceed by clicking \u0026ldquo;Continue to site.\u0026rdquo; Initial Setup: Click \u0026lsquo;Get Started\u0026rsquo; and follow the prompts. When uncertain, default settings are typically fine. Set Credentials: Set up the Username and Password. Step 4: Integrate AdGuard Home with Your Router After this, your AdGuard Home is running, but in order to use it on your devices you need to set up inside your home router for all your devices to be protected. For that, I can\u0026rsquo;t walk you through each and every router\u0026rsquo;s settings, but the steps are pretty similar.\nFind your router IP address, you should be able to find it on the back of your router (commonly 192.168.0.1 or 192.169.1.1) enter it into your browser. Login into your router using the credentials mentioned in the back of your router; the default is often admin for both username and password. I suggest you change your default password. Configure DNS Settings: Enable DHCP Server: Ensure your router\u0026rsquo;s DHCP server is active. Set DNS Addresses: Input your AdGuard Home server\u0026rsquo;s IP as the primary DNS (mine was 96.126.113.207). For secondary DNS, options like 1.1.1.1 (Cloudflare), 9.9.9.9 (Quad9), or 8.8.8.8 (Google) are reliable. Save and apply the changes. Fine-Tuning AdGuard Home If you\u0026rsquo;ve done everything till here you should be good, but for those who enjoy customizations, AdGuard Home offers a plethora of settings. Some of the customization I did are:\nSettings Go to Settings -\u0026gt; General Settings: You can enable Parental Control and Safe Search. You can also make your Statistics last longer than 24hrs which is default. Now on Settings -\u0026gt; DNS Settings By default, it uses DNS from quad9 which is pretty good but I suggest you add more. You can click on the list of known DNS providers, which you can choose from. I used: https://dns.quad9.net/dns-query https://dns.google/dns-query https://dns.cloudflare.com/dns-query Enable \u0026lsquo;Load Balancing\u0026rsquo; to distribute queries evenly. Scroll down to \u0026lsquo;DNS server configuration\u0026rsquo; and enable DNSSEC for enhanced security. Click on Save. Filters DNS blocklists Go to Filters -\u0026gt; DNS blocklists, here you can add a blocklist that people have created and use it to block even more things. By default, AdGuard uses the AdGuard DNS filter, and you can add more.\nClick on Add blocklist -\u0026gt; Choose from the list Don\u0026rsquo;t choose too many from the list cause it may slow your internet requests. These are the blocklists I added. And just like that you are blocking more and more things. DNS rewrites Go to Filters -\u0026gt; DNS rewrites, here you can add your own DNS entries, so I added AdGuard here.\nClick on Add DNS rewrite Type in domain adguardforme.local and your IP address for AdGuard Home. And save it. Now, when I want to go on the AdGuard Home dashboard I just type in adguardforme.local and I\u0026rsquo;m into AdGuard, I don\u0026rsquo;t have to remember the IP address.\nCustom filtering rules Go to Filters -\u0026gt; Custom filtering rules. For some reason when I use Facebook on mobile device stories and videos did not load up, so I added custom filtering rules.\n@@||graph.facebook.com^$important ","permalink":"http://localhost:1313/projects/adguard-home-on-cloud/","summary":"\u003ch1 id=\"whats-the-buzz-about-adguard-home\"\u003eWhat\u0026rsquo;s the buzz about AdGuard Home?\u003c/h1\u003e\n\u003cp\u003eImagine AdGuard Home as your personal internet guardian. This versatile tool blocks ads, trackers, and other online nuisances across all devices connected to your network. Whether you\u0026rsquo;re browsing on your phone, tablet, or computer, AdGuard Home has your back.\u003c/p\u003e\n\u003cp\u003eIn today\u0026rsquo;s digital landscape, robust security measures are paramount. Protecting each device shields your family from accidental clicks and malicious attacks, ensuring peace of mind and a secure online environment.\u003c/p\u003e","title":"Running Private Adguard Server on Cloud (Linode)"},{"content":"What\u0026rsquo;s a Docker Container? Before we dive into setting up DeepSeek-R1, let me explain what a Docker container is. Imagine you have a toy that works perfectly on your birthday but gets broken if you move it to another room. A Docker container is like a magic box that keeps your AI model (the toy) in perfect condition wherever you take it, whether it\u0026rsquo;s running as a background task, on a web server, or even in the cloud.\nDocker containers encapsulate everything required to run an application: the code, dependencies, and environment settings. This ensures consistency across different machines, which is super important for AI models that rely on precise configurations.\nSetting Up The Environment Step 1: Install Ubuntu on Windows (If You Haven\u0026rsquo;t Already) If you\u0026rsquo;re using Windows, the easiest way to get an Ubuntu environment is through the Microsoft Store. Here\u0026rsquo;s how:\nOpen the Microsoft Store and search for Ubuntu. Click Get and let it install. Once installed, open Ubuntu from the Start menu and follow the setup instructions. Update the system: sudo apt update \u0026amp;\u0026amp; sudo apt upgrade Now, you have an Ubuntu terminal running on Windows!\nStep 2: Install Docker (If You Haven\u0026rsquo;t Already) First, let\u0026rsquo;s check if you have Docker installed. Open a terminal and run:\ndocker --version If that returns a version number, congrats! If not, install Docker:\nsudo apt update \u0026amp;\u0026amp; sudo apt install docker.io -y sudo systemctl enable --now docker Step 3: Prerequisites for NVIDIA GPU Install NVIDIA Container Toolkit:\nConfiguring the production repository: curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list Update the package list: sudo apt-get update Install the NVIDIA Container Toolkit: sudo apt-get install -y nvidia-container-toolkit Running Ollama Inside Docker Run these commands(P.S. shoutout to NetworkChuck):\ndocker run -d \\ --gpus all \\ -v ollama:/root/.ollama \\ -p 11434:11434 \\ --security-opt=no-new-privileges \\ --cap-drop=ALL \\ --cap-add=SYS_NICE \\ --memory=8g \\ --memory-swap=8g \\ --cpus=4 \\ --read-only \\ --name ollama \\ ollama/ollama Running DeepSeek-R1 Locally Time to bring DeepSeek-R1 to life locally and containerized:\ndocker exec -it ollama ollama run deepseek-r1 or you can run other versions of deepseek-r1 just by typing in the version at the end after a colon(:)\ndocker exec -it ollama ollama run deepseek-r1:7b After this, play around with the AI, if you wanna exit just type:\n/bye Starting Deepseek-R1 To Start Deepseek-R1 from next time go to Ubuntu and type:\ndocker start ollama this will start ollama docker container; then type:\ndocker exec -it ollama ollama run deepseek-r1:7b ","permalink":"http://localhost:1313/projects/running-deepseek-r1-on-docker-container-on-ubuntu/","summary":"\u003ch1 id=\"whats-a-docker-container\"\u003eWhat\u0026rsquo;s a Docker Container?\u003c/h1\u003e\n\u003cp\u003eBefore we dive into setting up DeepSeek-R1, let me explain what a Docker container is. Imagine you have a toy that works perfectly on your birthday but gets broken if you move it to another room. A Docker container is like a magic box that keeps your AI model (the toy) in perfect condition wherever you take it, whether it\u0026rsquo;s running as a background task, on a web server, or even in the cloud.\u003c/p\u003e","title":"Dive into AI Fun: Running DeepSeek-R1 on a Docker Container on Ubuntu"},{"content":"Description I joined AbbVie initially as a contractor and quickly demonstrated the skills and dedication that led to my conversion to a full-time position. In my role, I stepped into a high-stakes production environment where precision and operational stability are paramount. My work centered on the optimization and maintenance of sophisticated, machine learning-based visual inspection systems. I was responsible for fine-tuning these models, analyzing their performance data, and troubleshooting complex technical issues across both hardware and software, including the POM and PCE systems.\nThis wasn\u0026rsquo;t just about keeping machines running; it was about enhancing them. By applying a systematic, data-driven approach, I contributed to a 30% reduction in product waste, a metric that translates directly to improved efficiency and sustainability. Working within strict GMPs and utilizing systems like SAP for material tracking, I learned to balance technical problem-solving with rigorous compliance, ensuring that every action contributed to the stability and reliability of mission-critical operations.\n","permalink":"http://localhost:1313/experience/abbvie/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eI joined AbbVie initially as a contractor and quickly demonstrated the skills and dedication that led to my conversion to a full-time position. In my role, I stepped into a high-stakes production environment where precision and operational stability are paramount. My work centered on the optimization and maintenance of sophisticated, machine learning-based visual inspection systems. I was responsible for fine-tuning these models, analyzing their performance data, and troubleshooting complex technical issues across both hardware and software, including the POM and PCE systems.\u003c/p\u003e","title":"Operator III"},{"content":"Description As my first professional role after moving to the United States, my position at FedEx was a crucial step in adapting my technical skills to a new corporate environment. My journey began as a contractor, where my performance and analytical skills in a fast-paced setting led to my transition to a full-time Associate role. At the device testing center, I was on the front lines of quality assurance for a wide array of consumer electronics. I conducted comprehensive, systematic testing on mobile devices, smartwatches, and routers, executing detailed test plans to identify hardware vulnerabilities, software bugs, and non-compliance with network standards.\nMy responsibilities included meticulously documenting my findings, reproducing bugs to assist developers, and providing clear, actionable reports to engineering teams. This collaborative process was crucial in accelerating the repair cycle and ensuring that products met the highest standards of quality and security before reaching the market. The role sharpened my analytical skills and gave me a deep appreciation for the importance of rigorous testing in the software development lifecycle.\n","permalink":"http://localhost:1313/experience/fedex/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs my first professional role after moving to the United States, my position at FedEx was a crucial step in adapting my technical skills to a new corporate environment. My journey began as a contractor, where my performance and analytical skills in a fast-paced setting led to my transition to a full-time Associate role. At the device testing center, I was on the front lines of quality assurance for a wide array of consumer electronics. I conducted comprehensive, systematic testing on mobile devices, smartwatches, and routers, executing detailed test plans to identify hardware vulnerabilities, software bugs, and non-compliance with network standards.\u003c/p\u003e","title":"Product Testing Associate"},{"content":"Description As the IT Support Specialist for a bustling international college with over 2,500 students and staff, I was at the heart of the campus\u0026rsquo;s technical operations. My role was dynamic and comprehensive, involving end-to-end technical support across a diverse, multi-building campus. I managed the entire user lifecycle, from onboarding new accounts to ensuring smooth system setups across Windows, Linux, and Mac environments. I was the primary point of contact for all technical challenges, resolving Tier 1 and 2 support tickets with a 90% SLA adherence and troubleshooting complex OS issues to minimize downtime.\nMy tenure was marked by significant growth and adaptation. I led the complete technical setup of eight new computer labs, managing everything from hardware deployment and network cabling to software installation and configuration. When the COVID-19 pandemic hit, I was instrumental in transitioning the campus to a hybrid learning model, my first professional experience navigating such a large-scale shift. This required rapidly scaling our remote support capabilities and ensuring both students and faculty could operate effectively from anywhere.\nA cornerstone project of my time was the complete technical overhaul of the newly acquired Kumari Film Hall. I was deeply involved in the project to transform the old cinema into modern lecture halls, which included designing and deploying the entire network infrastructure, setting up AV systems, and ensuring seamless integration with the main campus network.\nTo support these expanding operations, I took the lead in deploying a new UVDesk help desk ticketing system on a CentOS server and embraced automation, utilizing tools like OK Goldy to streamline user creation in Google Workspace. These initiatives standardized processes, improved efficiency, and allowed our team to successfully manage the college\u0026rsquo;s ambitious growth.\n","permalink":"http://localhost:1313/experience/islingtoncollege/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs the IT Support Specialist for a bustling international college with over 2,500 students and staff, I was at the heart of the campus\u0026rsquo;s technical operations. My role was dynamic and comprehensive, involving end-to-end technical support across a diverse, multi-building campus. I managed the entire user lifecycle, from onboarding new accounts to ensuring smooth system setups across Windows, Linux, and Mac environments. I was the primary point of contact for all technical challenges, resolving Tier 1 and 2 support tickets with a 90% SLA adherence and troubleshooting complex OS issues to minimize downtime.\u003c/p\u003e","title":"IT Support Specialist"},{"content":"Description Building on my foundational experience, my internship at BlackBox Technologies immersed me in a more complex, project-based environment. I was an integral part of a development team tasked with building a web-based attendance system from the ground up. This role provided me with invaluable hands-on, full-stack experience. I contributed to the backend by assisting senior engineers with the development of business logic in .NET, giving me insight into server-side architecture. Simultaneously, I was responsible for building responsive, user-facing components for the front-end using HTML, CSS, and JavaScript.\nThis experience was a deep dive into the software development lifecycle. I learned how to translate business requirements into technical specifications, participated in code reviews, and understood the synergy between front-end and back-end systems. Working in close collaboration with the engineering team on a single, focused product was an excellent opportunity to apply my skills to a real-world project and solidify my understanding of creating robust, scalable web applications.\n","permalink":"http://localhost:1313/experience/blackbox/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eBuilding on my foundational experience, my internship at BlackBox Technologies immersed me in a more complex, project-based environment. I was an integral part of a development team tasked with building a web-based attendance system from the ground up. This role provided me with invaluable hands-on, full-stack experience. I contributed to the backend by assisting senior engineers with the development of business logic in .NET, giving me insight into server-side architecture. Simultaneously, I was responsible for building responsive, user-facing components for the front-end using HTML, CSS, and JavaScript.\u003c/p\u003e","title":"Web Development Intern"},{"content":"Description My journey into professional software development began at Radiant Infotech, my first internship and job in the tech industry. This role was a pivotal transition from academic theory to real-world application. I was entrusted with supporting the full lifecycle of client websites, which provided an immersive learning experience. My primary responsibility was to develop responsive, pixel-perfect front-end interfaces using HTML, CSS, and Bootstrap, translating design files into functional web components. A key part of this process was using Adobe Photoshop to prepare and optimize web graphics, ensuring both aesthetic quality and optimal performance.\nBeyond the initial development, my role extended to managing website content through various CMS platforms and performing rigorous debugging to ensure cross-browser compatibility and a seamless user experience. This foundational internship was crucial in building my confidence and skills in modern web development, teaching me how to collaborate effectively within a team to deliver high-quality digital products for clients.\n","permalink":"http://localhost:1313/experience/radiantinfotech/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eMy journey into professional software development began at Radiant Infotech, my first internship and job in the tech industry. This role was a pivotal transition from academic theory to real-world application. I was entrusted with supporting the full lifecycle of client websites, which provided an immersive learning experience. My primary responsibility was to develop responsive, pixel-perfect front-end interfaces using HTML, CSS, and Bootstrap, translating design files into functional web components. A key part of this process was using Adobe Photoshop to prepare and optimize web graphics, ensuring both aesthetic quality and optimal performance.\u003c/p\u003e","title":"Web Development Intern"},{"content":"Introduction Welcome to my personal portfolio website. I respect your privacy and am committed to protecting it. This policy outlines what information is collected when you visit my site and how that information is used.\nInformation Collection and Use I collect information in two ways: information you provide directly and anonymous data collected by analytics services.\nPersonal Data You Provide When you request a copy of my resume, you are asked to voluntarily provide your email address.\nHow it\u0026rsquo;s collected: This information is collected via an embedded Google Form. Why it\u0026rsquo;s collected: It is used for the sole purpose of sending the requested resume document to you through an automated process managed by Google Apps Script. How it\u0026rsquo;s used: Your email will not be used for marketing purposes, sold, or shared with any third parties. Anonymous Usage Data To improve the user experience and analyze traffic, this website uses the following third-party services:\nUmami (Self-Hosted): This website uses a self-hosted instance of Umami for privacy-focused web analytics. Umami collects anonymous usage data such as page views, referrers, and geographic regions to help me understand website traffic. This service does not use cookies, does not collect any personally identifiable information, and all data is stored on a private server under my control.\nCloudflare Web Analytics: This service collects anonymous traffic data such as page views and country of origin. It does not use cookies or collect personally identifiable information. You can view their privacy policy here.\nService Providers This website relies on the following third-party service providers to function:\nGoogle Workspace (Forms, Sheets, Apps Script): Used to manage and automate resume requests. Cloudflare: Used as a Content Delivery Network (CDN) to improve website performance, as a security firewall to protect against malicious attacks, and for collecting anonymous web analytics. Vercel: Used to host the self-hosted Umami analytics application. Netlify \u0026amp; GitHub: Used for hosting and deploying the website. Changes to This Privacy Policy I may update this Privacy Policy from time to time. I will notify you of any changes by posting the new Privacy Policy on this page. You are advised to review this page periodically for any changes.\nContact Me If you have any questions about this Privacy Policy, please contact me at: prajwolad18@gmail.com\n","permalink":"http://localhost:1313/privacy-policy/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWelcome to my personal portfolio website. I respect your privacy and am committed to protecting it. This policy outlines what information is collected when you visit my site and how that information is used.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"information-collection-and-use\"\u003eInformation Collection and Use\u003c/h2\u003e\n\u003cp\u003eI collect information in two ways: information you provide directly and anonymous data collected by analytics services.\u003c/p\u003e\n\u003ch3 id=\"personal-data-you-provide\"\u003ePersonal Data You Provide\u003c/h3\u003e\n\u003cp\u003eWhen you request a copy of my resume, you are asked to voluntarily provide your email address.\u003c/p\u003e","title":"Privacy Policy"},{"content":"Introduction Welcome to the part 4 of the homelab series! In the previous parts, we built a server, deployed a suite of services, and configured our network. Now, it\u0026rsquo;s time to make it resilient and self-maintaining. A homelab isn\u0026rsquo;t just about setting things up; it\u0026rsquo;s about keeping them running reliably.\nThis guide will show you how to set up the three pillars of modern IT operations: Automated Backups, Automated Updates, and Proactive Alerting. By the end, you\u0026rsquo;ll have a homelab that runs itself, ensures your data is safe, stays up-to-date, and notifies you when something goes wrong.\nChapter 1: The Automated Backup Strategy (at 3 AM) A solid backup strategy is non-negotiable. I implemented a robust system inspired by the \u0026ldquo;3-2-1\u0026rdquo; rule, focusing on redundancy and an off-site copy. My strategy involves maintaining two copies of my data in two separate locations: one local backup on the server itself for fast recovery, and one automated, off-site backup to Google Drive to protect against a local disaster like a fire or hardware failure.\nThis script runs at 3 AM, creates a local backup, uploads it, and then notifies Discord.\nStep 1: Configure rclone for Google Drive First, you need a tool to communicate with Google Drive. We\u0026rsquo;ll use rclone.\nInstall rclone on your Debian server: sudo -v ; curl [https://rclone.org/install.sh](https://rclone.org/install.sh) | sudo bash Run the interactive setup: rclone config Follow the Prompts: n (New remote) * name\u0026gt;: gdrive (You can name it anything) storage\u0026gt;: Find and select drive (Google Drive). client_id\u0026gt; \u0026amp; client_secret\u0026gt;: Press Enter for both to leave blank. scope\u0026gt;: Choose 1 (Full access). Use auto config? y/n\u0026gt;: This is a critical step. Since we are on a headless server, type n and press Enter. Authorize Headless: rclone will give you a command to run on a machine with a web browser (like your main computer). On your main computer (where you have rclone installed), run the rclone authorize \u0026quot;drive\u0026quot; \u0026quot;...\u0026quot; command. This will open your browser, ask you to log in to Google, and grant permission. Your main computer\u0026rsquo;s terminal will then output a block of text (your config_token). Paste Token: Copy the token from your main computer and paste it back into your server\u0026rsquo;s rclone prompt. Finish the prompts, and your connection is complete. Step 2: Create the Backup Script Next, create a shell script to perform the backup.\nCreate the file and make it executable:\nnano ~/backup.sh chmod +x ~/backup.sh Paste in the following script. You must edit the first 7 variables to match your setup.\n#!/bin/bash # --- Configuration --- SOURCE_DIR=\u0026#34;/path/to/your/docker\u0026#34; # \u0026lt;-- Change to your Docker projects directory BACKUP_DIR=\u0026#34;/path/to/your/backups\u0026#34; # \u0026lt;-- Change to your backups folder FILENAME=\u0026#34;homelab-backup-$(date +%Y-%m-%d).tar.gz\u0026#34; LOCAL_RETENTION_DAYS=3 CLOUD_RETENTION_DAYS=3 RCLONE_REMOTE=\u0026#34;gdrive\u0026#34; # \u0026lt;-- Must match your rclone remote name RCLONE_DEST=\u0026#34;Homelab Backups\u0026#34; # \u0026lt;-- Folder name in Google Drive # --- \u0026#34;https://discordapp.com/api/webhooks/141949178941/6Tx6f1yjf26LztQ\u0026#34; --- DISCORD_WEBHOOK_URL=\u0026#34;YOUR_DISCORD_WEBHOOK_URL\u0026#34; # --- Notification Function --- send_notification() { MESSAGE=$1 curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#34;{\\\u0026#34;content\\\u0026#34;: \\\u0026#34;$MESSAGE\\\u0026#34;}\u0026#34; \u0026#34;$DISCORD_WEBHOOK_URL\u0026#34; } # --- Script Logic --- echo \u0026#34;--- Starting Homelab Backup: $(date) ---\u0026#34; send_notification \u0026#34;✅ Starting Homelab Backup...\u0026#34; # 1. Create local backup echo \u0026#34;Creating local backup...\u0026#34; tar -czf \u0026#34;${BACKUP_DIR}/${FILENAME}\u0026#34; -C \u0026#34;${SOURCE_DIR}\u0026#34; . echo \u0026#34;Local backup created at ${BACKUP_DIR}/${FILENAME}\u0026#34; # 2. Upload to Google Drive echo \u0026#34;Uploading backup to ${RCLONE_REMOTE}...\u0026#34; rclone copy \u0026#34;${BACKUP_DIR}/${FILENAME}\u0026#34; \u0026#34;${RCLONE_REMOTE}:${RCLONE_DEST}\u0026#34; echo \u0026#34;Upload complete.\u0026#34; # 3. Clean up local backups echo \u0026#34;Cleaning up local backups older than ${LOCAL_RETENTION_DAYS} days...\u0026#34; find \u0026#34;${BACKUP_DIR}\u0026#34; -type f -name \u0026#34;*.tar.gz\u0026#34; -mtime +${LOCAL_RETENTION_DAYS} -delete echo \u0026#34;Local cleanup complete.\u0026#34; # 4. Clean up cloud backups echo \u0026#34;Cleaning up cloud backups older than ${CLOUD_RETENTION_DAYS} days...\u0026#34; rclone delete \u0026#34;${RCLONE_REMOTE}:${RCLONE_DEST}\u0026#34; --min-age ${CLOUD_RETENTION_DAYS}d echo \u0026#34;Cloud cleanup complete.\u0026#34; echo \u0026#34;Backup process finished.\u0026#34; send_notification \u0026#34;🎉 Homelab backup and cloud upload completed successfully!\u0026#34; Step 3: Automate with Cron To run this script automatically, you must add it to the root user\u0026rsquo;s crontab. This is critical for giving the script permission to read all Docker files.\nOpen the root crontab editor: sudo crontab -e Add the following line to schedule the backup for 3:00 AM every morning: 0 3 * * * /path/to/your/backup.sh You will now get a fresh, onsite and off-site backup every night and a Discord message when it\u0026rsquo;s done. Chapter 2: Automated Updates with Watchtower (at 6 AM) Manually updating every Docker container is tedious. We can automate this by deploying Watchtower.\nStep 1: The Docker Compose File Create a docker-compose.yml for Watchtower. This configuration schedules it to run once a day at 6:00 AM, clean up old images, and send a Discord notification only if it finds an update.\nmkdir -p ~/docker/watchtower\ncd ~/docker/watchtower\nnano docker-compose.yml\nPaste in this configuration:\nservices: watchtower: image: containrrr/watchtower container_name: watchtower restart: unless-stopped volumes: - /var/run/docker.sock:/var/run/docker.sock environment: # Timezone setting TZ: America/Chicago # Discord notification settings WATCHTOWER_NOTIFICATIONS: shoutrrr WATCHTOWER_NOTIFICATION_URL: \u0026#34;discord://YOUR_DISCORD_WEBHOOK_ID_URL\u0026gt; # Notification settings WATCHTOWER_NOTIFICATIONS_LEVEL: info WATCHTOWER_NOTIFICATION_REPORT: \u0026#34;true\u0026#34; WATCHTOWER_NOTIFICATIONS_HOSTNAME: Homelab-Laptop # Update settings WATCHTOWER_CLEANUP: \u0026#34;true\u0026#34; WATCHTOWER_INCLUDE_STOPPED: \u0026#34;false\u0026#34; WATCHTOWER_INCLUDE_RESTARTING: \u0026#34;true\u0026#34; WATCHTOWER_SCHEDULE: \u0026#34;0 0 6 * * *\u0026#34; Note: The WATCHTOWER_NOTIFICATION_URL uses a special shoutrrr format for Discord, which looks like discord://token@webhook-id.\nNow, every morning at 6:00 AM, Watchtower will scan all running containers and update any that have a new image available.\nChapter 3: Proactive Alerting (24/7) The final piece of automation is proactive alerting. This setup ensures you are immediately notified via Discord if something goes wrong.\nStep 1: The Alerting Pipeline The pipeline we\u0026rsquo;ll build is: Prometheus (detects problems) -\u0026gt; Alertmanager (groups and routes alerts) -\u0026gt; Discord (notifies you).\nStep 2: Deploy Alertmanager First, deploy Alertmanager. It must be on the same npm_default network as Prometheus.\nmkdir -p ~/docker/alertmanager\ncd ~/docker/alertmanager\nCreate the alertmanager.yml configuration file:\nnano alertmanager.yml Paste in this configuration. It uses advanced routing to send critical alerts every 2 hours and warning alerts every 12 hours.\nglobal: resolve_timeout: 5m route: group_by: [\u0026#34;alertname\u0026#34;, \u0026#34;severity\u0026#34;] group_wait: 30s group_interval: 10m repeat_interbal: 12h receiver: \u0026#34;discord-notifications\u0026#34; routes: - receiver: \u0026#34;discord-notifications\u0026#34; matchers: - severity=\u0026#34;critical\u0026#34; repeat_interval: 2h - receiver: \u0026#34;discord-notifications\u0026#34; matchers: - severity=\u0026#34;warning\u0026#34; repeat_interval: 12h receivers: - name: \u0026#34;discord-notifications\u0026#34; discord_configs: - webhook_url: \u0026#34;YOUR_DISCORD_WEBHOOK_URL\u0026#34; send_resolved: true Now create the docker-compose.yml for Alertmanager:\nnano docker-compose.yml Paste in the following:\nservices: alertmanager: image: prom/alertmanager:latest container_name: alertmanager restart: unless-stopped volumes: - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml networks: - npm_default networks: npm_default: external: true Launch it: docker compose up -d\nStep 3: Configure Prometheus Finally, tell Prometheus to send alerts to Alertmanager and load your rules.\nCreate your rules file, ~/docker/monitoring/alert_rules.yml, with rules for \u0026ldquo;Instance Down,\u0026rdquo; \u0026ldquo;High CPU,\u0026rdquo; \u0026ldquo;Low Disk Space,\u0026rdquo; etc.\ncd ~/docker/monitoring nano alert_rules.yml Add the alert_rules.yml as a volume in your ~/docker/monitoring/docker-compose.yml.\nvolumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - ./alert_rules.yml:/etc/prometheus/alert_rules.yml - prometheus_data:/prometheus Add the alerting and rule_files blocks to your ~/docker/monitoring/prometheus.yml:\ngroups: -name: Critical System Alerts interval: 30s rules: - alert: InstanceDown expr: up == 0 for: 2m labels: severity: critical annotations: summary: \u0026#34;🔴 Instance {{ $labels.instance }} is DOWN\u0026#34; description: \u0026#34;Service {{ $labels.job }} has been unreachable for 2 minutes.\u0026#34; - alert: LaptopOnBattery expr: node_power_supply_online == 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;🔋 Server running on BATTERY\u0026#34; description: \u0026#34;Homelab has been unplugged for 5 minutes. Check power connection!\u0026#34; - alert: LowBatteryLevel expr: node_power_supply_capacity \u0026lt; 20 and node_power_supply_online == 0 for: 1m labels: severity: critical annotations: summary: \u0026#34;⚠️ CRITICAL: Battery at {{ $value }}%\u0026#34; description: \u0026#34;Battery below 20%. Server may shut down soon!\u0026#34; - alert: DiskAlmostFull expr: (node_filesystem_avail_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;} / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;}) * 100 \u0026lt; 10 for: 5m labels: severity: critical annotations: summary: \u0026#34;💾 Disk space critically low: {{ $value | humanize }}% remaining\u0026#34; description: \u0026#34;Root filesystem has less than 10% free space.\u0026#34; - alert: OutOfMemory expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 \u0026lt; 5 for: 2m labels: severity: critical annotations: summary: \u0026#34;🧠 Memory critically low: {{ $value | humanize }}% available\u0026#34; description: \u0026#34;Less than 5% memory available. System may become unresponsive.\u0026#34; - alert: CriticalCpuTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;coretemp\u0026#34;} \u0026gt; 95 for: 2m labels: severity: critical annotations: summary: \u0026#34;🔥 CRITICAL CPU Temperature: {{ $value }}°C\u0026#34; description: \u0026#34;CPU temperature exceeds 95°C. Thermal throttling or shutdown imminent!\u0026#34; - name: Warning System Alerts interval: 1m rules: - alert: HighCpuUsage expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[5m])) * 100) \u0026gt; 80 for: 5m labels: severity: warning annotations: summary: \u0026#34;⚡ High CPU usage: {{ $value | humanize }}%\u0026#34; description: \u0026#34;CPU usage above 80% for 5 minutes on {{ $labels.instance }}\u0026#34; - alert: HighSystemLoad expr: node_load5 / on(instance) count(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}) by (instance) \u0026gt; 1.5 for: 10m labels: severity: warning annotations: summary: \u0026#34;📊 High system load: {{ $value | humanize }}\u0026#34; description: \u0026#34;5-minute load average is 1.5x CPU cores for 10 minutes.\u0026#34; - alert: HighMemoryUsage expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 \u0026lt; 20 for: 5m labels: severity: warning annotations: summary: \u0026#34;🧠 High memory usage: {{ $value | humanize }}% available\u0026#34; description: \u0026#34;Less than 20% memory available.\u0026#34; - alert: HighCpuTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;coretemp\u0026#34;} \u0026gt; 85 for: 5m labels: severity: warning annotations: summary: \u0026#34;🌡️ High CPU temperature: {{ $value }}°C\u0026#34; description: \u0026#34;CPU temperature above 85°C. Consider improving cooling.\u0026#34; - alert: HighNvmeTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;nvme\u0026#34;} \u0026gt; 65 for: 10m labels: severity: warning annotations: summary: \u0026#34;💿 High NVMe temperature: {{ $value }}°C\u0026#34; description: \u0026#34;NVMe drive temperature above 65°C for 10 minutes.\u0026#34; - alert: DiskSpaceLow expr: (node_filesystem_avail_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;} / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;}) * 100 \u0026lt; 20 for: 10m labels: severity: warning annotations: summary: \u0026#34;💾 Disk space low: {{ $value | humanize }}% remaining\u0026#34; description: \u0026#34;Root filesystem has less than 20% free space.\u0026#34; - alert: HighSwapUsage expr: ((node_memory_SwapTotal_bytes - node_memory_SwapFree_bytes) / node_memory_SwapTotal_bytes * 100) \u0026gt; 50 for: 10m labels: severity: warning annotations: summary: \u0026#34;💱 High swap usage: {{ $value | humanize }}%\u0026#34; description: \u0026#34;Swap usage above 50%. System may be memory-constrained.\u0026#34; # Monitor your USB-C hub ethernet adapter (enx00) - alert: EthernetInterfaceDown expr: node_network_up{device=\u0026#34;enx00\u0026#34;} == 0 for: 2m labels: severity: warning annotations: summary: \u0026#34;🌐 USB-C Ethernet adapter is DISCONNECTED\u0026#34; description: \u0026#34;Your USB-C hub ethernet connection (enx00) is down. Check cable or hub.\u0026#34; - alert: HighNetworkErrors expr: rate(node_network_receive_errs_total{device=\u0026#34;enx00\u0026#34;}[5m]) \u0026gt; 10 or rate(node_network_transmit_errs_total{device=\u0026#34;enx00\u0026#34;}[5m]) \u0026gt; 10 for: 5m labels: severity: warning annotations: summary: \u0026#34;🌐 High network errors on USB-C ethernet\u0026#34; description: \u0026#34;Your ethernet adapter is experiencing high error rate. Check cable quality.\u0026#34; - name: Docker Container Alerts interval: 1m rules: # Simplified alert - just checks if container exporter is working - alert: ContainerMonitoringDown expr: absent(container_last_seen) for: 2m labels: severity: warning annotations: summary: \u0026#34;🐳 Container monitoring is down\u0026#34; description: \u0026#34;cAdvisor or container metrics are not available. Check if containers are being monitored.\u0026#34; - alert: ContainerRestarting expr: rate(container_start_time_seconds[5m]) \u0026gt; 0.01 for: 2m labels: severity: warning annotations: summary: \u0026#34;🐳 Container {{ $labels.name }} is restarting\u0026#34; description: \u0026#34;Container {{ $labels.name }} has restarted recently.\u0026#34; - alert: ContainerHighCpu expr: rate(container_cpu_usage_seconds_total{name!~\u0026#34;.*POD.*\u0026#34;,name!=\u0026#34;\u0026#34;}[5m]) * 100 \u0026gt; 80 for: 10m labels: severity: warning annotations: summary: \u0026#34;🐳 Container {{ $labels.name }} high CPU: {{ $value | humanize }}%\u0026#34; description: \u0026#34;Container CPU usage above 80% for 10 minutes.\u0026#34; Restart Prometheus to apply the changes:\ncd ~/docker/monitoring docker compose up -d --force-recreate prometheus Now, if any service fails or your server\u0026rsquo;s resources run low, you will get an instant notification in Discord.\nStep 3: The Critical Firewall Fix You may find your alerts are not sending. This is often due to a conflict between Docker and ufw.\nOpen the main ufw configuration file:\nsudo nano /etc/default/ufw Change DEFAULT_FORWARD_POLICY=\u0026quot;DROP\u0026quot; to DEFAULT_FORWARD_POLICY=\u0026quot;ACCEPT\u0026quot;.\nReload the firewall:\nsudo ufw reload Restart your containers that need internet access:\ndocker compose restart Now, if any service fails or your server\u0026rsquo;s resources run low, you will get an instant notification in Discord.\nConclusion Our homelab has now truly come to life. It\u0026rsquo;s no longer just a collection of services but a resilient, self-maintaining platform. With automated backups to Google Drive, daily updates via Watchtower, and proactive alerts with Prometheus and Alertmanager, our server can now run 24/7 with minimal manual intervention. We\u0026rsquo;ve built a solid, reliable, and intelligent system.\nBut there\u0026rsquo;s one critical piece still missing: end-to-end security for our local services.\nRight now, we\u0026rsquo;re accessing our dashboards at addresses like http://grafana.local, which browsers flag as \u0026ldquo;Not Secure.\u0026rdquo; What if we could use a real, public domain name for our internal services and get a valid HTTPS certificate, all without opening a single port on our router?\nIn the next part of this series, I\u0026rsquo;ll show you exactly how to do that. We\u0026rsquo;ll dive into an advanced but powerful setup using Cloudflare and Nginx Proxy Manager to bring trusted, zero-exposure SSL to everything we\u0026rsquo;ve built.\nStay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-4-automation-and-alerting/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to the part 4 of the homelab series! In the previous parts, we built a server, deployed a suite of services, and configured our network. Now, it\u0026rsquo;s time to make it resilient and self-maintaining. A homelab isn\u0026rsquo;t just about setting things up; it\u0026rsquo;s about keeping them running reliably.\u003c/p\u003e\n\u003cp\u003eThis guide will show you how to set up the three pillars of modern IT operations: \u003cstrong\u003eAutomated Backups\u003c/strong\u003e, \u003cstrong\u003eAutomated Updates\u003c/strong\u003e, and \u003cstrong\u003eProactive Alerting\u003c/strong\u003e. By the end, you\u0026rsquo;ll have a homelab that runs itself, ensures your data is safe, stays up-to-date, and notifies you when something goes wrong.\u003c/p\u003e","title":"Part 4: Automating a Homelab with Backups, Updates, and Alerts"},{"content":"My New Weekend Project: Building a Personal Ad-Blocking Server in the Cloud! Hey everyone, Prajwol here.\nLike a lot of you, I spend a good chunk of my day online. And lately, it\u0026rsquo;s felt like I\u0026rsquo;m in a constant battle with pop-ups, trackers, and auto-playing video ads. I\u0026rsquo;ve used browser extensions for years, but I wanted a more powerful solution—something that would protect my entire home network, including my phone and smart TV, without needing to install software everywhere.\nSo, I decided to take on a new project: building my very own ad-blocking DNS server in the cloud.\nI\u0026rsquo;d done something similar a while back with Linode, but this time I wanted to dive into the world of Amazon Web Services (AWS) and see if I could build a reliable, secure, and cost-effective setup from scratch. It turned into quite the adventure, involving a late-night session of launching a virtual server, wrestling with firewalls, and securing my own private domain with an SSL certificate.\nThe end result? It\u0026rsquo;s been fantastic. My web pages load noticeably faster, and the general online experience feels so much cleaner and less intrusive. Plus, knowing that I have full control over my own corner of the internet is incredibly satisfying. It\u0026rsquo;s a great feeling to see the query logs fill up with blocked requests for domains I\u0026rsquo;ve never even heard of!\nI documented every single step of my journey, from the first click in the AWS console to the final configuration on my home router. If you\u0026rsquo;re curious about how to build one for yourself, I\u0026rsquo;ve written up a complete, step-by-step guide.\nYou can check out the full project guide here! It was a challenging but really rewarding project. Let me know what you think!\nPublished: Friday, August 22, 2025\n","permalink":"http://localhost:1313/blog/adguard-aws/","summary":"\u003ch1 id=\"my-new-weekend-project-building-a-personal-ad-blocking-server-in-the-cloud\"\u003eMy New Weekend Project: Building a Personal Ad-Blocking Server in the Cloud!\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eLike a lot of you, I spend a good chunk of my day online. And lately, it\u0026rsquo;s felt like I\u0026rsquo;m in a constant battle with pop-ups, trackers, and auto-playing video ads. I\u0026rsquo;ve used browser extensions for years, but I wanted a more powerful solution—something that would protect my entire home network, including my phone and smart TV, without needing to install software everywhere.\u003c/p\u003e","title":"Building a Personal Ad-Blocking Server in the Cloud!"},{"content":"Introduction Welcome to Part 3 of my homelab series! In the previous parts, I built my server and deployed a suite of management and monitoring tools. Now, it\u0026rsquo;s time to build the brain of my network: a robust, redundant, and high-availability DNS system using AdGuard Home that works both at home and on the go.\nIn this detailed guide, I\u0026rsquo;ll walk you through how I deployed a total of three AdGuard Home instances, each with its own unique IP address. I set up a primary resolver on my homelab, a secondary failover resolver in the cloud for my mobile devices, and a tertiary resolver on a separate virtual network for local redundancy.\nChapter 1: The Local Workhorse (Primary DNS) I started by deploying my main, day-to-day DNS resolver on my homelab server.\nStep 1: Deploying AdGuard Home with Docker Compose First, I SSHed into my server, created a directory for the project, and a docker-compose.yml file to define the service.\nmkdir -p ~/docker/adguard-primary cd ~/docker/adguard-primary nano docker-compose.yml I pasted in the following configuration. This runs the AdGuard Home container, maps all the necessary ports for DNS and the web UI, and connects it to the shared npm_default network I set up in Part 2.\nservices: adguardhome: image: adguard/adguardhome:latest container_name: adguard-primary restart: unless-stopped ports: - \u0026#34;53:53/tcp\u0026#34; - \u0026#34;53:53/udp\u0026#34; - \u0026#34;8080:80/tcp\u0026#34; # Web UI - \u0026#34;853:853/tcp\u0026#34; # DNS-over-TLS volumes: - ./workdir:/opt/adguardhome/work - ./confdir:/opt/adguardhome/conf networks: - npm_default networks: npm_default: external: true I then launched the container by running:\ndocker compose up -d Step 2: Initial AdGuard Home Setup Wizard I navigated to http://\u0026lt;your-server-ip\u0026gt;:3000 in my web browser to start the setup wizard.\nI clicked \u0026ldquo;Get Started.\u0026rdquo;\nOn the \u0026ldquo;Admin Web Interface\u0026rdquo; screen, I changed the \u0026ldquo;Listen Interface\u0026rdquo; to All interfaces and the port to 80.\nOn the \u0026ldquo;DNS server\u0026rdquo; screen, I changed the \u0026ldquo;Listen Interface\u0026rdquo; to All interfaces and left the port as 53.\nI followed the prompts to create my admin username and password.\nOnce the setup was complete, I was redirected to my main dashboard, now available at http://\u0026lt;your-server-ip\u0026gt;:8080.\nStep 3: Configure My Home Router To make all my devices use AdGuard automatically, I logged into my home router\u0026rsquo;s admin panel, found the DHCP Server settings, and changed the Primary DNS Server to my homelab\u0026rsquo;s static IP address (e.g., 192.168.1.10).\nChapter 2: The Cloud Failover (Secondary DNS on Oracle Cloud) An off-site DNS server ensures I have ad-blocking on my mobile devices and acts as a backup.\nWhy I Chose Oracle Cloud After testing the free tiers of both AWS and Linode, I chose Oracle Cloud Infrastructure (OCI). In my experience, OCI\u0026rsquo;s \u0026ldquo;Always Free\u0026rdquo; tier is far more generous with its resources. It provides powerful Ampere A1 Compute instances with up to 4 CPU cores and 24 GB of RAM, plus 200 GB of storage and significant bandwidth, all for free. This was ideal for running my service 24/7 without the strict limitations or eventual costs associated with other providers.\nStep 1: Launching the Oracle Cloud VM Sign Up: I created my account on the Oracle Cloud website.\nCreate VM Instance: In the OCI console, I navigated to Compute \u0026gt; Instances and clicked \u0026ldquo;Create instance\u0026rdquo;.\nConfigure Instance:\nName: I gave it a name like AdGuard-Cloud.\nImage and Shape: I clicked \u0026ldquo;Edit\u0026rdquo;. For the image, I selected Ubuntu. For the shape, I selected \u0026ldquo;Ampere\u0026rdquo; and chose the VM.Standard.A1.Flex shape (it\u0026rsquo;s \u0026ldquo;Always Free-eligible\u0026rdquo;).\nNetworking: I used the default VCN and made sure \u0026ldquo;Assign a public IPv4 address\u0026rdquo; was checked.\nSSH Keys: I added my SSH public key.\nI clicked Create. Once the instance was running, I took note of its Public IP Address.\nStep 2: Configuring the Cloud Firewall For maximum security, I locked down the administrative ports to only my home IP address.\nFind My Public IP: I went to a site like whatismyip.com and copied my home\u0026rsquo;s public IP address.\nEdit Security List: I navigated to my instance\u0026rsquo;s details page, clicked the subnet link, then clicked the \u0026ldquo;Security List\u0026rdquo; link.\nI clicked \u0026ldquo;Add Ingress Rules\u0026rdquo; and added the following rules:\nFor SSH (Port 22): I set the Source to my home\u0026rsquo;s public IP, followed by /32 (e.g., 203.0.113.55/32). This is a critical security step.\nFor AdGuard Setup (Port 3000): I also set the Source to my home\u0026rsquo;s public IP with /32.\nFor AdGuard Web UI (Port 80/443): I set the Source to my home\u0026rsquo;s public IP with /32 as well.\nFor Public DNS (Port 53, 853, etc.): I set the Source to 0.0.0.0/0 (Anywhere) to allow all my devices to connect from any network.\nStep 3: Installing AdGuard Home \u0026amp; Configuring SSL Connect via SSH: I used the public IP and my SSH key to connect to the VM.\nRun Install Script: I chose to install AdGuard Home directly on the OS for this instance.\ncurl -s -S -L [https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh](https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh) | sh -s -- -v Get a Hostname: I went to No-IP.com, created a free hostname (e.g., my-cloud-dns.ddns.net), and pointed it to my cloud VM\u0026rsquo;s public IP.\nEnable Encryption: In my cloud AdGuard\u0026rsquo;s dashboard (Settings \u0026gt; Encryption settings), I enabled encryption, entered my No-IP hostname, and used the built-in function to request a Let\u0026rsquo;s Encrypt certificate.\nStep 4: Creating a Cloud Backup (Snapshot) A critical final step for any cloud service is creating a backup. Here is how I did it in OCI:\nIn the OCI Console, I navigated to the details page for my AdGuard-Cloud instance.\nUnder the \u0026ldquo;Resources\u0026rdquo; menu on the left, I clicked on \u0026ldquo;Boot volume\u0026rdquo;.\nOn the Boot Volume details page, under \u0026ldquo;Resources,\u0026rdquo; I clicked \u0026ldquo;Boot volume backups\u0026rdquo;.\nI clicked the \u0026ldquo;Create boot volume backup\u0026rdquo; button.\nI gave the backup a descriptive name (e.g., AdGuard-Cloud-Backup-YYYY-MM-DD) and clicked the create button. This creates a full snapshot of my server that I can use to restore it in minutes.\nStep 5: ### How to Use Your Cloud DNS on Mobile Devices The main benefit of the cloud server is having ad-blocking on the go. Here’s how I set it up on my mobile phone using secure, encrypted DNS.\nFor Android (Version 9+): Modern Android has a built-in feature called \u0026ldquo;Private DNS\u0026rdquo; that uses DNS-over-TLS (DoT), which is perfect for this.\nOpen Settings on your Android device.\nTap on \u0026ldquo;Network \u0026amp; internet\u0026rdquo; (this may be called \u0026ldquo;Connections\u0026rdquo; on some devices).\nFind and tap on \u0026ldquo;Private DNS\u0026rdquo;. You may need to look under an \u0026ldquo;Advanced\u0026rdquo; section.\nSelect the option labeled \u0026ldquo;Private DNS provider hostname\u0026rdquo;.\nIn the text box, enter the No-IP hostname you created for your Oracle Cloud server (e.g., my-cloud-dns.ddns.net).\nTap Save.\nYour phone will now send all its DNS queries through an encrypted tunnel to your personal AdGuard Home server in the cloud, giving you ad-blocking on both Wi-Fi and cellular data.\nFor iOS (iPhone/iPad): On iOS, the easiest way to set up encrypted DNS is by installing a configuration profile.\nOn your iPhone or iPad, open Safari.\nGo to a DNS profile generator site, like the one provided by AdGuard.\nWhen prompted, enter the DNS-over-HTTPS (DoH) address for your cloud server. It will be your No-IP hostname with /dns-query at the end (e.g., https://my-cloud-dns.ddns.net/dns-query).\nDownload the generated configuration profile.\nGo to your device\u0026rsquo;s Settings app. You will see a new \u0026ldquo;Profile Downloaded\u0026rdquo; item near the top. Tap on it.\nFollow the on-screen prompts to Install the profile. You may need to enter your device passcode.\nOnce installed, your iOS device will also route its DNS traffic through your secure cloud server.\nChapter 3: Ultimate Local Redundancy (Tertiary DNS with Macvlan) For an extra layer of redundancy within my homelab, I created a third AdGuard instance. By using an advanced Docker network mode called macvlan, this container gets its own unique IP address on my home network, making it a truly independent resolver.\nCreate Macvlan Network: First, I created the macvlan network, telling it which of my physical network cards to use (eth0 in my case).\ndocker network create -d macvlan \\ --subnet=192.168.1.0/24 \\ --gateway=192.168.1.1 \\ -o parent=eth0 homelab_net Deploy Tertiary Instance: I created a new folder (~/docker/adguard-tertiary) and this docker-compose.yml. Notice there are no ports since the container gets its own IP.\nservices: adguardhome2: image: adguard/adguardhome:latest container_name: adguardhome2 volumes: - \u0026#34;./work:/opt/adguardhome/work\u0026#34; - \u0026#34;./conf:/opt/adguardhome/conf\u0026#34; networks: homelab_net: ipv4_address: 192.168.1.11 # The new, unique IP for this container restart: unless-stopped networks: homelab_net: external: true Configure Router for Local Failover: To complete the local redundancy, I went back into my router\u0026rsquo;s DHCP settings.\nIn the Primary DNS field, I have the IP of my main homelab server (e.g., 192.168.1.10).\nIn the Secondary DNS field, I entered the unique IP address I assigned to my macvlan container (e.g., 192.168.1.11).\nNow, if my primary AdGuard container has an issue, all devices on my network will automatically fail over to the tertiary instance.\nChapter 4: Fine-Tuning and Integration Finally, I implemented some best practices on my primary AdGuard Home instance.\nUpstream DNS Servers: Under Settings \u0026gt; DNS Settings, I configured AdGuard to send requests to multiple resolvers in parallel for speed and reliability, using Cloudflare (1.1.1.1), Google (8.8.8.8), and Quad9 (9.9.9.9).\nEnable DNSSEC: In the same settings page, I enabled DNSSEC to verify the integrity of DNS responses.\nDNS Blocklists: I added several popular lists from the \u0026ldquo;Filters \u0026gt; DNS blocklists\u0026rdquo; page, including the AdGuard DNS filter and the OISD Blocklist, for robust protection.\nDNS Rewrites for Local Services: This is the key to a clean homelab experience. For each service, I performed a detailed two-step process:\nCreate the Proxy Host in Nginx Proxy Manager: I logged into my NPM admin panel, went to Hosts \u0026gt; Proxy Hosts, and clicked \u0026ldquo;Add Proxy Host\u0026rdquo;. For my Homer dashboard, I set the Forward Hostname to homer (the container name) and the Forward Port to 8080 (its internal port), using homer.local as the domain name.\nCreate the DNS Rewrite in AdGuard Home: I logged into my primary AdGuard dashboard, went to Filters \u0026gt; DNS Rewrites, and clicked \u0026ldquo;Add DNS rewrite\u0026rdquo;. I entered homer.local as the domain and the IP address of my Nginx Proxy Manager server as the answer.\nConclusion I\u0026rsquo;ve now built an incredibly robust, multi-layered DNS infrastructure. My home devices use the primary local server, which is backed up by a second, independent local server, and my mobile devices use a completely separate cloud instance for on-the-go protection. This provides a resilient, secure, and ad-free internet experience.\nIn the final part of this series, we\u0026rsquo;ll shift our focus from deploying services to maintaining them. I\u0026rsquo;ll show you how I set up a fully automated operations pipeline for my homelab, including daily off-site backups, automatic container updates with Watchtower, and proactive alerting with Prometheus. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-3-high-availability-dns/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to Part 3 of my homelab series! In the previous parts, I built my server and deployed a suite of management and monitoring tools. Now, it\u0026rsquo;s time to build the brain of my network: a robust, redundant, and high-availability DNS system using \u003cstrong\u003eAdGuard Home\u003c/strong\u003e that works both at home and on the go.\u003c/p\u003e\n\u003cp\u003eIn this detailed guide, I\u0026rsquo;ll walk you through how I deployed a total of \u003cstrong\u003ethree\u003c/strong\u003e AdGuard Home instances, each with its own unique IP address. I set up a primary resolver on my homelab, a secondary failover resolver in the cloud for my mobile devices, and a tertiary resolver on a separate virtual network for local redundancy.\u003c/p\u003e","title":"Part 3: A High-Availability DNS Network with AdGuard Home"},{"content":"New Project Alert: Running a Powerful AI Locally with Docker! Hey everyone, Prajwol here.\nI\u0026rsquo;ve always been fascinated by the incredible advancements in AI and large language models. While cloud-based models are powerful, I was really curious about what it would take to run a high-performance model right on my own machine. This gives you ultimate privacy, control, and the ability to experiment without limits.\nSo, for my latest project, I decided to dive in and get the DeepSeek-R1 model, a powerful AI, up and running locally using Docker.\nDocker is an amazing tool that lets you package up applications and all their dependencies into a neat little box called a container. This means you can run complex software without the headache of complicated installations or conflicts with other programs on your system. It was the perfect way to tame this powerful AI and get it running smoothly on my Ubuntu machine.\nThe process was a fantastic learning experience, covering everything from setting up Docker to pulling the model and interacting with the AI. It’s amazing to have that kind of power running on your own hardware.\nI’ve documented my entire process in a detailed, step-by-step guide. If you’re interested in local AI and want to see how you can run a powerful model yourself, be sure to check it out!\nYou can find the full project guide right here! Let me know what you think of this one!\nPublished: February, 2025\n","permalink":"http://localhost:1313/blog/running-deepseek-r1-on-docker-container-on-ubuntu/","summary":"\u003ch1 id=\"new-project-alert-running-a-powerful-ai-locally-with-docker\"\u003eNew Project Alert: Running a Powerful AI Locally with Docker!\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve always been fascinated by the incredible advancements in AI and large language models. While cloud-based models are powerful, I was really curious about what it would take to run a high-performance model right on my own machine. This gives you ultimate privacy, control, and the ability to experiment without limits.\u003c/p\u003e\n\u003cp\u003eSo, for my latest project, I decided to dive in and get the \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e model, a powerful AI, up and running locally using \u003cstrong\u003eDocker\u003c/strong\u003e.\u003c/p\u003e","title":"Running a Powerful AI Locally with Docker!"},{"content":"Introduction Welcome to Part 2 of my homelab series! In Part 1, we built a solid foundation by turning an old laptop into a hardened Debian server with Docker. Now that our server is running, we need to deploy services to manage, monitor, and easily access our projects.\nIn this guide, we\u0026rsquo;ll deploy three essential stacks. First, Nginx Proxy Manager (NPM) will act as our server\u0026rsquo;s front door and create a shared network for our containers. Second, we\u0026rsquo;ll set up a professional-grade monitoring stack with Prometheus and Grafana. Finally, we\u0026rsquo;ll deploy a Homer dashboard to create a beautiful and convenient launchpad for all our services.\n1. The Management Layer: Nginx Proxy Manager (NPM) 🌐 Before we can deploy our other services, we need a way to manage connections between them. NPM will act as our reverse proxy and, crucially, will create the shared Docker network that all our other services will connect to.\nA. Deploy Nginx Proxy Manager First, let\u0026rsquo;s create a directory and the docker-compose.yml file for NPM.\n# Create the directory mkdir -p ~/docker/npm cd ~/docker/npm # Create the docker-compose.yml nano docker-compose.yml Paste in the following configuration. This file defines the NPM service and creates a network named npm_default.\nservices: app: image: \u0026#39;jc21/nginx-proxy-manager:latest\u0026#39; container_name: npm-app-1 restart: unless-stopped ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;81:81\u0026#39; volumes: - ./data:/data - ./letsencrypt:/etc/letsencrypt networks: default: name: npm_default Launch it with\ndocker compose up -d You can now log in to the admin UI at http://\u0026lt;your-server-ip\u0026gt;:81.\n2. The Monitoring Stack 📊 With our shared network in place, we can now deploy our monitoring stack.\nPrometheus: Collects all the metrics.\nNode Exporter: Exposes the server\u0026rsquo;s hardware metrics.\ncAdvisor: Exposes Docker container metrics.\nGrafana: Visualizes all the data in beautiful dashboards.\nA. Create the Prometheus Configuration Prometheus needs a config file to know what to monitor.\n# Create the project directory mkdir -p ~/docker/monitoring cd ~/docker/monitoring # Create the prometheus.yml file nano prometheus.yml Paste in the following configuration:\nglobal: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;node-exporter\u0026#39; static_configs: - targets: [\u0026#39;node-exporter:9100\u0026#39;] - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] B. Deploy the Stack with Docker Compose Next, create the docker-compose.yml file in the same ~/docker/monitoring directory.\nnano docker-compose.yml This file defines all four monitoring services and tells them to connect to the npm_default network we created earlier.\nservices: prometheus: image: prom/prometheus:latest container_name: prometheus restart: unless-stopped ports: - \u0026#34;9090:9090\u0026#34; volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - prometheus_data:/prometheus networks: - default grafana: image: grafana/grafana:latest container_name: grafana restart: unless-stopped ports: - \u0026#34;3001:3000\u0026#34; volumes: - grafana_data:/var/lib/grafana networks: - default node-exporter: image: prom/node-exporter:latest container_name: node-exporter restart: unless-stopped volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - \u0026#39;--path.procfs=/host/proc\u0026#39; - \u0026#39;--path.sysfs=/host/sys\u0026#39; - \u0026#39;--path.rootfs=/rootfs\u0026#39; - \u0026#39;--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)\u0026#39; networks: - default cadvisor: image: gcr.io/cadvisor/cadvisor:latest container_name: cadvisor restart: unless-stopped ports: - \u0026#34;8081:8080\u0026#34; volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro networks: - default volumes: prometheus_data: grafana_data: networks: default: name: npm_default external: true Now, launch the stack:\ndocker compose up -d C. Configure Grafana Log in to Grafana at http://\u0026lt;your-server-ip\u0026gt;:3001 (default: admin/admin).\nAdd Data Source: Go to Connections \u0026gt; Data Sources, add a Prometheus source, and set the URL to http://prometheus:9090.\nImport Dashboards: Go to Dashboards \u0026gt; New \u0026gt; Import and add these dashboards by ID:\nNode Exporter Full (ID: 1860)\nDocker Host/Container Metrics (ID: 193)\n3. The Homer Launchpad Dashboard 🚀 Finally, let\u0026rsquo;s deploy Homer as our beautiful start page with custom icons.\nCreate Directories \u0026amp; Download Icons: First, create a directory for Homer and an assets subdirectory. Then, cd into the assets folder and download the icons.\nmkdir -p ~/docker/homer/assets cd ~/docker/homer/assets wget -O grafana.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/grafana.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/grafana.png) wget -O prometheus.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/prometheus.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/prometheus.png) wget -O cadvisor.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/cadvisor.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/cadvisor.png) wget -O npm.png [https://nginxproxymanager.com/icon.png](https://nginxproxymanager.com/icon.png) Create Configuration: Go back to the main homer directory and create the config.yml file.\ncd ~/docker/homer nano config.yml Paste in the following configuration. The logo: lines point to the icons we just downloaded.\n--- title: \u0026#34;Homelab Dashboard\u0026#34; subtitle: \u0026#34;Server Management\u0026#34; theme: \u0026#34;dark\u0026#34; services: - name: \u0026#34;Management\u0026#34; icon: \u0026#34;fas fa-server\u0026#34; items: - name: \u0026#34;Nginx Proxy Manager\u0026#34; logo: \u0026#34;assets/tools/npm.png\u0026#34; subtitle: \u0026#34;Reverse Proxy Admin\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:81\u0026#34; - name: \u0026#34;Monitoring\u0026#34; icon: \u0026#34;fas fa-chart-bar\u0026#34; items: - name: \u0026#34;Grafana\u0026#34; logo: \u0026#34;assets/tools/grafana.png\u0026#34; subtitle: \u0026#34;Metrics Dashboard\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:3001\u0026#34; - name: \u0026#34;Prometheus\u0026#34; logo: \u0026#34;assets/tools/prometheus.png\u0026#34; subtitle: \u0026#34;Metrics Database\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:9090\u0026#34; - name: \u0026#34;cAdvisor\u0026#34; logo: \u0026#34;assets/tools/cadvisor.png\u0026#34; subtitle: \u0026#34;Container Metrics\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:8081\u0026#34; Create Docker Compose File: Finally, create the docker-compose.yml file.\nnano docker-compose.yml This configuration connects Homer to our shared network.\nservices: homer: image: b4bz/homer container_name: homer volumes: - ./config.yml:/www/assets/config.yml - ./assets:/www/assets/tools ports: - \u0026#34;8090:8080\u0026#34; restart: unless-stopped networks: - npm_default networks: npm_default: external: true Launch: Run docker compose up -d. You can now access your new dashboard with custom icons at http://\u0026lt;your-server-ip\u0026gt;:8090.\nConclusion Our homelab now has a powerful management and monitoring foundation. Nginx Proxy Manager is ready to direct traffic, Grafana is visualizing our server\u0026rsquo;s health, and Homer provides a central launchpad.\nIn the next part of the series, we\u0026rsquo;ll deploy our core network service, AdGuard Home, and use NPM to create clean, memorable local domains for all the applications we set up today. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-2-monitoring-and-management/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to Part 2 of my homelab series! In \u003ca href=\"/projects/homelab-series-part-1-debian-docker-foundation/\"\u003ePart 1\u003c/a\u003e, we built a solid foundation by turning an old laptop into a hardened Debian server with Docker. Now that our server is running, we need to deploy services to manage, monitor, and easily access our projects.\u003c/p\u003e\n\u003cp\u003eIn this guide, we\u0026rsquo;ll deploy three essential stacks. First, \u003cstrong\u003eNginx Proxy Manager (NPM)\u003c/strong\u003e will act as our server\u0026rsquo;s front door and create a shared network for our containers. Second, we\u0026rsquo;ll set up a professional-grade monitoring stack with \u003cstrong\u003ePrometheus\u003c/strong\u003e and \u003cstrong\u003eGrafana\u003c/strong\u003e. Finally, we\u0026rsquo;ll deploy a \u003cstrong\u003eHomer\u003c/strong\u003e dashboard to create a beautiful and convenient launchpad for all our services.\u003c/p\u003e","title":"Part 2: Homelab Management \u0026 Monitoring"},{"content":"My First Cloud Ad-Blocker: A Look Back at AdGuard Home on Linode Hey everyone, Prajwol here.\nAs I continue to explore different cloud projects, I often think back to the ones that had the biggest impact on my day-to-day life. One of the very first projects that truly changed my internet experience was setting up my own ad-blocking DNS server using AdGuard Home on a Linode instance.\nMy goal was to find a simple, cost-effective way to block ads and trackers across my entire home network. I wanted a \u0026ldquo;set it and forget it\u0026rdquo; solution that would cover every device—from my phone to my smart TV—without needing to install an app on each one. Linode (now Akamai) was the perfect platform for this: straightforward, powerful, and great for hosting a lightweight service like AdGuard Home.\nThe process of spinning up a small server, running a single installation script, and then seeing the query logs light up with blocked requests was incredibly satisfying. It felt like I had taken back a real measure of control over my own network.\nThis project remains a fantastic entry point for anyone wanting to get started with self-hosting and network privacy. I\u0026rsquo;ve kept the original, detailed guide for anyone who wants to follow along.\nYou can find the full step-by-step project guide here! It’s a rewarding project that delivers tangible results almost immediately. Let me know if you give it a try!\nPublished: Friday, August 22, 2025\n","permalink":"http://localhost:1313/blog/adguard-home-on-cloud/","summary":"\u003ch1 id=\"my-first-cloud-ad-blocker-a-look-back-at-adguard-home-on-linode\"\u003eMy First Cloud Ad-Blocker: A Look Back at AdGuard Home on Linode\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eAs I continue to explore different cloud projects, I often think back to the ones that had the biggest impact on my day-to-day life. One of the very first projects that truly changed my internet experience was setting up my own ad-blocking DNS server using AdGuard Home on a Linode instance.\u003c/p\u003e\n\u003cp\u003eMy goal was to find a simple, cost-effective way to block ads and trackers across my entire home network. I wanted a \u0026ldquo;set it and forget it\u0026rdquo; solution that would cover every device—from my phone to my smart TV—without needing to install an app on each one. Linode (now Akamai) was the perfect platform for this: straightforward, powerful, and great for hosting a lightweight service like AdGuard Home.\u003c/p\u003e","title":"My First Cloud Ad-Blocker - A Look Back at AdGuard Home on Linode"},{"content":"Introduction Welcome to the first post in my new homelab series! I\u0026rsquo;ve always been fascinated by self-hosting and DevOps, and I believe the best way to learn is by doing. In this series, I\u0026rsquo;ll document my journey of turning an old, unused laptop into a powerful, efficient, and secure bare-metal server for hosting a variety of network services.\nThe goal for this first part is to lay a solid foundation. We\u0026rsquo;ll take an old laptop, install a minimal and stable Linux operating system, perform some initial security hardening, and set up Docker as our containerization engine. By the end of this post, we\u0026rsquo;ll have a perfect blank canvas ready for the exciting services we\u0026rsquo;ll deploy in the upcoming parts.\n1. Choosing the Hardware \u0026amp; OS Why an Old Laptop? Before diving in, why use an old laptop instead of a Raspberry Pi or a dedicated server? For a starter homelab, a laptop has three huge advantages:\nCost-Effective: It\u0026rsquo;s free if you have one lying around! Built-in UPS: The battery acts as a built-in Uninterruptible Power Supply (UPS), keeping the server running through short power outages. Low Power Consumption: Laptop hardware is designed to be power-efficient, which is great for a device that will be running 24/7. Why Debian 13 \u0026ldquo;Trixie\u0026rdquo;? For the operating system, I chose Debian. It\u0026rsquo;s renowned for its stability, security, and massive package repository. It’s the bedrock of many other distributions (like Ubuntu) and is perfect for a server because it\u0026rsquo;s lightweight and doesn\u0026rsquo;t include unnecessary software. We\u0026rsquo;ll be using the minimal \u0026ldquo;net-install\u0026rdquo; to ensure we only install what we absolutely need.\n2. Installation and Network Configuration The installation process is straightforward, but the network setup is key to a reliable server.\nMinimal Installation Create a Bootable USB: I downloaded the Debian 13 \u0026ldquo;netinst\u0026rdquo; ISO from the official website and used Rufus on Windows to create a bootable USB drive. Boot from USB: I plugged the USB into the laptop and booted from it (usually pressing F12, F2, or Esc during startup to select the USB device). Language, Location, and Keyboard: Selected English, United States, and the default keyboard layout. Network Setup: Connected the laptop to my home network (Ethernet preferred for stability). Hostname \u0026amp; Domain: Entered a short, memorable hostname for the server (e.g., homelab) and left the domain blank. User Accounts: Set a root password. Created a non-root regular user (this will be used for daily management). Partition Disks: Chose Guided – use entire disk with separate /home partition. This is simpler for a server setup. Software Selection: At the “Software selection” screen: Unchecked “Debian desktop environment” Checked “SSH server” and “standard system utilities” This ensures a clean command-line system that can be accessed remotely. GRUB Bootloader: Installed GRUB on the primary drive (so the system boots correctly). Finish Installation: Removed the USB drive when prompted and rebooted into the fresh Debian install. Setting a Static IP A server needs a permanent, unchanging IP address. The best way to do this is with DHCP Reservation on your router. This tells your router to always assign the same IP address to your server\u0026rsquo;s unique MAC address.\nFirst, find your laptop’s current IP address and network interface name by running:\nip a You’ll see output similar to:\n2: enp3s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\rinet 192.168.0.45/24 brd 192.168.0.255 scope global dynamic enp3s0\rvalid_lft 86396sec preferred_lft 86396sec In this example:\nInterface name: enp3s0 Current IP: 192.168.0.45 MAC address: shown under link/ether in the same section. With this info, log into your router’s admin panel, find the \u0026ldquo;DHCP Reservation\u0026rdquo; or \u0026ldquo;Static Leases\u0026rdquo; section, and assign a memorable IP address (e.g., 192.168.0.45) to your server’s MAC address.\nThis ensures the server always gets the same IP from your router, making it easy to find on your network.\nConnecting Remotely with SSH With a static IP set, all future management will be done remotely using an SSH client. For Windows, I highly recommend Solar-PuTTY. I created a new session, entered the server\u0026rsquo;s static IP address, my username, and password, and connected.\n3. Initial Server Hardening With a remote SSH session active, the first thing to do is secure the server and configure it for its headless role.\nUpdate the System First, let\u0026rsquo;s make sure all packages are up to date.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y Configure the Firewall ufw (Uncomplicated Firewall) is perfect for a simple setup. We\u0026rsquo;ll set it to deny all incoming traffic by default and only allow SSH connections.\n# Install UFW sudo apt install ufw -y # Allow SSH connections sudo ufw allow ssh # Enable the firewall sudo ufw enable Configure Lid-Close Action To ensure the laptop keeps running when the lid is closed, we edit the logind.conf file.\nsudo nano /etc/systemd/logind.conf Uncomment the line:\nHandleLidSwitch=ignore Save the file, then restart the service:\nsudo systemctl restart systemd-logind.service 4. Installing the Containerization Engine: Docker Instead of installing applications directly on our host, we\u0026rsquo;ll use Docker to keep the system clean and make management easier.\nInstall Docker Engine The official convenience script is the easiest way to get the latest version.\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Add User to Docker Group To run docker commands without sudo, add your user to the docker group. The $USER variable automatically uses the currently logged-in user.\nsudo usermod -aG docker $USER After this, log out and log back in for the change to take effect.\nInstall Docker Compose Docker Compose is essential for managing multi-container applications with a simple YAML file.\nsudo apt install docker-compose-plugin -y To verify the installation:\ndocker compose version Conclusion And that\u0026rsquo;s it for Part 1! We\u0026rsquo;ve successfully turned an old piece of hardware into a hardened, modern server running Debian and Docker with a reliable network configuration. We have a solid and secure foundation to build upon.\nIn the next part of the series, we\u0026rsquo;ll deploy our first critical service: a local, network-wide ad-blocking DNS resolver using AdGuard Home. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-1-debian-docker-foundation/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to the first post in my new homelab series! I\u0026rsquo;ve always been fascinated by self-hosting and DevOps, and I believe the best way to learn is by doing. In this series, I\u0026rsquo;ll document my journey of turning an old, unused laptop into a powerful, efficient, and secure bare-metal server for hosting a variety of network services.\u003c/p\u003e\n\u003cp\u003eThe goal for this first part is to lay a solid foundation. We\u0026rsquo;ll take an old laptop, install a minimal and stable Linux operating system, perform some initial security hardening, and set up Docker as our containerization engine. By the end of this post, we\u0026rsquo;ll have a perfect blank canvas ready for the exciting services we\u0026rsquo;ll deploy in the upcoming parts.\u003c/p\u003e","title":"Part 1: Reviving an Old Laptop with Debian \u0026 Docker"},{"content":"Your Personal Internet Guardian: How to Build a FREE Ad-Blocker in the Cloud! 🚀 Hey everyone! A while back, I wrote a guide on setting up AdGuard Home on Linode. The world of tech moves fast, and it\u0026rsquo;s time for an upgrade! Today, we\u0026rsquo;re going to build our own powerful, network-wide ad-blocker using Amazon Web Services (AWS), and we\u0026rsquo;ll make it secure with our own domain and SSL certificate.\nThink of this as building a digital gatekeeper for your internet. Before any ads, trackers, or malicious sites can reach your devices, our AdGuard Home server will slam the door shut. The best part? This works on your phone, laptop, smart TV—anything on your network—without installing a single app on them.\nThis guide is for everyone, from seasoned tech wizards to curious beginners. We\u0026rsquo;ll break down every step in simple terms, so grab a coffee, and let\u0026rsquo;s build something awesome!\n## Chapter 1: Building Our Home in the AWS Cloud ☁️ First, we need a server. We\u0026rsquo;ll use an Amazon EC2 instance, which is just a fancy name for a virtual computer that you rent.\nSign Up for AWS: If you don\u0026rsquo;t have an account, head to the AWS website and sign up. You\u0026rsquo;ll need a credit card for verification, but for this guide, we can often stay within the Free Tier.\nLaunch Your EC2 Instance:\nLog in to your AWS Console and search for EC2. Click \u0026ldquo;Launch instance\u0026rdquo;. Name: Give your server a cool name, like AdGuard-Server. Application and OS Images: In the search bar, type Debian and select the latest version (e.g., Debian 12). Make sure it\u0026rsquo;s marked \u0026ldquo;Free tier eligible\u0026rdquo;. Instance Type: Choose t2.micro. This is your free, trusty little server. Key Pair (for login): This is your digital key to the server\u0026rsquo;s front door. Click \u0026ldquo;Create a new key pair\u0026rdquo;, name it something like my-adguard-key, and download the .pem file. Keep this file secret and safe! Network settings (The Firewall): This is crucial. We need to tell our server which doors to open. Click \u0026ldquo;Edit\u0026rdquo;. Check the box for \u0026ldquo;Allow SSH traffic from\u0026rdquo; and select My IP. This lets you securely log in. Check \u0026ldquo;Allow HTTPS traffic from the internet\u0026rdquo; and \u0026ldquo;Allow HTTP traffic from the internet\u0026rdquo;. We\u0026rsquo;ll need these for our secure dashboard later. Launch It! Hit the \u0026ldquo;Launch instance\u0026rdquo; button and watch as your new cloud server comes to life.\nGive Your Server a Permanent Address (Elastic IP):\nBy default, your server\u0026rsquo;s public IP address will change every time it reboots. Let\u0026rsquo;s make it permanent! In the EC2 menu on the left, go to \u0026ldquo;Elastic IPs\u0026rdquo;. Click \u0026ldquo;Allocate Elastic IP address\u0026rdquo; and then \u0026ldquo;Allocate\u0026rdquo;. Select the new IP address from the list, click \u0026ldquo;Actions\u0026rdquo;, and then \u0026ldquo;Associate Elastic IP address\u0026rdquo;. Choose your AdGuard-Server instance from the list and click \u0026ldquo;Associate\u0026rdquo;. Your server now has a static IP address that will never change! Make a note of this new IP. ## Chapter 2: Opening the Doors (Configuring the Firewall) 🚪 Our server is running, but for maximum security, we want to ensure only you can access the administrative parts of it. We\u0026rsquo;ll open the public DNS ports to everyone, but lock down the management ports to your home IP address.\nFind Your Public IP Address: Open a new browser tab and go to a site like WhatIsMyIP.com. It will display your home\u0026rsquo;s public IP address. Copy this IP address (it will look something like 203.0.113.55).\nEdit the Firewall Rules: Go to your EC2 Instance details, click the \u0026ldquo;Security\u0026rdquo; tab, and click on the Security Group name.\nClick \u0026ldquo;Edit inbound rules\u0026rdquo; and \u0026ldquo;Add rule\u0026rdquo; for each of the following. This makes sure your DNS is publicly available but the setup panel is locked down to your IP only.\nRule for AdGuard Setup (Port 3000):\nType: Custom TCP Port range: 3000 Source: Paste your IP address here, and add /32 to the end (e.g., 203.0.113.55/32). The /32 tells AWS it\u0026rsquo;s a single, specific IP address. Rule for DNS (Port 53):\nType: Custom UDP and Custom TCP (you will add two separate rules for this port) Port range: 53 Source: Anywhere-IPv4 Rule for DNS-over-TLS (Port 853):\nType: Custom TCP Port range: 853 Source: Anywhere-IPv4 Click \u0026ldquo;Save rules\u0026rdquo;. Your firewall is now configured to allow public DNS requests while keeping your management panel secure.\n## Chapter 3: Installing AdGuard Home 🛡️ Now, let\u0026rsquo;s connect to our server and install the magic software.\nConnect via SSH: Open a terminal (PowerShell on Windows, Terminal on Mac/Linux) and use the key you downloaded to connect. Use your new Elastic IP address! # Replace the path and Elastic IP with your own ssh -i \u0026#34;path/to/my-adguard-key.pem\u0026#34; admin@YOUR_ELASTIC_IP Install AdGuard Home: Run this one simple command. It downloads and installs everything for you. curl -s -S -L [https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh](https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh) | sh -s -- -v Run the Setup Wizard: The script will give you a link, like http://YOUR_ELASTIC_IP:3000. Open this in your browser. Follow the on-screen steps to create your admin username and password. ## Chapter 4: Teaching Your Guardian Who to Trust and What to Block With AdGuard Home installed, the next step is to configure its core brain: the DNS servers it gets its answers from and the blocklists it uses to protect your network.\n1. Setting Up Upstream DNS Servers Think of \u0026ldquo;Upstream DNS Servers\u0026rdquo; as the giant, public phonebooks of the internet. When your AdGuard server doesn\u0026rsquo;t know an address (and it\u0026rsquo;s not on a blocklist), it asks one of these upstreams. It\u0026rsquo;s recommended to use a mix of the best encrypted DNS providers for security, privacy, and speed.\nIn the AdGuard dashboard, go to Settings -\u0026gt; DNS settings. In the \u0026ldquo;Upstream DNS servers\u0026rdquo; box, enter the following, one per line:\nhttps://dns.quad9.net/dns-query https://dns.google/dns-query https://dns.cloudflare.com/dns-query Quad9: Focuses heavily on security, blocking malicious domains. Google: Known for being very fast. Cloudflare: A great all-around choice with a strong focus on privacy. 2. Optimizing DNS Performance Still in the DNS settings page, scroll down to optimize how your server queries the upstreams.\nParallel requests: Select this option. This is the fastest and most resilient mode. It sends your DNS query to all three of your upstream servers at the same time and uses the answer from the very first one that responds. This ensures you always get the quickest possible result.\nEnable EDNS client subnet (ECS): Check this box. This is very important for services like Netflix, YouTube, and other content delivery networks (CDNs). It helps them give you content from a server that is geographically closest to you, resulting in faster speeds and a better experience.\n3. Enabling DNSSEC Right below the upstream servers, there\u0026rsquo;s a checkbox for \u0026ldquo;Enable DNSSEC\u0026rdquo;. You should check this box. DNSSEC is like a digital wax seal on a letter; it verifies that the DNS answers you\u0026rsquo;re getting are authentic and haven\u0026rsquo;t been tampered with. It\u0026rsquo;s a simple, one-click security boost.\n4. Choosing Your Blocklists This is the fun part—the actual ad-blocking! Go to Filters -\u0026gt; DNS blocklists. For a \u0026ldquo;Balanced \u0026amp; Powerful\u0026rdquo; setup that blocks aggressively without a high risk of breaking websites, enable the following lists:\nAdGuard DNS filter: A great, well-maintained baseline. OISD Blocklist Big: Widely considered one of the best all-in-one lists for blocking ads, trackers, and malware. HaGeZi\u0026rsquo;s Pro Blocklist: A fantastic list that adds another layer of aggressive blocking for privacy. HaGeZi\u0026rsquo;s Threat Intelligence Feed: A crucial security-only list that focuses on protecting against active threats like phishing and malware. This combination will give you robust protection against both annoyances and real dangers.\n## Chapter 5: Giving Your Server a Name (Free Domain with No-IP) 📛 An IP address is hard to remember. Let\u0026rsquo;s get a free, memorable name for our server.\nSign Up at No-IP: Go to No-IP.com, create a free account, and create a hostname (e.g., my-dns.ddns.net). Point it to Your Server: When creating the hostname, enter your server\u0026rsquo;s permanent Elastic IP address. Confirm your account via email. ## Chapter 6: Making It Secure with SSL/TLS 🔐 We\u0026rsquo;ll use Let\u0026rsquo;s Encrypt and Certbot to get a free SSL certificate, which lets us use secure https:// and encrypted DNS.\nInstall Certbot: In your SSH session, run these commands:\nsudo apt update sudo apt install certbot -y Get the Certificate: Run this command, replacing the email and domain with your own.\n# This command will temporarily stop any service on port 80, get the certificate, and then finish. sudo certbot certonly --standalone --agree-tos --email YOUR_EMAIL@example.com -d your-no-ip-hostname.ddns.net If it\u0026rsquo;s successful, it will tell you where your certificate files are saved (usually in /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/).\nConfigure AdGuard Home Encryption:\nGo to your AdGuard Home dashboard (Settings -\u0026gt; Encryption settings). Check \u0026ldquo;Enable encryption\u0026rdquo;. In the \u0026ldquo;Server name\u0026rdquo; field, enter your No-IP hostname. Under \u0026ldquo;Certificates\u0026rdquo;, choose \u0026ldquo;Set a certificates file path\u0026rdquo;. Certificate path: /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/fullchain.pem Private key path: /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/privkey.pem Click \u0026ldquo;Save configuration\u0026rdquo;. The page will reload on a secure https:// connection! ## Chapter 7: Automating SSL Renewal (Cron Job Magic) ✨ Let\u0026rsquo;s Encrypt certificates last for 90 days. We can tell our server to automatically renew them.\nOpen the Cron Editor: In SSH, run sudo crontab -e and choose nano as your editor. Add the Renewal Job: Add this line to the bottom of the file. It tells the server to try renewing the certificate every day at 2:30 AM. 30 2 * * * systemctl stop AdGuardHome.service \u0026amp;\u0026amp; certbot renew --quiet \u0026amp;\u0026amp; systemctl start AdGuardHome.service Save and exit (Ctrl+X, then Y, then Enter). Your server will now keep its certificate fresh forever! ## Chapter 8: Testing Your New Superpowers (DoH \u0026amp; DoT) 🧪 For a direct confirmation, I used these commands on my computer:\nDNS-over-HTTPS (DoH) Test: This test checks if the secure web endpoint for DNS is alive.\ncurl -v [https://your-no-ip-hostname.ddns.net/dns-query](https://your-no-ip-hostname.ddns.net/dns-query) I got a \u0026ldquo;405 Method Not Allowed\u0026rdquo; error, which sounds bad but is actually great news. It means I successfully connected to the server, which correctly told me I didn\u0026rsquo;t send a real query. The connection works!\nDNS-over-TLS (DoT) Test: This checks the dedicated secure port for DNS. I used a tool called kdig.\n# I had to install it first with: sudo apt install knot-dnsutils kdig @your-no-ip-hostname.ddns.net +tls-ca +tls-host=your-no-ip-hostname.ddns.net example.com The command returned a perfect DNS answer for example.com, confirming the secure tunnel was working.\n## Chapter 9: Protecting Your Kingdom (Router \u0026amp; Phone Setup) 🏰 Now, let\u0026rsquo;s point your devices to their new guardian.\nOn Your Home Router: Log in to your router\u0026rsquo;s admin page, find the DNS settings, and enter your server\u0026rsquo;s Elastic IP as the primary DNS server. Leave the secondary field blank! This forces all devices on your Wi-Fi to be protected. Then, restart your router. On Your Mobile Phone: Android: Go to Settings -\u0026gt; Network -\u0026gt; Private DNS. Choose \u0026ldquo;Private DNS provider hostname\u0026rdquo; and enter your No-IP hostname (my-dns.ddns.net). This gives you ad-blocking everywhere, even on cellular data! iOS: You can use a profile to configure DoH. A simple way is to use a site like AdGuard\u0026rsquo;s DNS profile generator, but enter your own server\u0026rsquo;s DoH address (https://my-dns.ddns.net/dns-query). ## Chapter 10: The Ultimate Safety Net (Creating a Snapshot) 📸 Finally, let\u0026rsquo;s back up our perfect setup.\nIn the EC2 Console, go to your instance details. Click the \u0026ldquo;Storage\u0026rdquo; tab and click the \u0026ldquo;Volume ID\u0026rdquo;. Click \u0026ldquo;Actions\u0026rdquo; -\u0026gt; \u0026ldquo;Create snapshot\u0026rdquo;. Give it a description, like AdGuard-Working-Setup-Backup. If you ever mess something up, you can use this snapshot to restore your server to this exact working state in minutes.\n## Bonus Chapter: Common Troubleshooting Tips If things aren\u0026rsquo;t working, here are a few common pitfalls to check:\nBrowser Overrides Everything: If one device isn\u0026rsquo;t blocking ads, check its browser settings! Modern browsers like Chrome have a \u0026ldquo;Secure DNS\u0026rdquo; feature that can bypass your custom setup. You may need to turn this off. Check Your Laptop\u0026rsquo;s DNS: Make sure your computer\u0026rsquo;s network settings are set to \u0026ldquo;Obtain DNS automatically\u0026rdquo; so it listens to the router. A manually set DNS on your PC will ignore the router\u0026rsquo;s settings. Beware of IPv6: If you run into trouble on one device, try disabling IPv6 in that device\u0026rsquo;s Wi-Fi adapter properties to force it to use your working IPv4 setup. ## It’s a Wrap! And there you have it! You\u0026rsquo;ve successfully built a personal, secure, ad-blocking DNS server in the cloud. You\u0026rsquo;ve learned about cloud computing, firewalls, DNS, SSL, and automation. Go enjoy a faster, cleaner, and more private internet experience.\n","permalink":"http://localhost:1313/projects/adguard-updated/","summary":"\u003ch1 id=\"your-personal-internet-guardian-how-to-build-a-free-ad-blocker-in-the-cloud-\"\u003eYour Personal Internet Guardian: How to Build a FREE Ad-Blocker in the Cloud! 🚀\u003c/h1\u003e\n\u003cp\u003eHey everyone! A while back, I wrote a guide on setting up AdGuard Home on Linode. The world of tech moves fast, and it\u0026rsquo;s time for an upgrade! Today, we\u0026rsquo;re going to build our own powerful, network-wide ad-blocker using \u003cstrong\u003eAmazon Web Services (AWS)\u003c/strong\u003e, and we\u0026rsquo;ll make it secure with our own domain and SSL certificate.\u003c/p\u003e\n\u003cp\u003eThink of this as building a digital gatekeeper for your internet. Before any ads, trackers, or malicious sites can reach your devices, our AdGuard Home server will slam the door shut. The best part? This works on your phone, laptop, smart TV—anything on your network—without installing a single app on them.\u003c/p\u003e","title":"How I Built My Own Ad-Blocking DNS Server in the Cloud (2025 Updated Edition!)"},{"content":"What\u0026rsquo;s the buzz about AdGuard Home? Imagine AdGuard Home as your personal internet guardian. This versatile tool blocks ads, trackers, and other online nuisances across all devices connected to your network. Whether you\u0026rsquo;re browsing on your phone, tablet, or computer, AdGuard Home has your back.\nIn today\u0026rsquo;s digital landscape, robust security measures are paramount. Protecting each device shields your family from accidental clicks and malicious attacks, ensuring peace of mind and a secure online environment.\nWhy on the Cloud? While setting up AdGuard Home on your home network is great, installing it on a cloud server like Linode takes things up a notch. Here\u0026rsquo;s why:\nOn-the-Go Protection: Your devices stay protected from ads and trackers, no matter where you are, you can even share it with your family. Centralized Control: Manage and customize your ad-blocking settings from a single dashboard. Enhanced Privacy: Keep your browsing data away from prying eyes. Ready to embark on this ad-free adventure? Let\u0026rsquo;s get started!\nSetting Up The Environment Step 1: Create a Linode Cloud Account Why choose Linode? Through NetworkChuck\u0026rsquo;s referral link, you receive a generous $100 cloud credit - a fantastic start!\nSign Up: Navigate to Linode\u0026rsquo;s signup page and register. Access the Dashboard: Log in and select \u0026lsquo;Linodes\u0026rsquo; from the left-side menu. Create a Linode: Click \u0026lsquo;Create Linode,\u0026rsquo; choose your preferred region, and select an operating system (Debian 11 is a solid choice). Choose a Plan: The Shared 1GB Nanode instance is sufficient for AdGuard Home. Label and Secure: Assign a label to your Linode and set a strong root password. Deploy: Click \u0026lsquo;Create Linode\u0026rsquo; and wait for it to initialize. Once your Linode is up and running, access it via the LISH Console or SSH. (use root as localhost login)\nStep 2: Installing AdGuard Home on Linode Yes, we\u0026rsquo;re already into setting up at this point.\nLog In: Access your Linode using SSH or the LISH Console with your root credentials. Update the system: sudo apt update \u0026amp;\u0026amp; apt upgrade -y Go ahead and copy this command to Install Adguard Home: curl -s -S -L https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh | sh -s -- -v AdGuard Home is installed and running. You can use CTRL+Shift+V to paste into the terminal.\nStep 3: Configure AdGuard Home Post-installation, you\u0026rsquo;ll see a list of IP addresses with port :3000. Access the Web Interface: Open your browser and navigate to the IP address followed by :3000. If you encounter a security warning, proceed by clicking \u0026ldquo;Continue to site.\u0026rdquo; Initial Setup: Click \u0026lsquo;Get Started\u0026rsquo; and follow the prompts. When uncertain, default settings are typically fine. Set Credentials: Set up the Username and Password. Step 4: Integrate AdGuard Home with Your Router After this, your AdGuard Home is running, but in order to use it on your devices you need to set up inside your home router for all your devices to be protected. For that, I can\u0026rsquo;t walk you through each and every router\u0026rsquo;s settings, but the steps are pretty similar.\nFind your router IP address, you should be able to find it on the back of your router (commonly 192.168.0.1 or 192.169.1.1) enter it into your browser. Login into your router using the credentials mentioned in the back of your router; the default is often admin for both username and password. I suggest you change your default password. Configure DNS Settings: Enable DHCP Server: Ensure your router\u0026rsquo;s DHCP server is active. Set DNS Addresses: Input your AdGuard Home server\u0026rsquo;s IP as the primary DNS (mine was 96.126.113.207). For secondary DNS, options like 1.1.1.1 (Cloudflare), 9.9.9.9 (Quad9), or 8.8.8.8 (Google) are reliable. Save and apply the changes. Fine-Tuning AdGuard Home If you\u0026rsquo;ve done everything till here you should be good, but for those who enjoy customizations, AdGuard Home offers a plethora of settings. Some of the customization I did are:\nSettings Go to Settings -\u0026gt; General Settings: You can enable Parental Control and Safe Search. You can also make your Statistics last longer than 24hrs which is default. Now on Settings -\u0026gt; DNS Settings By default, it uses DNS from quad9 which is pretty good but I suggest you add more. You can click on the list of known DNS providers, which you can choose from. I used: https://dns.quad9.net/dns-query https://dns.google/dns-query https://dns.cloudflare.com/dns-query Enable \u0026lsquo;Load Balancing\u0026rsquo; to distribute queries evenly. Scroll down to \u0026lsquo;DNS server configuration\u0026rsquo; and enable DNSSEC for enhanced security. Click on Save. Filters DNS blocklists Go to Filters -\u0026gt; DNS blocklists, here you can add a blocklist that people have created and use it to block even more things. By default, AdGuard uses the AdGuard DNS filter, and you can add more.\nClick on Add blocklist -\u0026gt; Choose from the list Don\u0026rsquo;t choose too many from the list cause it may slow your internet requests. These are the blocklists I added. And just like that you are blocking more and more things. DNS rewrites Go to Filters -\u0026gt; DNS rewrites, here you can add your own DNS entries, so I added AdGuard here.\nClick on Add DNS rewrite Type in domain adguardforme.local and your IP address for AdGuard Home. And save it. Now, when I want to go on the AdGuard Home dashboard I just type in adguardforme.local and I\u0026rsquo;m into AdGuard, I don\u0026rsquo;t have to remember the IP address.\nCustom filtering rules Go to Filters -\u0026gt; Custom filtering rules. For some reason when I use Facebook on mobile device stories and videos did not load up, so I added custom filtering rules.\n@@||graph.facebook.com^$important ","permalink":"http://localhost:1313/projects/adguard-home-on-cloud/","summary":"\u003ch1 id=\"whats-the-buzz-about-adguard-home\"\u003eWhat\u0026rsquo;s the buzz about AdGuard Home?\u003c/h1\u003e\n\u003cp\u003eImagine AdGuard Home as your personal internet guardian. This versatile tool blocks ads, trackers, and other online nuisances across all devices connected to your network. Whether you\u0026rsquo;re browsing on your phone, tablet, or computer, AdGuard Home has your back.\u003c/p\u003e\n\u003cp\u003eIn today\u0026rsquo;s digital landscape, robust security measures are paramount. Protecting each device shields your family from accidental clicks and malicious attacks, ensuring peace of mind and a secure online environment.\u003c/p\u003e","title":"Running Private Adguard Server on Cloud (Linode)"},{"content":"What\u0026rsquo;s a Docker Container? Before we dive into setting up DeepSeek-R1, let me explain what a Docker container is. Imagine you have a toy that works perfectly on your birthday but gets broken if you move it to another room. A Docker container is like a magic box that keeps your AI model (the toy) in perfect condition wherever you take it, whether it\u0026rsquo;s running as a background task, on a web server, or even in the cloud.\nDocker containers encapsulate everything required to run an application: the code, dependencies, and environment settings. This ensures consistency across different machines, which is super important for AI models that rely on precise configurations.\nSetting Up The Environment Step 1: Install Ubuntu on Windows (If You Haven\u0026rsquo;t Already) If you\u0026rsquo;re using Windows, the easiest way to get an Ubuntu environment is through the Microsoft Store. Here\u0026rsquo;s how:\nOpen the Microsoft Store and search for Ubuntu. Click Get and let it install. Once installed, open Ubuntu from the Start menu and follow the setup instructions. Update the system: sudo apt update \u0026amp;\u0026amp; sudo apt upgrade Now, you have an Ubuntu terminal running on Windows!\nStep 2: Install Docker (If You Haven\u0026rsquo;t Already) First, let\u0026rsquo;s check if you have Docker installed. Open a terminal and run:\ndocker --version If that returns a version number, congrats! If not, install Docker:\nsudo apt update \u0026amp;\u0026amp; sudo apt install docker.io -y sudo systemctl enable --now docker Step 3: Prerequisites for NVIDIA GPU Install NVIDIA Container Toolkit:\nConfiguring the production repository: curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list Update the package list: sudo apt-get update Install the NVIDIA Container Toolkit: sudo apt-get install -y nvidia-container-toolkit Running Ollama Inside Docker Run these commands(P.S. shoutout to NetworkChuck):\ndocker run -d \\ --gpus all \\ -v ollama:/root/.ollama \\ -p 11434:11434 \\ --security-opt=no-new-privileges \\ --cap-drop=ALL \\ --cap-add=SYS_NICE \\ --memory=8g \\ --memory-swap=8g \\ --cpus=4 \\ --read-only \\ --name ollama \\ ollama/ollama Running DeepSeek-R1 Locally Time to bring DeepSeek-R1 to life locally and containerized:\ndocker exec -it ollama ollama run deepseek-r1 or you can run other versions of deepseek-r1 just by typing in the version at the end after a colon(:)\ndocker exec -it ollama ollama run deepseek-r1:7b After this, play around with the AI, if you wanna exit just type:\n/bye Starting Deepseek-R1 To Start Deepseek-R1 from next time go to Ubuntu and type:\ndocker start ollama this will start ollama docker container; then type:\ndocker exec -it ollama ollama run deepseek-r1:7b ","permalink":"http://localhost:1313/projects/running-deepseek-r1-on-docker-container-on-ubuntu/","summary":"\u003ch1 id=\"whats-a-docker-container\"\u003eWhat\u0026rsquo;s a Docker Container?\u003c/h1\u003e\n\u003cp\u003eBefore we dive into setting up DeepSeek-R1, let me explain what a Docker container is. Imagine you have a toy that works perfectly on your birthday but gets broken if you move it to another room. A Docker container is like a magic box that keeps your AI model (the toy) in perfect condition wherever you take it, whether it\u0026rsquo;s running as a background task, on a web server, or even in the cloud.\u003c/p\u003e","title":"Dive into AI Fun: Running DeepSeek-R1 on a Docker Container on Ubuntu"},{"content":"Description I joined AbbVie initially as a contractor and quickly demonstrated the skills and dedication that led to my conversion to a full-time position. In my role, I stepped into a high-stakes production environment where precision and operational stability are paramount. My work centered on the optimization and maintenance of sophisticated, machine learning-based visual inspection systems. I was responsible for fine-tuning these models, analyzing their performance data, and troubleshooting complex technical issues across both hardware and software, including the POM and PCE systems.\nThis wasn\u0026rsquo;t just about keeping machines running; it was about enhancing them. By applying a systematic, data-driven approach, I contributed to a 30% reduction in product waste, a metric that translates directly to improved efficiency and sustainability. Working within strict GMPs and utilizing systems like SAP for material tracking, I learned to balance technical problem-solving with rigorous compliance, ensuring that every action contributed to the stability and reliability of mission-critical operations.\n","permalink":"http://localhost:1313/experience/abbvie/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eI joined AbbVie initially as a contractor and quickly demonstrated the skills and dedication that led to my conversion to a full-time position. In my role, I stepped into a high-stakes production environment where precision and operational stability are paramount. My work centered on the optimization and maintenance of sophisticated, machine learning-based visual inspection systems. I was responsible for fine-tuning these models, analyzing their performance data, and troubleshooting complex technical issues across both hardware and software, including the POM and PCE systems.\u003c/p\u003e","title":"Operator III"},{"content":"Description As my first professional role after moving to the United States, my position at FedEx was a crucial step in adapting my technical skills to a new corporate environment. My journey began as a contractor, where my performance and analytical skills in a fast-paced setting led to my transition to a full-time Associate role. At the device testing center, I was on the front lines of quality assurance for a wide array of consumer electronics. I conducted comprehensive, systematic testing on mobile devices, smartwatches, and routers, executing detailed test plans to identify hardware vulnerabilities, software bugs, and non-compliance with network standards.\nMy responsibilities included meticulously documenting my findings, reproducing bugs to assist developers, and providing clear, actionable reports to engineering teams. This collaborative process was crucial in accelerating the repair cycle and ensuring that products met the highest standards of quality and security before reaching the market. The role sharpened my analytical skills and gave me a deep appreciation for the importance of rigorous testing in the software development lifecycle.\n","permalink":"http://localhost:1313/experience/fedex/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs my first professional role after moving to the United States, my position at FedEx was a crucial step in adapting my technical skills to a new corporate environment. My journey began as a contractor, where my performance and analytical skills in a fast-paced setting led to my transition to a full-time Associate role. At the device testing center, I was on the front lines of quality assurance for a wide array of consumer electronics. I conducted comprehensive, systematic testing on mobile devices, smartwatches, and routers, executing detailed test plans to identify hardware vulnerabilities, software bugs, and non-compliance with network standards.\u003c/p\u003e","title":"Product Testing Associate"},{"content":"Description As the IT Support Specialist for a bustling international college with over 2,500 students and staff, I was at the heart of the campus\u0026rsquo;s technical operations. My role was dynamic and comprehensive, involving end-to-end technical support across a diverse, multi-building campus. I managed the entire user lifecycle, from onboarding new accounts to ensuring smooth system setups across Windows, Linux, and Mac environments. I was the primary point of contact for all technical challenges, resolving Tier 1 and 2 support tickets with a 90% SLA adherence and troubleshooting complex OS issues to minimize downtime.\nMy tenure was marked by significant growth and adaptation. I led the complete technical setup of eight new computer labs, managing everything from hardware deployment and network cabling to software installation and configuration. When the COVID-19 pandemic hit, I was instrumental in transitioning the campus to a hybrid learning model, my first professional experience navigating such a large-scale shift. This required rapidly scaling our remote support capabilities and ensuring both students and faculty could operate effectively from anywhere.\nA cornerstone project of my time was the complete technical overhaul of the newly acquired Kumari Film Hall. I was deeply involved in the project to transform the old cinema into modern lecture halls, which included designing and deploying the entire network infrastructure, setting up AV systems, and ensuring seamless integration with the main campus network.\nTo support these expanding operations, I took the lead in deploying a new UVDesk help desk ticketing system on a CentOS server and embraced automation, utilizing tools like OK Goldy to streamline user creation in Google Workspace. These initiatives standardized processes, improved efficiency, and allowed our team to successfully manage the college\u0026rsquo;s ambitious growth.\n","permalink":"http://localhost:1313/experience/islingtoncollege/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs the IT Support Specialist for a bustling international college with over 2,500 students and staff, I was at the heart of the campus\u0026rsquo;s technical operations. My role was dynamic and comprehensive, involving end-to-end technical support across a diverse, multi-building campus. I managed the entire user lifecycle, from onboarding new accounts to ensuring smooth system setups across Windows, Linux, and Mac environments. I was the primary point of contact for all technical challenges, resolving Tier 1 and 2 support tickets with a 90% SLA adherence and troubleshooting complex OS issues to minimize downtime.\u003c/p\u003e","title":"IT Support Specialist"},{"content":"Description Building on my foundational experience, my internship at BlackBox Technologies immersed me in a more complex, project-based environment. I was an integral part of a development team tasked with building a web-based attendance system from the ground up. This role provided me with invaluable hands-on, full-stack experience. I contributed to the backend by assisting senior engineers with the development of business logic in .NET, giving me insight into server-side architecture. Simultaneously, I was responsible for building responsive, user-facing components for the front-end using HTML, CSS, and JavaScript.\nThis experience was a deep dive into the software development lifecycle. I learned how to translate business requirements into technical specifications, participated in code reviews, and understood the synergy between front-end and back-end systems. Working in close collaboration with the engineering team on a single, focused product was an excellent opportunity to apply my skills to a real-world project and solidify my understanding of creating robust, scalable web applications.\n","permalink":"http://localhost:1313/experience/blackbox/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eBuilding on my foundational experience, my internship at BlackBox Technologies immersed me in a more complex, project-based environment. I was an integral part of a development team tasked with building a web-based attendance system from the ground up. This role provided me with invaluable hands-on, full-stack experience. I contributed to the backend by assisting senior engineers with the development of business logic in .NET, giving me insight into server-side architecture. Simultaneously, I was responsible for building responsive, user-facing components for the front-end using HTML, CSS, and JavaScript.\u003c/p\u003e","title":"Web Development Intern"},{"content":"Description My journey into professional software development began at Radiant Infotech, my first internship and job in the tech industry. This role was a pivotal transition from academic theory to real-world application. I was entrusted with supporting the full lifecycle of client websites, which provided an immersive learning experience. My primary responsibility was to develop responsive, pixel-perfect front-end interfaces using HTML, CSS, and Bootstrap, translating design files into functional web components. A key part of this process was using Adobe Photoshop to prepare and optimize web graphics, ensuring both aesthetic quality and optimal performance.\nBeyond the initial development, my role extended to managing website content through various CMS platforms and performing rigorous debugging to ensure cross-browser compatibility and a seamless user experience. This foundational internship was crucial in building my confidence and skills in modern web development, teaching me how to collaborate effectively within a team to deliver high-quality digital products for clients.\n","permalink":"http://localhost:1313/experience/radiantinfotech/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eMy journey into professional software development began at Radiant Infotech, my first internship and job in the tech industry. This role was a pivotal transition from academic theory to real-world application. I was entrusted with supporting the full lifecycle of client websites, which provided an immersive learning experience. My primary responsibility was to develop responsive, pixel-perfect front-end interfaces using HTML, CSS, and Bootstrap, translating design files into functional web components. A key part of this process was using Adobe Photoshop to prepare and optimize web graphics, ensuring both aesthetic quality and optimal performance.\u003c/p\u003e","title":"Web Development Intern"},{"content":"Introduction Welcome to my personal portfolio website. I respect your privacy and am committed to protecting it. This policy outlines what information is collected when you visit my site and how that information is used.\nInformation Collection and Use I collect information in two ways: information you provide directly and anonymous data collected by analytics services.\nPersonal Data You Provide When you request a copy of my resume, you are asked to voluntarily provide your email address.\nHow it\u0026rsquo;s collected: This information is collected via an embedded Google Form. Why it\u0026rsquo;s collected: It is used for the sole purpose of sending the requested resume document to you through an automated process managed by Google Apps Script. How it\u0026rsquo;s used: Your email will not be used for marketing purposes, sold, or shared with any third parties. Anonymous Usage Data To improve the user experience and analyze traffic, this website uses the following third-party services:\nUmami (Self-Hosted): This website uses a self-hosted instance of Umami for privacy-focused web analytics. Umami collects anonymous usage data such as page views, referrers, and geographic regions to help me understand website traffic. This service does not use cookies, does not collect any personally identifiable information, and all data is stored on a private server under my control.\nCloudflare Web Analytics: This service collects anonymous traffic data such as page views and country of origin. It does not use cookies or collect personally identifiable information. You can view their privacy policy here.\nService Providers This website relies on the following third-party service providers to function:\nGoogle Workspace (Forms, Sheets, Apps Script): Used to manage and automate resume requests. Cloudflare: Used as a Content Delivery Network (CDN) to improve website performance, as a security firewall to protect against malicious attacks, and for collecting anonymous web analytics. Vercel: Used to host the self-hosted Umami analytics application. Netlify \u0026amp; GitHub: Used for hosting and deploying the website. Changes to This Privacy Policy I may update this Privacy Policy from time to time. I will notify you of any changes by posting the new Privacy Policy on this page. You are advised to review this page periodically for any changes.\nContact Me If you have any questions about this Privacy Policy, please contact me at: prajwolad18@gmail.com\n","permalink":"http://localhost:1313/privacy-policy/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWelcome to my personal portfolio website. I respect your privacy and am committed to protecting it. This policy outlines what information is collected when you visit my site and how that information is used.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"information-collection-and-use\"\u003eInformation Collection and Use\u003c/h2\u003e\n\u003cp\u003eI collect information in two ways: information you provide directly and anonymous data collected by analytics services.\u003c/p\u003e\n\u003ch3 id=\"personal-data-you-provide\"\u003ePersonal Data You Provide\u003c/h3\u003e\n\u003cp\u003eWhen you request a copy of my resume, you are asked to voluntarily provide your email address.\u003c/p\u003e","title":"Privacy Policy"},{"content":"Introduction Welcome to the part 4 of the homelab series! In the previous parts, we built a server, deployed a suite of services, and configured our network. Now, it\u0026rsquo;s time to make it resilient and self-maintaining. A homelab isn\u0026rsquo;t just about setting things up; it\u0026rsquo;s about keeping them running reliably.\nThis guide will show you how to set up the three pillars of modern IT operations: Automated Backups, Automated Updates, and Proactive Alerting. By the end, you\u0026rsquo;ll have a homelab that runs itself, ensures your data is safe, stays up-to-date, and notifies you when something goes wrong.\nChapter 1: The Automated Backup Strategy (at 3 AM) A solid backup strategy is non-negotiable. I implemented a robust system inspired by the \u0026ldquo;3-2-1\u0026rdquo; rule, focusing on redundancy and an off-site copy. My strategy involves maintaining two copies of my data in two separate locations: one local backup on the server itself for fast recovery, and one automated, off-site backup to Google Drive to protect against a local disaster like a fire or hardware failure.\nThis script runs at 3 AM, creates a local backup, uploads it, and then notifies Discord.\nStep 1: Configure rclone for Google Drive First, you need a tool to communicate with Google Drive. We\u0026rsquo;ll use rclone.\nInstall rclone on your Debian server: sudo -v ; curl [https://rclone.org/install.sh](https://rclone.org/install.sh) | sudo bash Run the interactive setup: rclone config Follow the Prompts: n (New remote) * name\u0026gt;: gdrive (You can name it anything) storage\u0026gt;: Find and select drive (Google Drive). client_id\u0026gt; \u0026amp; client_secret\u0026gt;: Press Enter for both to leave blank. scope\u0026gt;: Choose 1 (Full access). Use auto config? y/n\u0026gt;: This is a critical step. Since we are on a headless server, type n and press Enter. Authorize Headless: rclone will give you a command to run on a machine with a web browser (like your main computer). On your main computer (where you have rclone installed), run the rclone authorize \u0026quot;drive\u0026quot; \u0026quot;...\u0026quot; command. This will open your browser, ask you to log in to Google, and grant permission. Your main computer\u0026rsquo;s terminal will then output a block of text (your config_token). Paste Token: Copy the token from your main computer and paste it back into your server\u0026rsquo;s rclone prompt. Finish the prompts, and your connection is complete. Step 2: Create the Backup Script Next, create a shell script to perform the backup.\nCreate the file and make it executable:\nnano ~/backup.sh chmod +x ~/backup.sh Paste in the following script. You must edit the first 7 variables to match your setup.\n#!/bin/bash # --- Configuration --- SOURCE_DIR=\u0026#34;/path/to/your/docker\u0026#34; # \u0026lt;-- Change to your Docker projects directory BACKUP_DIR=\u0026#34;/path/to/your/backups\u0026#34; # \u0026lt;-- Change to your backups folder FILENAME=\u0026#34;homelab-backup-$(date +%Y-%m-%d).tar.gz\u0026#34; LOCAL_RETENTION_DAYS=3 CLOUD_RETENTION_DAYS=3 RCLONE_REMOTE=\u0026#34;gdrive\u0026#34; # \u0026lt;-- Must match your rclone remote name RCLONE_DEST=\u0026#34;Homelab Backups\u0026#34; # \u0026lt;-- Folder name in Google Drive # --- \u0026#34;https://discordapp.com/api/webhooks/141949178941/6Tx6f1yjf26LztQ\u0026#34; --- DISCORD_WEBHOOK_URL=\u0026#34;YOUR_DISCORD_WEBHOOK_URL\u0026#34; # --- Notification Function --- send_notification() { MESSAGE=$1 curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#34;{\\\u0026#34;content\\\u0026#34;: \\\u0026#34;$MESSAGE\\\u0026#34;}\u0026#34; \u0026#34;$DISCORD_WEBHOOK_URL\u0026#34; } # --- Script Logic --- echo \u0026#34;--- Starting Homelab Backup: $(date) ---\u0026#34; send_notification \u0026#34;✅ Starting Homelab Backup...\u0026#34; # 1. Create local backup echo \u0026#34;Creating local backup...\u0026#34; tar -czf \u0026#34;${BACKUP_DIR}/${FILENAME}\u0026#34; -C \u0026#34;${SOURCE_DIR}\u0026#34; . echo \u0026#34;Local backup created at ${BACKUP_DIR}/${FILENAME}\u0026#34; # 2. Upload to Google Drive echo \u0026#34;Uploading backup to ${RCLONE_REMOTE}...\u0026#34; rclone copy \u0026#34;${BACKUP_DIR}/${FILENAME}\u0026#34; \u0026#34;${RCLONE_REMOTE}:${RCLONE_DEST}\u0026#34; echo \u0026#34;Upload complete.\u0026#34; # 3. Clean up local backups echo \u0026#34;Cleaning up local backups older than ${LOCAL_RETENTION_DAYS} days...\u0026#34; find \u0026#34;${BACKUP_DIR}\u0026#34; -type f -name \u0026#34;*.tar.gz\u0026#34; -mtime +${LOCAL_RETENTION_DAYS} -delete echo \u0026#34;Local cleanup complete.\u0026#34; # 4. Clean up cloud backups echo \u0026#34;Cleaning up cloud backups older than ${CLOUD_RETENTION_DAYS} days...\u0026#34; rclone delete \u0026#34;${RCLONE_REMOTE}:${RCLONE_DEST}\u0026#34; --min-age ${CLOUD_RETENTION_DAYS}d echo \u0026#34;Cloud cleanup complete.\u0026#34; echo \u0026#34;Backup process finished.\u0026#34; send_notification \u0026#34;🎉 Homelab backup and cloud upload completed successfully!\u0026#34; Step 3: Automate with Cron To run this script automatically, you must add it to the root user\u0026rsquo;s crontab. This is critical for giving the script permission to read all Docker files.\nOpen the root crontab editor: sudo crontab -e Add the following line to schedule the backup for 3:00 AM every morning: 0 3 * * * /path/to/your/backup.sh You will now get a fresh, onsite and off-site backup every night and a Discord message when it\u0026rsquo;s done. Chapter 2: Automated Updates with Watchtower (at 6 AM) Manually updating every Docker container is tedious. We can automate this by deploying Watchtower.\nStep 1: The Docker Compose File Create a docker-compose.yml for Watchtower. This configuration schedules it to run once a day at 6:00 AM, clean up old images, and send a Discord notification only if it finds an update.\nmkdir -p ~/docker/watchtower\ncd ~/docker/watchtower\nnano docker-compose.yml\nPaste in this configuration:\nservices: watchtower: image: containrrr/watchtower container_name: watchtower restart: unless-stopped volumes: - /var/run/docker.sock:/var/run/docker.sock environment: # Timezone setting TZ: America/Chicago # Discord notification settings WATCHTOWER_NOTIFICATIONS: shoutrrr WATCHTOWER_NOTIFICATION_URL: \u0026#34;discord://YOUR_DISCORD_WEBHOOK_ID_URL\u0026gt; # Notification settings WATCHTOWER_NOTIFICATIONS_LEVEL: info WATCHTOWER_NOTIFICATION_REPORT: \u0026#34;true\u0026#34; WATCHTOWER_NOTIFICATIONS_HOSTNAME: Homelab-Laptop # Update settings WATCHTOWER_CLEANUP: \u0026#34;true\u0026#34; WATCHTOWER_INCLUDE_STOPPED: \u0026#34;false\u0026#34; WATCHTOWER_INCLUDE_RESTARTING: \u0026#34;true\u0026#34; WATCHTOWER_SCHEDULE: \u0026#34;0 0 6 * * *\u0026#34; Note: The WATCHTOWER_NOTIFICATION_URL uses a special shoutrrr format for Discord, which looks like discord://token@webhook-id.\nNow, every morning at 6:00 AM, Watchtower will scan all running containers and update any that have a new image available.\nChapter 3: Proactive Alerting (24/7) The final piece of automation is proactive alerting. This setup ensures you are immediately notified via Discord if something goes wrong.\nStep 1: The Alerting Pipeline The pipeline we\u0026rsquo;ll build is: Prometheus (detects problems) -\u0026gt; Alertmanager (groups and routes alerts) -\u0026gt; Discord (notifies you).\nStep 2: Deploy Alertmanager First, deploy Alertmanager. It must be on the same npm_default network as Prometheus.\nmkdir -p ~/docker/alertmanager\ncd ~/docker/alertmanager\nCreate the alertmanager.yml configuration file:\nnano alertmanager.yml Paste in this configuration. It uses advanced routing to send critical alerts every 2 hours and warning alerts every 12 hours.\nglobal: resolve_timeout: 5m route: group_by: [\u0026#34;alertname\u0026#34;, \u0026#34;severity\u0026#34;] group_wait: 30s group_interval: 10m repeat_interbal: 12h receiver: \u0026#34;discord-notifications\u0026#34; routes: - receiver: \u0026#34;discord-notifications\u0026#34; matchers: - severity=\u0026#34;critical\u0026#34; repeat_interval: 2h - receiver: \u0026#34;discord-notifications\u0026#34; matchers: - severity=\u0026#34;warning\u0026#34; repeat_interval: 12h receivers: - name: \u0026#34;discord-notifications\u0026#34; discord_configs: - webhook_url: \u0026#34;YOUR_DISCORD_WEBHOOK_URL\u0026#34; send_resolved: true Now create the docker-compose.yml for Alertmanager:\nnano docker-compose.yml Paste in the following:\nservices: alertmanager: image: prom/alertmanager:latest container_name: alertmanager restart: unless-stopped volumes: - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml networks: - npm_default networks: npm_default: external: true Launch it: docker compose up -d\nStep 3: Configure Prometheus Finally, tell Prometheus to send alerts to Alertmanager and load your rules.\nCreate your rules file, ~/docker/monitoring/alert_rules.yml, with rules for \u0026ldquo;Instance Down,\u0026rdquo; \u0026ldquo;High CPU,\u0026rdquo; \u0026ldquo;Low Disk Space,\u0026rdquo; etc.\ncd ~/docker/monitoring nano alert_rules.yml Add the alert_rules.yml as a volume in your ~/docker/monitoring/docker-compose.yml.\nvolumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - ./alert_rules.yml:/etc/prometheus/alert_rules.yml - prometheus_data:/prometheus Add the alerting and rule_files blocks to your ~/docker/monitoring/prometheus.yml:\ngroups: -name: Critical System Alerts interval: 30s rules: - alert: InstanceDown expr: up == 0 for: 2m labels: severity: critical annotations: summary: \u0026#34;🔴 Instance {{ $labels.instance }} is DOWN\u0026#34; description: \u0026#34;Service {{ $labels.job }} has been unreachable for 2 minutes.\u0026#34; - alert: LaptopOnBattery expr: node_power_supply_online == 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;🔋 Server running on BATTERY\u0026#34; description: \u0026#34;Homelab has been unplugged for 5 minutes. Check power connection!\u0026#34; - alert: LowBatteryLevel expr: node_power_supply_capacity \u0026lt; 20 and node_power_supply_online == 0 for: 1m labels: severity: critical annotations: summary: \u0026#34;⚠️ CRITICAL: Battery at {{ $value }}%\u0026#34; description: \u0026#34;Battery below 20%. Server may shut down soon!\u0026#34; - alert: DiskAlmostFull expr: (node_filesystem_avail_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;} / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;}) * 100 \u0026lt; 10 for: 5m labels: severity: critical annotations: summary: \u0026#34;💾 Disk space critically low: {{ $value | humanize }}% remaining\u0026#34; description: \u0026#34;Root filesystem has less than 10% free space.\u0026#34; - alert: OutOfMemory expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 \u0026lt; 5 for: 2m labels: severity: critical annotations: summary: \u0026#34;🧠 Memory critically low: {{ $value | humanize }}% available\u0026#34; description: \u0026#34;Less than 5% memory available. System may become unresponsive.\u0026#34; - alert: CriticalCpuTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;coretemp\u0026#34;} \u0026gt; 95 for: 2m labels: severity: critical annotations: summary: \u0026#34;🔥 CRITICAL CPU Temperature: {{ $value }}°C\u0026#34; description: \u0026#34;CPU temperature exceeds 95°C. Thermal throttling or shutdown imminent!\u0026#34; - name: Warning System Alerts interval: 1m rules: - alert: HighCpuUsage expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[5m])) * 100) \u0026gt; 80 for: 5m labels: severity: warning annotations: summary: \u0026#34;⚡ High CPU usage: {{ $value | humanize }}%\u0026#34; description: \u0026#34;CPU usage above 80% for 5 minutes on {{ $labels.instance }}\u0026#34; - alert: HighSystemLoad expr: node_load5 / on(instance) count(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}) by (instance) \u0026gt; 1.5 for: 10m labels: severity: warning annotations: summary: \u0026#34;📊 High system load: {{ $value | humanize }}\u0026#34; description: \u0026#34;5-minute load average is 1.5x CPU cores for 10 minutes.\u0026#34; - alert: HighMemoryUsage expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 \u0026lt; 20 for: 5m labels: severity: warning annotations: summary: \u0026#34;🧠 High memory usage: {{ $value | humanize }}% available\u0026#34; description: \u0026#34;Less than 20% memory available.\u0026#34; - alert: HighCpuTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;coretemp\u0026#34;} \u0026gt; 85 for: 5m labels: severity: warning annotations: summary: \u0026#34;🌡️ High CPU temperature: {{ $value }}°C\u0026#34; description: \u0026#34;CPU temperature above 85°C. Consider improving cooling.\u0026#34; - alert: HighNvmeTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;nvme\u0026#34;} \u0026gt; 65 for: 10m labels: severity: warning annotations: summary: \u0026#34;💿 High NVMe temperature: {{ $value }}°C\u0026#34; description: \u0026#34;NVMe drive temperature above 65°C for 10 minutes.\u0026#34; - alert: DiskSpaceLow expr: (node_filesystem_avail_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;} / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;}) * 100 \u0026lt; 20 for: 10m labels: severity: warning annotations: summary: \u0026#34;💾 Disk space low: {{ $value | humanize }}% remaining\u0026#34; description: \u0026#34;Root filesystem has less than 20% free space.\u0026#34; - alert: HighSwapUsage expr: ((node_memory_SwapTotal_bytes - node_memory_SwapFree_bytes) / node_memory_SwapTotal_bytes * 100) \u0026gt; 50 for: 10m labels: severity: warning annotations: summary: \u0026#34;💱 High swap usage: {{ $value | humanize }}%\u0026#34; description: \u0026#34;Swap usage above 50%. System may be memory-constrained.\u0026#34; # Monitor your USB-C hub ethernet adapter (enx00) - alert: EthernetInterfaceDown expr: node_network_up{device=\u0026#34;enx00\u0026#34;} == 0 for: 2m labels: severity: warning annotations: summary: \u0026#34;🌐 USB-C Ethernet adapter is DISCONNECTED\u0026#34; description: \u0026#34;Your USB-C hub ethernet connection (enx00) is down. Check cable or hub.\u0026#34; - alert: HighNetworkErrors expr: rate(node_network_receive_errs_total{device=\u0026#34;enx00\u0026#34;}[5m]) \u0026gt; 10 or rate(node_network_transmit_errs_total{device=\u0026#34;enx00\u0026#34;}[5m]) \u0026gt; 10 for: 5m labels: severity: warning annotations: summary: \u0026#34;🌐 High network errors on USB-C ethernet\u0026#34; description: \u0026#34;Your ethernet adapter is experiencing high error rate. Check cable quality.\u0026#34; - name: Docker Container Alerts interval: 1m rules: # Simplified alert - just checks if container exporter is working - alert: ContainerMonitoringDown expr: absent(container_last_seen) for: 2m labels: severity: warning annotations: summary: \u0026#34;🐳 Container monitoring is down\u0026#34; description: \u0026#34;cAdvisor or container metrics are not available. Check if containers are being monitored.\u0026#34; - alert: ContainerRestarting expr: rate(container_start_time_seconds[5m]) \u0026gt; 0.01 for: 2m labels: severity: warning annotations: summary: \u0026#34;🐳 Container {{ $labels.name }} is restarting\u0026#34; description: \u0026#34;Container {{ $labels.name }} has restarted recently.\u0026#34; - alert: ContainerHighCpu expr: rate(container_cpu_usage_seconds_total{name!~\u0026#34;.*POD.*\u0026#34;,name!=\u0026#34;\u0026#34;}[5m]) * 100 \u0026gt; 80 for: 10m labels: severity: warning annotations: summary: \u0026#34;🐳 Container {{ $labels.name }} high CPU: {{ $value | humanize }}%\u0026#34; description: \u0026#34;Container CPU usage above 80% for 10 minutes.\u0026#34; Restart Prometheus to apply the changes:\ncd ~/docker/monitoring docker compose up -d --force-recreate prometheus Now, if any service fails or your server\u0026rsquo;s resources run low, you will get an instant notification in Discord.\nStep 3: The Critical Firewall Fix You may find your alerts are not sending. This is often due to a conflict between Docker and ufw.\nOpen the main ufw configuration file:\nsudo nano /etc/default/ufw Change DEFAULT_FORWARD_POLICY=\u0026quot;DROP\u0026quot; to DEFAULT_FORWARD_POLICY=\u0026quot;ACCEPT\u0026quot;.\nReload the firewall:\nsudo ufw reload Restart your containers that need internet access:\ndocker compose restart Now, if any service fails or your server\u0026rsquo;s resources run low, you will get an instant notification in Discord.\nConclusion Our homelab has now truly come to life. It\u0026rsquo;s no longer just a collection of services but a resilient, self-maintaining platform. With automated backups to Google Drive, daily updates via Watchtower, and proactive alerts with Prometheus and Alertmanager, our server can now run 24/7 with minimal manual intervention. We\u0026rsquo;ve built a solid, reliable, and intelligent system.\nBut there\u0026rsquo;s one critical piece still missing: end-to-end security for our local services.\nRight now, we\u0026rsquo;re accessing our dashboards at addresses like http://grafana.local, which browsers flag as \u0026ldquo;Not Secure.\u0026rdquo; What if we could use a real, public domain name for our internal services and get a valid HTTPS certificate, all without opening a single port on our router?\nIn the next part of this series, I\u0026rsquo;ll show you exactly how to do that. We\u0026rsquo;ll dive into an advanced but powerful setup using Cloudflare and Nginx Proxy Manager to bring trusted, zero-exposure SSL to everything we\u0026rsquo;ve built.\nStay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-4-automation-and-alerting/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to the part 4 of the homelab series! In the previous parts, we built a server, deployed a suite of services, and configured our network. Now, it\u0026rsquo;s time to make it resilient and self-maintaining. A homelab isn\u0026rsquo;t just about setting things up; it\u0026rsquo;s about keeping them running reliably.\u003c/p\u003e\n\u003cp\u003eThis guide will show you how to set up the three pillars of modern IT operations: \u003cstrong\u003eAutomated Backups\u003c/strong\u003e, \u003cstrong\u003eAutomated Updates\u003c/strong\u003e, and \u003cstrong\u003eProactive Alerting\u003c/strong\u003e. By the end, you\u0026rsquo;ll have a homelab that runs itself, ensures your data is safe, stays up-to-date, and notifies you when something goes wrong.\u003c/p\u003e","title":"Part 4: Automating a Homelab with Backups, Updates, and Alerts"},{"content":"My New Weekend Project: Building a Personal Ad-Blocking Server in the Cloud! Hey everyone, Prajwol here.\nLike a lot of you, I spend a good chunk of my day online. And lately, it\u0026rsquo;s felt like I\u0026rsquo;m in a constant battle with pop-ups, trackers, and auto-playing video ads. I\u0026rsquo;ve used browser extensions for years, but I wanted a more powerful solution—something that would protect my entire home network, including my phone and smart TV, without needing to install software everywhere.\nSo, I decided to take on a new project: building my very own ad-blocking DNS server in the cloud.\nI\u0026rsquo;d done something similar a while back with Linode, but this time I wanted to dive into the world of Amazon Web Services (AWS) and see if I could build a reliable, secure, and cost-effective setup from scratch. It turned into quite the adventure, involving a late-night session of launching a virtual server, wrestling with firewalls, and securing my own private domain with an SSL certificate.\nThe end result? It\u0026rsquo;s been fantastic. My web pages load noticeably faster, and the general online experience feels so much cleaner and less intrusive. Plus, knowing that I have full control over my own corner of the internet is incredibly satisfying. It\u0026rsquo;s a great feeling to see the query logs fill up with blocked requests for domains I\u0026rsquo;ve never even heard of!\nI documented every single step of my journey, from the first click in the AWS console to the final configuration on my home router. If you\u0026rsquo;re curious about how to build one for yourself, I\u0026rsquo;ve written up a complete, step-by-step guide.\nYou can check out the full project guide here! It was a challenging but really rewarding project. Let me know what you think!\nPublished: Friday, August 22, 2025\n","permalink":"http://localhost:1313/blog/adguard-aws/","summary":"\u003ch1 id=\"my-new-weekend-project-building-a-personal-ad-blocking-server-in-the-cloud\"\u003eMy New Weekend Project: Building a Personal Ad-Blocking Server in the Cloud!\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eLike a lot of you, I spend a good chunk of my day online. And lately, it\u0026rsquo;s felt like I\u0026rsquo;m in a constant battle with pop-ups, trackers, and auto-playing video ads. I\u0026rsquo;ve used browser extensions for years, but I wanted a more powerful solution—something that would protect my entire home network, including my phone and smart TV, without needing to install software everywhere.\u003c/p\u003e","title":"Building a Personal Ad-Blocking Server in the Cloud!"},{"content":"Introduction Welcome to Part 3 of my homelab series! In the previous parts, I built my server and deployed a suite of management and monitoring tools. Now, it\u0026rsquo;s time to build the brain of my network: a robust, redundant, and high-availability DNS system using AdGuard Home that works both at home and on the go.\nIn this detailed guide, I\u0026rsquo;ll walk you through how I deployed a total of three AdGuard Home instances, each with its own unique IP address. I set up a primary resolver on my homelab, a secondary failover resolver in the cloud for my mobile devices, and a tertiary resolver on a separate virtual network for local redundancy.\nChapter 1: The Local Workhorse (Primary DNS) I started by deploying my main, day-to-day DNS resolver on my homelab server.\nStep 1: Deploying AdGuard Home with Docker Compose First, I SSHed into my server, created a directory for the project, and a docker-compose.yml file to define the service.\nmkdir -p ~/docker/adguard-primary cd ~/docker/adguard-primary nano docker-compose.yml I pasted in the following configuration. This runs the AdGuard Home container, maps all the necessary ports for DNS and the web UI, and connects it to the shared npm_default network I set up in Part 2.\nservices: adguardhome: image: adguard/adguardhome:latest container_name: adguard-primary restart: unless-stopped ports: - \u0026#34;53:53/tcp\u0026#34; - \u0026#34;53:53/udp\u0026#34; - \u0026#34;8080:80/tcp\u0026#34; # Web UI - \u0026#34;853:853/tcp\u0026#34; # DNS-over-TLS volumes: - ./workdir:/opt/adguardhome/work - ./confdir:/opt/adguardhome/conf networks: - npm_default networks: npm_default: external: true I then launched the container by running:\ndocker compose up -d Step 2: Initial AdGuard Home Setup Wizard I navigated to http://\u0026lt;your-server-ip\u0026gt;:3000 in my web browser to start the setup wizard.\nI clicked \u0026ldquo;Get Started.\u0026rdquo;\nOn the \u0026ldquo;Admin Web Interface\u0026rdquo; screen, I changed the \u0026ldquo;Listen Interface\u0026rdquo; to All interfaces and the port to 80.\nOn the \u0026ldquo;DNS server\u0026rdquo; screen, I changed the \u0026ldquo;Listen Interface\u0026rdquo; to All interfaces and left the port as 53.\nI followed the prompts to create my admin username and password.\nOnce the setup was complete, I was redirected to my main dashboard, now available at http://\u0026lt;your-server-ip\u0026gt;:8080.\nStep 3: Configure My Home Router To make all my devices use AdGuard automatically, I logged into my home router\u0026rsquo;s admin panel, found the DHCP Server settings, and changed the Primary DNS Server to my homelab\u0026rsquo;s static IP address (e.g., 192.168.1.10).\nChapter 2: The Cloud Failover (Secondary DNS on Oracle Cloud) An off-site DNS server ensures I have ad-blocking on my mobile devices and acts as a backup.\nWhy I Chose Oracle Cloud After testing the free tiers of both AWS and Linode, I chose Oracle Cloud Infrastructure (OCI). In my experience, OCI\u0026rsquo;s \u0026ldquo;Always Free\u0026rdquo; tier is far more generous with its resources. It provides powerful Ampere A1 Compute instances with up to 4 CPU cores and 24 GB of RAM, plus 200 GB of storage and significant bandwidth, all for free. This was ideal for running my service 24/7 without the strict limitations or eventual costs associated with other providers.\nStep 1: Launching the Oracle Cloud VM Sign Up: I created my account on the Oracle Cloud website.\nCreate VM Instance: In the OCI console, I navigated to Compute \u0026gt; Instances and clicked \u0026ldquo;Create instance\u0026rdquo;.\nConfigure Instance:\nName: I gave it a name like AdGuard-Cloud.\nImage and Shape: I clicked \u0026ldquo;Edit\u0026rdquo;. For the image, I selected Ubuntu. For the shape, I selected \u0026ldquo;Ampere\u0026rdquo; and chose the VM.Standard.A1.Flex shape (it\u0026rsquo;s \u0026ldquo;Always Free-eligible\u0026rdquo;).\nNetworking: I used the default VCN and made sure \u0026ldquo;Assign a public IPv4 address\u0026rdquo; was checked.\nSSH Keys: I added my SSH public key.\nI clicked Create. Once the instance was running, I took note of its Public IP Address.\nStep 2: Configuring the Cloud Firewall For maximum security, I locked down the administrative ports to only my home IP address.\nFind My Public IP: I went to a site like whatismyip.com and copied my home\u0026rsquo;s public IP address.\nEdit Security List: I navigated to my instance\u0026rsquo;s details page, clicked the subnet link, then clicked the \u0026ldquo;Security List\u0026rdquo; link.\nI clicked \u0026ldquo;Add Ingress Rules\u0026rdquo; and added the following rules:\nFor SSH (Port 22): I set the Source to my home\u0026rsquo;s public IP, followed by /32 (e.g., 203.0.113.55/32). This is a critical security step.\nFor AdGuard Setup (Port 3000): I also set the Source to my home\u0026rsquo;s public IP with /32.\nFor AdGuard Web UI (Port 80/443): I set the Source to my home\u0026rsquo;s public IP with /32 as well.\nFor Public DNS (Port 53, 853, etc.): I set the Source to 0.0.0.0/0 (Anywhere) to allow all my devices to connect from any network.\nStep 3: Installing AdGuard Home \u0026amp; Configuring SSL Connect via SSH: I used the public IP and my SSH key to connect to the VM.\nRun Install Script: I chose to install AdGuard Home directly on the OS for this instance.\ncurl -s -S -L [https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh](https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh) | sh -s -- -v Get a Hostname: I went to No-IP.com, created a free hostname (e.g., my-cloud-dns.ddns.net), and pointed it to my cloud VM\u0026rsquo;s public IP.\nEnable Encryption: In my cloud AdGuard\u0026rsquo;s dashboard (Settings \u0026gt; Encryption settings), I enabled encryption, entered my No-IP hostname, and used the built-in function to request a Let\u0026rsquo;s Encrypt certificate.\nStep 4: Creating a Cloud Backup (Snapshot) A critical final step for any cloud service is creating a backup. Here is how I did it in OCI:\nIn the OCI Console, I navigated to the details page for my AdGuard-Cloud instance.\nUnder the \u0026ldquo;Resources\u0026rdquo; menu on the left, I clicked on \u0026ldquo;Boot volume\u0026rdquo;.\nOn the Boot Volume details page, under \u0026ldquo;Resources,\u0026rdquo; I clicked \u0026ldquo;Boot volume backups\u0026rdquo;.\nI clicked the \u0026ldquo;Create boot volume backup\u0026rdquo; button.\nI gave the backup a descriptive name (e.g., AdGuard-Cloud-Backup-YYYY-MM-DD) and clicked the create button. This creates a full snapshot of my server that I can use to restore it in minutes.\nStep 5: ### How to Use Your Cloud DNS on Mobile Devices The main benefit of the cloud server is having ad-blocking on the go. Here’s how I set it up on my mobile phone using secure, encrypted DNS.\nFor Android (Version 9+): Modern Android has a built-in feature called \u0026ldquo;Private DNS\u0026rdquo; that uses DNS-over-TLS (DoT), which is perfect for this.\nOpen Settings on your Android device.\nTap on \u0026ldquo;Network \u0026amp; internet\u0026rdquo; (this may be called \u0026ldquo;Connections\u0026rdquo; on some devices).\nFind and tap on \u0026ldquo;Private DNS\u0026rdquo;. You may need to look under an \u0026ldquo;Advanced\u0026rdquo; section.\nSelect the option labeled \u0026ldquo;Private DNS provider hostname\u0026rdquo;.\nIn the text box, enter the No-IP hostname you created for your Oracle Cloud server (e.g., my-cloud-dns.ddns.net).\nTap Save.\nYour phone will now send all its DNS queries through an encrypted tunnel to your personal AdGuard Home server in the cloud, giving you ad-blocking on both Wi-Fi and cellular data.\nFor iOS (iPhone/iPad): On iOS, the easiest way to set up encrypted DNS is by installing a configuration profile.\nOn your iPhone or iPad, open Safari.\nGo to a DNS profile generator site, like the one provided by AdGuard.\nWhen prompted, enter the DNS-over-HTTPS (DoH) address for your cloud server. It will be your No-IP hostname with /dns-query at the end (e.g., https://my-cloud-dns.ddns.net/dns-query).\nDownload the generated configuration profile.\nGo to your device\u0026rsquo;s Settings app. You will see a new \u0026ldquo;Profile Downloaded\u0026rdquo; item near the top. Tap on it.\nFollow the on-screen prompts to Install the profile. You may need to enter your device passcode.\nOnce installed, your iOS device will also route its DNS traffic through your secure cloud server.\nChapter 3: Ultimate Local Redundancy (Tertiary DNS with Macvlan) For an extra layer of redundancy within my homelab, I created a third AdGuard instance. By using an advanced Docker network mode called macvlan, this container gets its own unique IP address on my home network, making it a truly independent resolver.\nCreate Macvlan Network: First, I created the macvlan network, telling it which of my physical network cards to use (eth0 in my case).\ndocker network create -d macvlan \\ --subnet=192.168.1.0/24 \\ --gateway=192.168.1.1 \\ -o parent=eth0 homelab_net Deploy Tertiary Instance: I created a new folder (~/docker/adguard-tertiary) and this docker-compose.yml. Notice there are no ports since the container gets its own IP.\nservices: adguardhome2: image: adguard/adguardhome:latest container_name: adguardhome2 volumes: - \u0026#34;./work:/opt/adguardhome/work\u0026#34; - \u0026#34;./conf:/opt/adguardhome/conf\u0026#34; networks: homelab_net: ipv4_address: 192.168.1.11 # The new, unique IP for this container restart: unless-stopped networks: homelab_net: external: true Configure Router for Local Failover: To complete the local redundancy, I went back into my router\u0026rsquo;s DHCP settings.\nIn the Primary DNS field, I have the IP of my main homelab server (e.g., 192.168.1.10).\nIn the Secondary DNS field, I entered the unique IP address I assigned to my macvlan container (e.g., 192.168.1.11).\nNow, if my primary AdGuard container has an issue, all devices on my network will automatically fail over to the tertiary instance.\nChapter 4: Fine-Tuning and Integration Finally, I implemented some best practices on my primary AdGuard Home instance.\nUpstream DNS Servers: Under Settings \u0026gt; DNS Settings, I configured AdGuard to send requests to multiple resolvers in parallel for speed and reliability, using Cloudflare (1.1.1.1), Google (8.8.8.8), and Quad9 (9.9.9.9).\nEnable DNSSEC: In the same settings page, I enabled DNSSEC to verify the integrity of DNS responses.\nDNS Blocklists: I added several popular lists from the \u0026ldquo;Filters \u0026gt; DNS blocklists\u0026rdquo; page, including the AdGuard DNS filter and the OISD Blocklist, for robust protection.\nDNS Rewrites for Local Services: This is the key to a clean homelab experience. For each service, I performed a detailed two-step process:\nCreate the Proxy Host in Nginx Proxy Manager: I logged into my NPM admin panel, went to Hosts \u0026gt; Proxy Hosts, and clicked \u0026ldquo;Add Proxy Host\u0026rdquo;. For my Homer dashboard, I set the Forward Hostname to homer (the container name) and the Forward Port to 8080 (its internal port), using homer.local as the domain name.\nCreate the DNS Rewrite in AdGuard Home: I logged into my primary AdGuard dashboard, went to Filters \u0026gt; DNS Rewrites, and clicked \u0026ldquo;Add DNS rewrite\u0026rdquo;. I entered homer.local as the domain and the IP address of my Nginx Proxy Manager server as the answer.\nConclusion I\u0026rsquo;ve now built an incredibly robust, multi-layered DNS infrastructure. My home devices use the primary local server, which is backed up by a second, independent local server, and my mobile devices use a completely separate cloud instance for on-the-go protection. This provides a resilient, secure, and ad-free internet experience.\nIn the final part of this series, we\u0026rsquo;ll shift our focus from deploying services to maintaining them. I\u0026rsquo;ll show you how I set up a fully automated operations pipeline for my homelab, including daily off-site backups, automatic container updates with Watchtower, and proactive alerting with Prometheus. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-3-high-availability-dns/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to Part 3 of my homelab series! In the previous parts, I built my server and deployed a suite of management and monitoring tools. Now, it\u0026rsquo;s time to build the brain of my network: a robust, redundant, and high-availability DNS system using \u003cstrong\u003eAdGuard Home\u003c/strong\u003e that works both at home and on the go.\u003c/p\u003e\n\u003cp\u003eIn this detailed guide, I\u0026rsquo;ll walk you through how I deployed a total of \u003cstrong\u003ethree\u003c/strong\u003e AdGuard Home instances, each with its own unique IP address. I set up a primary resolver on my homelab, a secondary failover resolver in the cloud for my mobile devices, and a tertiary resolver on a separate virtual network for local redundancy.\u003c/p\u003e","title":"Part 3: A High-Availability DNS Network with AdGuard Home"},{"content":"New Project Alert: Running a Powerful AI Locally with Docker! Hey everyone, Prajwol here.\nI\u0026rsquo;ve always been fascinated by the incredible advancements in AI and large language models. While cloud-based models are powerful, I was really curious about what it would take to run a high-performance model right on my own machine. This gives you ultimate privacy, control, and the ability to experiment without limits.\nSo, for my latest project, I decided to dive in and get the DeepSeek-R1 model, a powerful AI, up and running locally using Docker.\nDocker is an amazing tool that lets you package up applications and all their dependencies into a neat little box called a container. This means you can run complex software without the headache of complicated installations or conflicts with other programs on your system. It was the perfect way to tame this powerful AI and get it running smoothly on my Ubuntu machine.\nThe process was a fantastic learning experience, covering everything from setting up Docker to pulling the model and interacting with the AI. It’s amazing to have that kind of power running on your own hardware.\nI’ve documented my entire process in a detailed, step-by-step guide. If you’re interested in local AI and want to see how you can run a powerful model yourself, be sure to check it out!\nYou can find the full project guide right here! Let me know what you think of this one!\nPublished: February, 2025\n","permalink":"http://localhost:1313/blog/running-deepseek-r1-on-docker-container-on-ubuntu/","summary":"\u003ch1 id=\"new-project-alert-running-a-powerful-ai-locally-with-docker\"\u003eNew Project Alert: Running a Powerful AI Locally with Docker!\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve always been fascinated by the incredible advancements in AI and large language models. While cloud-based models are powerful, I was really curious about what it would take to run a high-performance model right on my own machine. This gives you ultimate privacy, control, and the ability to experiment without limits.\u003c/p\u003e\n\u003cp\u003eSo, for my latest project, I decided to dive in and get the \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e model, a powerful AI, up and running locally using \u003cstrong\u003eDocker\u003c/strong\u003e.\u003c/p\u003e","title":"Running a Powerful AI Locally with Docker!"},{"content":"Introduction Welcome to Part 2 of my homelab series! In Part 1, we built a solid foundation by turning an old laptop into a hardened Debian server with Docker. Now that our server is running, we need to deploy services to manage, monitor, and easily access our projects.\nIn this guide, we\u0026rsquo;ll deploy three essential stacks. First, Nginx Proxy Manager (NPM) will act as our server\u0026rsquo;s front door and create a shared network for our containers. Second, we\u0026rsquo;ll set up a professional-grade monitoring stack with Prometheus and Grafana. Finally, we\u0026rsquo;ll deploy a Homer dashboard to create a beautiful and convenient launchpad for all our services.\n1. The Management Layer: Nginx Proxy Manager (NPM) 🌐 Before we can deploy our other services, we need a way to manage connections between them. NPM will act as our reverse proxy and, crucially, will create the shared Docker network that all our other services will connect to.\nA. Deploy Nginx Proxy Manager First, let\u0026rsquo;s create a directory and the docker-compose.yml file for NPM.\n# Create the directory mkdir -p ~/docker/npm cd ~/docker/npm # Create the docker-compose.yml nano docker-compose.yml Paste in the following configuration. This file defines the NPM service and creates a network named npm_default.\nservices: app: image: \u0026#39;jc21/nginx-proxy-manager:latest\u0026#39; container_name: npm-app-1 restart: unless-stopped ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;81:81\u0026#39; volumes: - ./data:/data - ./letsencrypt:/etc/letsencrypt networks: default: name: npm_default Launch it with\ndocker compose up -d You can now log in to the admin UI at http://\u0026lt;your-server-ip\u0026gt;:81.\n2. The Monitoring Stack 📊 With our shared network in place, we can now deploy our monitoring stack.\nPrometheus: Collects all the metrics.\nNode Exporter: Exposes the server\u0026rsquo;s hardware metrics.\ncAdvisor: Exposes Docker container metrics.\nGrafana: Visualizes all the data in beautiful dashboards.\nA. Create the Prometheus Configuration Prometheus needs a config file to know what to monitor.\n# Create the project directory mkdir -p ~/docker/monitoring cd ~/docker/monitoring # Create the prometheus.yml file nano prometheus.yml Paste in the following configuration:\nglobal: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;node-exporter\u0026#39; static_configs: - targets: [\u0026#39;node-exporter:9100\u0026#39;] - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] B. Deploy the Stack with Docker Compose Next, create the docker-compose.yml file in the same ~/docker/monitoring directory.\nnano docker-compose.yml This file defines all four monitoring services and tells them to connect to the npm_default network we created earlier.\nservices: prometheus: image: prom/prometheus:latest container_name: prometheus restart: unless-stopped ports: - \u0026#34;9090:9090\u0026#34; volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - prometheus_data:/prometheus networks: - default grafana: image: grafana/grafana:latest container_name: grafana restart: unless-stopped ports: - \u0026#34;3001:3000\u0026#34; volumes: - grafana_data:/var/lib/grafana networks: - default node-exporter: image: prom/node-exporter:latest container_name: node-exporter restart: unless-stopped volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - \u0026#39;--path.procfs=/host/proc\u0026#39; - \u0026#39;--path.sysfs=/host/sys\u0026#39; - \u0026#39;--path.rootfs=/rootfs\u0026#39; - \u0026#39;--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)\u0026#39; networks: - default cadvisor: image: gcr.io/cadvisor/cadvisor:latest container_name: cadvisor restart: unless-stopped ports: - \u0026#34;8081:8080\u0026#34; volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro networks: - default volumes: prometheus_data: grafana_data: networks: default: name: npm_default external: true Now, launch the stack:\ndocker compose up -d C. Configure Grafana Log in to Grafana at http://\u0026lt;your-server-ip\u0026gt;:3001 (default: admin/admin).\nAdd Data Source: Go to Connections \u0026gt; Data Sources, add a Prometheus source, and set the URL to http://prometheus:9090.\nImport Dashboards: Go to Dashboards \u0026gt; New \u0026gt; Import and add these dashboards by ID:\nNode Exporter Full (ID: 1860)\nDocker Host/Container Metrics (ID: 193)\n3. The Homer Launchpad Dashboard 🚀 Finally, let\u0026rsquo;s deploy Homer as our beautiful start page with custom icons.\nCreate Directories \u0026amp; Download Icons: First, create a directory for Homer and an assets subdirectory. Then, cd into the assets folder and download the icons.\nmkdir -p ~/docker/homer/assets cd ~/docker/homer/assets wget -O grafana.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/grafana.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/grafana.png) wget -O prometheus.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/prometheus.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/prometheus.png) wget -O cadvisor.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/cadvisor.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/cadvisor.png) wget -O npm.png [https://nginxproxymanager.com/icon.png](https://nginxproxymanager.com/icon.png) Create Configuration: Go back to the main homer directory and create the config.yml file.\ncd ~/docker/homer nano config.yml Paste in the following configuration. The logo: lines point to the icons we just downloaded.\n--- title: \u0026#34;Homelab Dashboard\u0026#34; subtitle: \u0026#34;Server Management\u0026#34; theme: \u0026#34;dark\u0026#34; services: - name: \u0026#34;Management\u0026#34; icon: \u0026#34;fas fa-server\u0026#34; items: - name: \u0026#34;Nginx Proxy Manager\u0026#34; logo: \u0026#34;assets/tools/npm.png\u0026#34; subtitle: \u0026#34;Reverse Proxy Admin\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:81\u0026#34; - name: \u0026#34;Monitoring\u0026#34; icon: \u0026#34;fas fa-chart-bar\u0026#34; items: - name: \u0026#34;Grafana\u0026#34; logo: \u0026#34;assets/tools/grafana.png\u0026#34; subtitle: \u0026#34;Metrics Dashboard\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:3001\u0026#34; - name: \u0026#34;Prometheus\u0026#34; logo: \u0026#34;assets/tools/prometheus.png\u0026#34; subtitle: \u0026#34;Metrics Database\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:9090\u0026#34; - name: \u0026#34;cAdvisor\u0026#34; logo: \u0026#34;assets/tools/cadvisor.png\u0026#34; subtitle: \u0026#34;Container Metrics\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:8081\u0026#34; Create Docker Compose File: Finally, create the docker-compose.yml file.\nnano docker-compose.yml This configuration connects Homer to our shared network.\nservices: homer: image: b4bz/homer container_name: homer volumes: - ./config.yml:/www/assets/config.yml - ./assets:/www/assets/tools ports: - \u0026#34;8090:8080\u0026#34; restart: unless-stopped networks: - npm_default networks: npm_default: external: true Launch: Run docker compose up -d. You can now access your new dashboard with custom icons at http://\u0026lt;your-server-ip\u0026gt;:8090.\nConclusion Our homelab now has a powerful management and monitoring foundation. Nginx Proxy Manager is ready to direct traffic, Grafana is visualizing our server\u0026rsquo;s health, and Homer provides a central launchpad.\nIn the next part of the series, we\u0026rsquo;ll deploy our core network service, AdGuard Home, and use NPM to create clean, memorable local domains for all the applications we set up today. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-2-monitoring-and-management/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to Part 2 of my homelab series! In \u003ca href=\"/projects/homelab-series-part-1-debian-docker-foundation/\"\u003ePart 1\u003c/a\u003e, we built a solid foundation by turning an old laptop into a hardened Debian server with Docker. Now that our server is running, we need to deploy services to manage, monitor, and easily access our projects.\u003c/p\u003e\n\u003cp\u003eIn this guide, we\u0026rsquo;ll deploy three essential stacks. First, \u003cstrong\u003eNginx Proxy Manager (NPM)\u003c/strong\u003e will act as our server\u0026rsquo;s front door and create a shared network for our containers. Second, we\u0026rsquo;ll set up a professional-grade monitoring stack with \u003cstrong\u003ePrometheus\u003c/strong\u003e and \u003cstrong\u003eGrafana\u003c/strong\u003e. Finally, we\u0026rsquo;ll deploy a \u003cstrong\u003eHomer\u003c/strong\u003e dashboard to create a beautiful and convenient launchpad for all our services.\u003c/p\u003e","title":"Part 2: Homelab Management \u0026 Monitoring"},{"content":"My First Cloud Ad-Blocker: A Look Back at AdGuard Home on Linode Hey everyone, Prajwol here.\nAs I continue to explore different cloud projects, I often think back to the ones that had the biggest impact on my day-to-day life. One of the very first projects that truly changed my internet experience was setting up my own ad-blocking DNS server using AdGuard Home on a Linode instance.\nMy goal was to find a simple, cost-effective way to block ads and trackers across my entire home network. I wanted a \u0026ldquo;set it and forget it\u0026rdquo; solution that would cover every device—from my phone to my smart TV—without needing to install an app on each one. Linode (now Akamai) was the perfect platform for this: straightforward, powerful, and great for hosting a lightweight service like AdGuard Home.\nThe process of spinning up a small server, running a single installation script, and then seeing the query logs light up with blocked requests was incredibly satisfying. It felt like I had taken back a real measure of control over my own network.\nThis project remains a fantastic entry point for anyone wanting to get started with self-hosting and network privacy. I\u0026rsquo;ve kept the original, detailed guide for anyone who wants to follow along.\nYou can find the full step-by-step project guide here! It’s a rewarding project that delivers tangible results almost immediately. Let me know if you give it a try!\nPublished: Friday, August 22, 2025\n","permalink":"http://localhost:1313/blog/adguard-home-on-cloud/","summary":"\u003ch1 id=\"my-first-cloud-ad-blocker-a-look-back-at-adguard-home-on-linode\"\u003eMy First Cloud Ad-Blocker: A Look Back at AdGuard Home on Linode\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eAs I continue to explore different cloud projects, I often think back to the ones that had the biggest impact on my day-to-day life. One of the very first projects that truly changed my internet experience was setting up my own ad-blocking DNS server using AdGuard Home on a Linode instance.\u003c/p\u003e\n\u003cp\u003eMy goal was to find a simple, cost-effective way to block ads and trackers across my entire home network. I wanted a \u0026ldquo;set it and forget it\u0026rdquo; solution that would cover every device—from my phone to my smart TV—without needing to install an app on each one. Linode (now Akamai) was the perfect platform for this: straightforward, powerful, and great for hosting a lightweight service like AdGuard Home.\u003c/p\u003e","title":"My First Cloud Ad-Blocker - A Look Back at AdGuard Home on Linode"},{"content":"Introduction Welcome to the first post in my new homelab series! I\u0026rsquo;ve always been fascinated by self-hosting and DevOps, and I believe the best way to learn is by doing. In this series, I\u0026rsquo;ll document my journey of turning an old, unused laptop into a powerful, efficient, and secure bare-metal server for hosting a variety of network services.\nThe goal for this first part is to lay a solid foundation. We\u0026rsquo;ll take an old laptop, install a minimal and stable Linux operating system, perform some initial security hardening, and set up Docker as our containerization engine. By the end of this post, we\u0026rsquo;ll have a perfect blank canvas ready for the exciting services we\u0026rsquo;ll deploy in the upcoming parts.\n1. Choosing the Hardware \u0026amp; OS Why an Old Laptop? Before diving in, why use an old laptop instead of a Raspberry Pi or a dedicated server? For a starter homelab, a laptop has three huge advantages:\nCost-Effective: It\u0026rsquo;s free if you have one lying around! Built-in UPS: The battery acts as a built-in Uninterruptible Power Supply (UPS), keeping the server running through short power outages. Low Power Consumption: Laptop hardware is designed to be power-efficient, which is great for a device that will be running 24/7. Why Debian 13 \u0026ldquo;Trixie\u0026rdquo;? For the operating system, I chose Debian. It\u0026rsquo;s renowned for its stability, security, and massive package repository. It’s the bedrock of many other distributions (like Ubuntu) and is perfect for a server because it\u0026rsquo;s lightweight and doesn\u0026rsquo;t include unnecessary software. We\u0026rsquo;ll be using the minimal \u0026ldquo;net-install\u0026rdquo; to ensure we only install what we absolutely need.\n2. Installation and Network Configuration The installation process is straightforward, but the network setup is key to a reliable server.\nMinimal Installation Create a Bootable USB: I downloaded the Debian 13 \u0026ldquo;netinst\u0026rdquo; ISO from the official website and used Rufus on Windows to create a bootable USB drive. Boot from USB: I plugged the USB into the laptop and booted from it (usually pressing F12, F2, or Esc during startup to select the USB device). Language, Location, and Keyboard: Selected English, United States, and the default keyboard layout. Network Setup: Connected the laptop to my home network (Ethernet preferred for stability). Hostname \u0026amp; Domain: Entered a short, memorable hostname for the server (e.g., homelab) and left the domain blank. User Accounts: Set a root password. Created a non-root regular user (this will be used for daily management). Partition Disks: Chose Guided – use entire disk with separate /home partition. This is simpler for a server setup. Software Selection: At the “Software selection” screen: Unchecked “Debian desktop environment” Checked “SSH server” and “standard system utilities” This ensures a clean command-line system that can be accessed remotely. GRUB Bootloader: Installed GRUB on the primary drive (so the system boots correctly). Finish Installation: Removed the USB drive when prompted and rebooted into the fresh Debian install. Setting a Static IP A server needs a permanent, unchanging IP address. The best way to do this is with DHCP Reservation on your router. This tells your router to always assign the same IP address to your server\u0026rsquo;s unique MAC address.\nFirst, find your laptop’s current IP address and network interface name by running:\nip a You’ll see output similar to:\n2: enp3s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\rinet 192.168.0.45/24 brd 192.168.0.255 scope global dynamic enp3s0\rvalid_lft 86396sec preferred_lft 86396sec In this example:\nInterface name: enp3s0 Current IP: 192.168.0.45 MAC address: shown under link/ether in the same section. With this info, log into your router’s admin panel, find the \u0026ldquo;DHCP Reservation\u0026rdquo; or \u0026ldquo;Static Leases\u0026rdquo; section, and assign a memorable IP address (e.g., 192.168.0.45) to your server’s MAC address.\nThis ensures the server always gets the same IP from your router, making it easy to find on your network.\nConnecting Remotely with SSH With a static IP set, all future management will be done remotely using an SSH client. For Windows, I highly recommend Solar-PuTTY. I created a new session, entered the server\u0026rsquo;s static IP address, my username, and password, and connected.\n3. Initial Server Hardening With a remote SSH session active, the first thing to do is secure the server and configure it for its headless role.\nUpdate the System First, let\u0026rsquo;s make sure all packages are up to date.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y Configure the Firewall ufw (Uncomplicated Firewall) is perfect for a simple setup. We\u0026rsquo;ll set it to deny all incoming traffic by default and only allow SSH connections.\n# Install UFW sudo apt install ufw -y # Allow SSH connections sudo ufw allow ssh # Enable the firewall sudo ufw enable Configure Lid-Close Action To ensure the laptop keeps running when the lid is closed, we edit the logind.conf file.\nsudo nano /etc/systemd/logind.conf Uncomment the line:\nHandleLidSwitch=ignore Save the file, then restart the service:\nsudo systemctl restart systemd-logind.service 4. Installing the Containerization Engine: Docker Instead of installing applications directly on our host, we\u0026rsquo;ll use Docker to keep the system clean and make management easier.\nInstall Docker Engine The official convenience script is the easiest way to get the latest version.\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Add User to Docker Group To run docker commands without sudo, add your user to the docker group. The $USER variable automatically uses the currently logged-in user.\nsudo usermod -aG docker $USER After this, log out and log back in for the change to take effect.\nInstall Docker Compose Docker Compose is essential for managing multi-container applications with a simple YAML file.\nsudo apt install docker-compose-plugin -y To verify the installation:\ndocker compose version Conclusion And that\u0026rsquo;s it for Part 1! We\u0026rsquo;ve successfully turned an old piece of hardware into a hardened, modern server running Debian and Docker with a reliable network configuration. We have a solid and secure foundation to build upon.\nIn the next part of the series, we\u0026rsquo;ll deploy our first critical service: a local, network-wide ad-blocking DNS resolver using AdGuard Home. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-1-debian-docker-foundation/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to the first post in my new homelab series! I\u0026rsquo;ve always been fascinated by self-hosting and DevOps, and I believe the best way to learn is by doing. In this series, I\u0026rsquo;ll document my journey of turning an old, unused laptop into a powerful, efficient, and secure bare-metal server for hosting a variety of network services.\u003c/p\u003e\n\u003cp\u003eThe goal for this first part is to lay a solid foundation. We\u0026rsquo;ll take an old laptop, install a minimal and stable Linux operating system, perform some initial security hardening, and set up Docker as our containerization engine. By the end of this post, we\u0026rsquo;ll have a perfect blank canvas ready for the exciting services we\u0026rsquo;ll deploy in the upcoming parts.\u003c/p\u003e","title":"Part 1: Reviving an Old Laptop with Debian \u0026 Docker"},{"content":"Your Personal Internet Guardian: How to Build a FREE Ad-Blocker in the Cloud! 🚀 Hey everyone! A while back, I wrote a guide on setting up AdGuard Home on Linode. The world of tech moves fast, and it\u0026rsquo;s time for an upgrade! Today, we\u0026rsquo;re going to build our own powerful, network-wide ad-blocker using Amazon Web Services (AWS), and we\u0026rsquo;ll make it secure with our own domain and SSL certificate.\nThink of this as building a digital gatekeeper for your internet. Before any ads, trackers, or malicious sites can reach your devices, our AdGuard Home server will slam the door shut. The best part? This works on your phone, laptop, smart TV—anything on your network—without installing a single app on them.\nThis guide is for everyone, from seasoned tech wizards to curious beginners. We\u0026rsquo;ll break down every step in simple terms, so grab a coffee, and let\u0026rsquo;s build something awesome!\n## Chapter 1: Building Our Home in the AWS Cloud ☁️ First, we need a server. We\u0026rsquo;ll use an Amazon EC2 instance, which is just a fancy name for a virtual computer that you rent.\nSign Up for AWS: If you don\u0026rsquo;t have an account, head to the AWS website and sign up. You\u0026rsquo;ll need a credit card for verification, but for this guide, we can often stay within the Free Tier.\nLaunch Your EC2 Instance:\nLog in to your AWS Console and search for EC2. Click \u0026ldquo;Launch instance\u0026rdquo;. Name: Give your server a cool name, like AdGuard-Server. Application and OS Images: In the search bar, type Debian and select the latest version (e.g., Debian 12). Make sure it\u0026rsquo;s marked \u0026ldquo;Free tier eligible\u0026rdquo;. Instance Type: Choose t2.micro. This is your free, trusty little server. Key Pair (for login): This is your digital key to the server\u0026rsquo;s front door. Click \u0026ldquo;Create a new key pair\u0026rdquo;, name it something like my-adguard-key, and download the .pem file. Keep this file secret and safe! Network settings (The Firewall): This is crucial. We need to tell our server which doors to open. Click \u0026ldquo;Edit\u0026rdquo;. Check the box for \u0026ldquo;Allow SSH traffic from\u0026rdquo; and select My IP. This lets you securely log in. Check \u0026ldquo;Allow HTTPS traffic from the internet\u0026rdquo; and \u0026ldquo;Allow HTTP traffic from the internet\u0026rdquo;. We\u0026rsquo;ll need these for our secure dashboard later. Launch It! Hit the \u0026ldquo;Launch instance\u0026rdquo; button and watch as your new cloud server comes to life.\nGive Your Server a Permanent Address (Elastic IP):\nBy default, your server\u0026rsquo;s public IP address will change every time it reboots. Let\u0026rsquo;s make it permanent! In the EC2 menu on the left, go to \u0026ldquo;Elastic IPs\u0026rdquo;. Click \u0026ldquo;Allocate Elastic IP address\u0026rdquo; and then \u0026ldquo;Allocate\u0026rdquo;. Select the new IP address from the list, click \u0026ldquo;Actions\u0026rdquo;, and then \u0026ldquo;Associate Elastic IP address\u0026rdquo;. Choose your AdGuard-Server instance from the list and click \u0026ldquo;Associate\u0026rdquo;. Your server now has a static IP address that will never change! Make a note of this new IP. ## Chapter 2: Opening the Doors (Configuring the Firewall) 🚪 Our server is running, but for maximum security, we want to ensure only you can access the administrative parts of it. We\u0026rsquo;ll open the public DNS ports to everyone, but lock down the management ports to your home IP address.\nFind Your Public IP Address: Open a new browser tab and go to a site like WhatIsMyIP.com. It will display your home\u0026rsquo;s public IP address. Copy this IP address (it will look something like 203.0.113.55).\nEdit the Firewall Rules: Go to your EC2 Instance details, click the \u0026ldquo;Security\u0026rdquo; tab, and click on the Security Group name.\nClick \u0026ldquo;Edit inbound rules\u0026rdquo; and \u0026ldquo;Add rule\u0026rdquo; for each of the following. This makes sure your DNS is publicly available but the setup panel is locked down to your IP only.\nRule for AdGuard Setup (Port 3000):\nType: Custom TCP Port range: 3000 Source: Paste your IP address here, and add /32 to the end (e.g., 203.0.113.55/32). The /32 tells AWS it\u0026rsquo;s a single, specific IP address. Rule for DNS (Port 53):\nType: Custom UDP and Custom TCP (you will add two separate rules for this port) Port range: 53 Source: Anywhere-IPv4 Rule for DNS-over-TLS (Port 853):\nType: Custom TCP Port range: 853 Source: Anywhere-IPv4 Click \u0026ldquo;Save rules\u0026rdquo;. Your firewall is now configured to allow public DNS requests while keeping your management panel secure.\n## Chapter 3: Installing AdGuard Home 🛡️ Now, let\u0026rsquo;s connect to our server and install the magic software.\nConnect via SSH: Open a terminal (PowerShell on Windows, Terminal on Mac/Linux) and use the key you downloaded to connect. Use your new Elastic IP address! # Replace the path and Elastic IP with your own ssh -i \u0026#34;path/to/my-adguard-key.pem\u0026#34; admin@YOUR_ELASTIC_IP Install AdGuard Home: Run this one simple command. It downloads and installs everything for you. curl -s -S -L [https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh](https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh) | sh -s -- -v Run the Setup Wizard: The script will give you a link, like http://YOUR_ELASTIC_IP:3000. Open this in your browser. Follow the on-screen steps to create your admin username and password. ## Chapter 4: Teaching Your Guardian Who to Trust and What to Block With AdGuard Home installed, the next step is to configure its core brain: the DNS servers it gets its answers from and the blocklists it uses to protect your network.\n1. Setting Up Upstream DNS Servers Think of \u0026ldquo;Upstream DNS Servers\u0026rdquo; as the giant, public phonebooks of the internet. When your AdGuard server doesn\u0026rsquo;t know an address (and it\u0026rsquo;s not on a blocklist), it asks one of these upstreams. It\u0026rsquo;s recommended to use a mix of the best encrypted DNS providers for security, privacy, and speed.\nIn the AdGuard dashboard, go to Settings -\u0026gt; DNS settings. In the \u0026ldquo;Upstream DNS servers\u0026rdquo; box, enter the following, one per line:\nhttps://dns.quad9.net/dns-query https://dns.google/dns-query https://dns.cloudflare.com/dns-query Quad9: Focuses heavily on security, blocking malicious domains. Google: Known for being very fast. Cloudflare: A great all-around choice with a strong focus on privacy. 2. Optimizing DNS Performance Still in the DNS settings page, scroll down to optimize how your server queries the upstreams.\nParallel requests: Select this option. This is the fastest and most resilient mode. It sends your DNS query to all three of your upstream servers at the same time and uses the answer from the very first one that responds. This ensures you always get the quickest possible result.\nEnable EDNS client subnet (ECS): Check this box. This is very important for services like Netflix, YouTube, and other content delivery networks (CDNs). It helps them give you content from a server that is geographically closest to you, resulting in faster speeds and a better experience.\n3. Enabling DNSSEC Right below the upstream servers, there\u0026rsquo;s a checkbox for \u0026ldquo;Enable DNSSEC\u0026rdquo;. You should check this box. DNSSEC is like a digital wax seal on a letter; it verifies that the DNS answers you\u0026rsquo;re getting are authentic and haven\u0026rsquo;t been tampered with. It\u0026rsquo;s a simple, one-click security boost.\n4. Choosing Your Blocklists This is the fun part—the actual ad-blocking! Go to Filters -\u0026gt; DNS blocklists. For a \u0026ldquo;Balanced \u0026amp; Powerful\u0026rdquo; setup that blocks aggressively without a high risk of breaking websites, enable the following lists:\nAdGuard DNS filter: A great, well-maintained baseline. OISD Blocklist Big: Widely considered one of the best all-in-one lists for blocking ads, trackers, and malware. HaGeZi\u0026rsquo;s Pro Blocklist: A fantastic list that adds another layer of aggressive blocking for privacy. HaGeZi\u0026rsquo;s Threat Intelligence Feed: A crucial security-only list that focuses on protecting against active threats like phishing and malware. This combination will give you robust protection against both annoyances and real dangers.\n## Chapter 5: Giving Your Server a Name (Free Domain with No-IP) 📛 An IP address is hard to remember. Let\u0026rsquo;s get a free, memorable name for our server.\nSign Up at No-IP: Go to No-IP.com, create a free account, and create a hostname (e.g., my-dns.ddns.net). Point it to Your Server: When creating the hostname, enter your server\u0026rsquo;s permanent Elastic IP address. Confirm your account via email. ## Chapter 6: Making It Secure with SSL/TLS 🔐 We\u0026rsquo;ll use Let\u0026rsquo;s Encrypt and Certbot to get a free SSL certificate, which lets us use secure https:// and encrypted DNS.\nInstall Certbot: In your SSH session, run these commands:\nsudo apt update sudo apt install certbot -y Get the Certificate: Run this command, replacing the email and domain with your own.\n# This command will temporarily stop any service on port 80, get the certificate, and then finish. sudo certbot certonly --standalone --agree-tos --email YOUR_EMAIL@example.com -d your-no-ip-hostname.ddns.net If it\u0026rsquo;s successful, it will tell you where your certificate files are saved (usually in /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/).\nConfigure AdGuard Home Encryption:\nGo to your AdGuard Home dashboard (Settings -\u0026gt; Encryption settings). Check \u0026ldquo;Enable encryption\u0026rdquo;. In the \u0026ldquo;Server name\u0026rdquo; field, enter your No-IP hostname. Under \u0026ldquo;Certificates\u0026rdquo;, choose \u0026ldquo;Set a certificates file path\u0026rdquo;. Certificate path: /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/fullchain.pem Private key path: /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/privkey.pem Click \u0026ldquo;Save configuration\u0026rdquo;. The page will reload on a secure https:// connection! ## Chapter 7: Automating SSL Renewal (Cron Job Magic) ✨ Let\u0026rsquo;s Encrypt certificates last for 90 days. We can tell our server to automatically renew them.\nOpen the Cron Editor: In SSH, run sudo crontab -e and choose nano as your editor. Add the Renewal Job: Add this line to the bottom of the file. It tells the server to try renewing the certificate every day at 2:30 AM. 30 2 * * * systemctl stop AdGuardHome.service \u0026amp;\u0026amp; certbot renew --quiet \u0026amp;\u0026amp; systemctl start AdGuardHome.service Save and exit (Ctrl+X, then Y, then Enter). Your server will now keep its certificate fresh forever! ## Chapter 8: Testing Your New Superpowers (DoH \u0026amp; DoT) 🧪 For a direct confirmation, I used these commands on my computer:\nDNS-over-HTTPS (DoH) Test: This test checks if the secure web endpoint for DNS is alive.\ncurl -v [https://your-no-ip-hostname.ddns.net/dns-query](https://your-no-ip-hostname.ddns.net/dns-query) I got a \u0026ldquo;405 Method Not Allowed\u0026rdquo; error, which sounds bad but is actually great news. It means I successfully connected to the server, which correctly told me I didn\u0026rsquo;t send a real query. The connection works!\nDNS-over-TLS (DoT) Test: This checks the dedicated secure port for DNS. I used a tool called kdig.\n# I had to install it first with: sudo apt install knot-dnsutils kdig @your-no-ip-hostname.ddns.net +tls-ca +tls-host=your-no-ip-hostname.ddns.net example.com The command returned a perfect DNS answer for example.com, confirming the secure tunnel was working.\n## Chapter 9: Protecting Your Kingdom (Router \u0026amp; Phone Setup) 🏰 Now, let\u0026rsquo;s point your devices to their new guardian.\nOn Your Home Router: Log in to your router\u0026rsquo;s admin page, find the DNS settings, and enter your server\u0026rsquo;s Elastic IP as the primary DNS server. Leave the secondary field blank! This forces all devices on your Wi-Fi to be protected. Then, restart your router. On Your Mobile Phone: Android: Go to Settings -\u0026gt; Network -\u0026gt; Private DNS. Choose \u0026ldquo;Private DNS provider hostname\u0026rdquo; and enter your No-IP hostname (my-dns.ddns.net). This gives you ad-blocking everywhere, even on cellular data! iOS: You can use a profile to configure DoH. A simple way is to use a site like AdGuard\u0026rsquo;s DNS profile generator, but enter your own server\u0026rsquo;s DoH address (https://my-dns.ddns.net/dns-query). ## Chapter 10: The Ultimate Safety Net (Creating a Snapshot) 📸 Finally, let\u0026rsquo;s back up our perfect setup.\nIn the EC2 Console, go to your instance details. Click the \u0026ldquo;Storage\u0026rdquo; tab and click the \u0026ldquo;Volume ID\u0026rdquo;. Click \u0026ldquo;Actions\u0026rdquo; -\u0026gt; \u0026ldquo;Create snapshot\u0026rdquo;. Give it a description, like AdGuard-Working-Setup-Backup. If you ever mess something up, you can use this snapshot to restore your server to this exact working state in minutes.\n## Bonus Chapter: Common Troubleshooting Tips If things aren\u0026rsquo;t working, here are a few common pitfalls to check:\nBrowser Overrides Everything: If one device isn\u0026rsquo;t blocking ads, check its browser settings! Modern browsers like Chrome have a \u0026ldquo;Secure DNS\u0026rdquo; feature that can bypass your custom setup. You may need to turn this off. Check Your Laptop\u0026rsquo;s DNS: Make sure your computer\u0026rsquo;s network settings are set to \u0026ldquo;Obtain DNS automatically\u0026rdquo; so it listens to the router. A manually set DNS on your PC will ignore the router\u0026rsquo;s settings. Beware of IPv6: If you run into trouble on one device, try disabling IPv6 in that device\u0026rsquo;s Wi-Fi adapter properties to force it to use your working IPv4 setup. ## It’s a Wrap! And there you have it! You\u0026rsquo;ve successfully built a personal, secure, ad-blocking DNS server in the cloud. You\u0026rsquo;ve learned about cloud computing, firewalls, DNS, SSL, and automation. Go enjoy a faster, cleaner, and more private internet experience.\n","permalink":"http://localhost:1313/projects/adguard-updated/","summary":"\u003ch1 id=\"your-personal-internet-guardian-how-to-build-a-free-ad-blocker-in-the-cloud-\"\u003eYour Personal Internet Guardian: How to Build a FREE Ad-Blocker in the Cloud! 🚀\u003c/h1\u003e\n\u003cp\u003eHey everyone! A while back, I wrote a guide on setting up AdGuard Home on Linode. The world of tech moves fast, and it\u0026rsquo;s time for an upgrade! Today, we\u0026rsquo;re going to build our own powerful, network-wide ad-blocker using \u003cstrong\u003eAmazon Web Services (AWS)\u003c/strong\u003e, and we\u0026rsquo;ll make it secure with our own domain and SSL certificate.\u003c/p\u003e\n\u003cp\u003eThink of this as building a digital gatekeeper for your internet. Before any ads, trackers, or malicious sites can reach your devices, our AdGuard Home server will slam the door shut. The best part? This works on your phone, laptop, smart TV—anything on your network—without installing a single app on them.\u003c/p\u003e","title":"How I Built My Own Ad-Blocking DNS Server in the Cloud (2025 Updated Edition!)"},{"content":"What\u0026rsquo;s the buzz about AdGuard Home? Imagine AdGuard Home as your personal internet guardian. This versatile tool blocks ads, trackers, and other online nuisances across all devices connected to your network. Whether you\u0026rsquo;re browsing on your phone, tablet, or computer, AdGuard Home has your back.\nIn today\u0026rsquo;s digital landscape, robust security measures are paramount. Protecting each device shields your family from accidental clicks and malicious attacks, ensuring peace of mind and a secure online environment.\nWhy on the Cloud? While setting up AdGuard Home on your home network is great, installing it on a cloud server like Linode takes things up a notch. Here\u0026rsquo;s why:\nOn-the-Go Protection: Your devices stay protected from ads and trackers, no matter where you are, you can even share it with your family. Centralized Control: Manage and customize your ad-blocking settings from a single dashboard. Enhanced Privacy: Keep your browsing data away from prying eyes. Ready to embark on this ad-free adventure? Let\u0026rsquo;s get started!\nSetting Up The Environment Step 1: Create a Linode Cloud Account Why choose Linode? Through NetworkChuck\u0026rsquo;s referral link, you receive a generous $100 cloud credit - a fantastic start!\nSign Up: Navigate to Linode\u0026rsquo;s signup page and register. Access the Dashboard: Log in and select \u0026lsquo;Linodes\u0026rsquo; from the left-side menu. Create a Linode: Click \u0026lsquo;Create Linode,\u0026rsquo; choose your preferred region, and select an operating system (Debian 11 is a solid choice). Choose a Plan: The Shared 1GB Nanode instance is sufficient for AdGuard Home. Label and Secure: Assign a label to your Linode and set a strong root password. Deploy: Click \u0026lsquo;Create Linode\u0026rsquo; and wait for it to initialize. Once your Linode is up and running, access it via the LISH Console or SSH. (use root as localhost login)\nStep 2: Installing AdGuard Home on Linode Yes, we\u0026rsquo;re already into setting up at this point.\nLog In: Access your Linode using SSH or the LISH Console with your root credentials. Update the system: sudo apt update \u0026amp;\u0026amp; apt upgrade -y Go ahead and copy this command to Install Adguard Home: curl -s -S -L https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh | sh -s -- -v AdGuard Home is installed and running. You can use CTRL+Shift+V to paste into the terminal.\nStep 3: Configure AdGuard Home Post-installation, you\u0026rsquo;ll see a list of IP addresses with port :3000. Access the Web Interface: Open your browser and navigate to the IP address followed by :3000. If you encounter a security warning, proceed by clicking \u0026ldquo;Continue to site.\u0026rdquo; Initial Setup: Click \u0026lsquo;Get Started\u0026rsquo; and follow the prompts. When uncertain, default settings are typically fine. Set Credentials: Set up the Username and Password. Step 4: Integrate AdGuard Home with Your Router After this, your AdGuard Home is running, but in order to use it on your devices you need to set up inside your home router for all your devices to be protected. For that, I can\u0026rsquo;t walk you through each and every router\u0026rsquo;s settings, but the steps are pretty similar.\nFind your router IP address, you should be able to find it on the back of your router (commonly 192.168.0.1 or 192.169.1.1) enter it into your browser. Login into your router using the credentials mentioned in the back of your router; the default is often admin for both username and password. I suggest you change your default password. Configure DNS Settings: Enable DHCP Server: Ensure your router\u0026rsquo;s DHCP server is active. Set DNS Addresses: Input your AdGuard Home server\u0026rsquo;s IP as the primary DNS (mine was 96.126.113.207). For secondary DNS, options like 1.1.1.1 (Cloudflare), 9.9.9.9 (Quad9), or 8.8.8.8 (Google) are reliable. Save and apply the changes. Fine-Tuning AdGuard Home If you\u0026rsquo;ve done everything till here you should be good, but for those who enjoy customizations, AdGuard Home offers a plethora of settings. Some of the customization I did are:\nSettings Go to Settings -\u0026gt; General Settings: You can enable Parental Control and Safe Search. You can also make your Statistics last longer than 24hrs which is default. Now on Settings -\u0026gt; DNS Settings By default, it uses DNS from quad9 which is pretty good but I suggest you add more. You can click on the list of known DNS providers, which you can choose from. I used: https://dns.quad9.net/dns-query https://dns.google/dns-query https://dns.cloudflare.com/dns-query Enable \u0026lsquo;Load Balancing\u0026rsquo; to distribute queries evenly. Scroll down to \u0026lsquo;DNS server configuration\u0026rsquo; and enable DNSSEC for enhanced security. Click on Save. Filters DNS blocklists Go to Filters -\u0026gt; DNS blocklists, here you can add a blocklist that people have created and use it to block even more things. By default, AdGuard uses the AdGuard DNS filter, and you can add more.\nClick on Add blocklist -\u0026gt; Choose from the list Don\u0026rsquo;t choose too many from the list cause it may slow your internet requests. These are the blocklists I added. And just like that you are blocking more and more things. DNS rewrites Go to Filters -\u0026gt; DNS rewrites, here you can add your own DNS entries, so I added AdGuard here.\nClick on Add DNS rewrite Type in domain adguardforme.local and your IP address for AdGuard Home. And save it. Now, when I want to go on the AdGuard Home dashboard I just type in adguardforme.local and I\u0026rsquo;m into AdGuard, I don\u0026rsquo;t have to remember the IP address.\nCustom filtering rules Go to Filters -\u0026gt; Custom filtering rules. For some reason when I use Facebook on mobile device stories and videos did not load up, so I added custom filtering rules.\n@@||graph.facebook.com^$important ","permalink":"http://localhost:1313/projects/adguard-home-on-cloud/","summary":"\u003ch1 id=\"whats-the-buzz-about-adguard-home\"\u003eWhat\u0026rsquo;s the buzz about AdGuard Home?\u003c/h1\u003e\n\u003cp\u003eImagine AdGuard Home as your personal internet guardian. This versatile tool blocks ads, trackers, and other online nuisances across all devices connected to your network. Whether you\u0026rsquo;re browsing on your phone, tablet, or computer, AdGuard Home has your back.\u003c/p\u003e\n\u003cp\u003eIn today\u0026rsquo;s digital landscape, robust security measures are paramount. Protecting each device shields your family from accidental clicks and malicious attacks, ensuring peace of mind and a secure online environment.\u003c/p\u003e","title":"Running Private Adguard Server on Cloud (Linode)"},{"content":"What\u0026rsquo;s a Docker Container? Before we dive into setting up DeepSeek-R1, let me explain what a Docker container is. Imagine you have a toy that works perfectly on your birthday but gets broken if you move it to another room. A Docker container is like a magic box that keeps your AI model (the toy) in perfect condition wherever you take it, whether it\u0026rsquo;s running as a background task, on a web server, or even in the cloud.\nDocker containers encapsulate everything required to run an application: the code, dependencies, and environment settings. This ensures consistency across different machines, which is super important for AI models that rely on precise configurations.\nSetting Up The Environment Step 1: Install Ubuntu on Windows (If You Haven\u0026rsquo;t Already) If you\u0026rsquo;re using Windows, the easiest way to get an Ubuntu environment is through the Microsoft Store. Here\u0026rsquo;s how:\nOpen the Microsoft Store and search for Ubuntu. Click Get and let it install. Once installed, open Ubuntu from the Start menu and follow the setup instructions. Update the system: sudo apt update \u0026amp;\u0026amp; sudo apt upgrade Now, you have an Ubuntu terminal running on Windows!\nStep 2: Install Docker (If You Haven\u0026rsquo;t Already) First, let\u0026rsquo;s check if you have Docker installed. Open a terminal and run:\ndocker --version If that returns a version number, congrats! If not, install Docker:\nsudo apt update \u0026amp;\u0026amp; sudo apt install docker.io -y sudo systemctl enable --now docker Step 3: Prerequisites for NVIDIA GPU Install NVIDIA Container Toolkit:\nConfiguring the production repository: curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list Update the package list: sudo apt-get update Install the NVIDIA Container Toolkit: sudo apt-get install -y nvidia-container-toolkit Running Ollama Inside Docker Run these commands(P.S. shoutout to NetworkChuck):\ndocker run -d \\ --gpus all \\ -v ollama:/root/.ollama \\ -p 11434:11434 \\ --security-opt=no-new-privileges \\ --cap-drop=ALL \\ --cap-add=SYS_NICE \\ --memory=8g \\ --memory-swap=8g \\ --cpus=4 \\ --read-only \\ --name ollama \\ ollama/ollama Running DeepSeek-R1 Locally Time to bring DeepSeek-R1 to life locally and containerized:\ndocker exec -it ollama ollama run deepseek-r1 or you can run other versions of deepseek-r1 just by typing in the version at the end after a colon(:)\ndocker exec -it ollama ollama run deepseek-r1:7b After this, play around with the AI, if you wanna exit just type:\n/bye Starting Deepseek-R1 To Start Deepseek-R1 from next time go to Ubuntu and type:\ndocker start ollama this will start ollama docker container; then type:\ndocker exec -it ollama ollama run deepseek-r1:7b ","permalink":"http://localhost:1313/projects/running-deepseek-r1-on-docker-container-on-ubuntu/","summary":"\u003ch1 id=\"whats-a-docker-container\"\u003eWhat\u0026rsquo;s a Docker Container?\u003c/h1\u003e\n\u003cp\u003eBefore we dive into setting up DeepSeek-R1, let me explain what a Docker container is. Imagine you have a toy that works perfectly on your birthday but gets broken if you move it to another room. A Docker container is like a magic box that keeps your AI model (the toy) in perfect condition wherever you take it, whether it\u0026rsquo;s running as a background task, on a web server, or even in the cloud.\u003c/p\u003e","title":"Dive into AI Fun: Running DeepSeek-R1 on a Docker Container on Ubuntu"},{"content":"Description I joined AbbVie initially as a contractor and quickly demonstrated the skills and dedication that led to my conversion to a full-time position. In my role, I stepped into a high-stakes production environment where precision and operational stability are paramount. My work centered on the optimization and maintenance of sophisticated, machine learning-based visual inspection systems. I was responsible for fine-tuning these models, analyzing their performance data, and troubleshooting complex technical issues across both hardware and software, including the POM and PCE systems.\nThis wasn\u0026rsquo;t just about keeping machines running; it was about enhancing them. By applying a systematic, data-driven approach, I contributed to a 30% reduction in product waste, a metric that translates directly to improved efficiency and sustainability. Working within strict GMPs and utilizing systems like SAP for material tracking, I learned to balance technical problem-solving with rigorous compliance, ensuring that every action contributed to the stability and reliability of mission-critical operations.\n","permalink":"http://localhost:1313/experience/abbvie/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eI joined AbbVie initially as a contractor and quickly demonstrated the skills and dedication that led to my conversion to a full-time position. In my role, I stepped into a high-stakes production environment where precision and operational stability are paramount. My work centered on the optimization and maintenance of sophisticated, machine learning-based visual inspection systems. I was responsible for fine-tuning these models, analyzing their performance data, and troubleshooting complex technical issues across both hardware and software, including the POM and PCE systems.\u003c/p\u003e","title":"Operator III"},{"content":"Description As my first professional role after moving to the United States, my position at FedEx was a crucial step in adapting my technical skills to a new corporate environment. My journey began as a contractor, where my performance and analytical skills in a fast-paced setting led to my transition to a full-time Associate role. At the device testing center, I was on the front lines of quality assurance for a wide array of consumer electronics. I conducted comprehensive, systematic testing on mobile devices, smartwatches, and routers, executing detailed test plans to identify hardware vulnerabilities, software bugs, and non-compliance with network standards.\nMy responsibilities included meticulously documenting my findings, reproducing bugs to assist developers, and providing clear, actionable reports to engineering teams. This collaborative process was crucial in accelerating the repair cycle and ensuring that products met the highest standards of quality and security before reaching the market. The role sharpened my analytical skills and gave me a deep appreciation for the importance of rigorous testing in the software development lifecycle.\n","permalink":"http://localhost:1313/experience/fedex/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs my first professional role after moving to the United States, my position at FedEx was a crucial step in adapting my technical skills to a new corporate environment. My journey began as a contractor, where my performance and analytical skills in a fast-paced setting led to my transition to a full-time Associate role. At the device testing center, I was on the front lines of quality assurance for a wide array of consumer electronics. I conducted comprehensive, systematic testing on mobile devices, smartwatches, and routers, executing detailed test plans to identify hardware vulnerabilities, software bugs, and non-compliance with network standards.\u003c/p\u003e","title":"Product Testing Associate"},{"content":"Description As the IT Support Specialist for a bustling international college with over 2,500 students and staff, I was at the heart of the campus\u0026rsquo;s technical operations. My role was dynamic and comprehensive, involving end-to-end technical support across a diverse, multi-building campus. I managed the entire user lifecycle, from onboarding new accounts to ensuring smooth system setups across Windows, Linux, and Mac environments. I was the primary point of contact for all technical challenges, resolving Tier 1 and 2 support tickets with a 90% SLA adherence and troubleshooting complex OS issues to minimize downtime.\nMy tenure was marked by significant growth and adaptation. I led the complete technical setup of eight new computer labs, managing everything from hardware deployment and network cabling to software installation and configuration. When the COVID-19 pandemic hit, I was instrumental in transitioning the campus to a hybrid learning model, my first professional experience navigating such a large-scale shift. This required rapidly scaling our remote support capabilities and ensuring both students and faculty could operate effectively from anywhere.\nA cornerstone project of my time was the complete technical overhaul of the newly acquired Kumari Film Hall. I was deeply involved in the project to transform the old cinema into modern lecture halls, which included designing and deploying the entire network infrastructure, setting up AV systems, and ensuring seamless integration with the main campus network.\nTo support these expanding operations, I took the lead in deploying a new UVDesk help desk ticketing system on a CentOS server and embraced automation, utilizing tools like OK Goldy to streamline user creation in Google Workspace. These initiatives standardized processes, improved efficiency, and allowed our team to successfully manage the college\u0026rsquo;s ambitious growth.\n","permalink":"http://localhost:1313/experience/islingtoncollege/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs the IT Support Specialist for a bustling international college with over 2,500 students and staff, I was at the heart of the campus\u0026rsquo;s technical operations. My role was dynamic and comprehensive, involving end-to-end technical support across a diverse, multi-building campus. I managed the entire user lifecycle, from onboarding new accounts to ensuring smooth system setups across Windows, Linux, and Mac environments. I was the primary point of contact for all technical challenges, resolving Tier 1 and 2 support tickets with a 90% SLA adherence and troubleshooting complex OS issues to minimize downtime.\u003c/p\u003e","title":"IT Support Specialist"},{"content":"Description Building on my foundational experience, my internship at BlackBox Technologies immersed me in a more complex, project-based environment. I was an integral part of a development team tasked with building a web-based attendance system from the ground up. This role provided me with invaluable hands-on, full-stack experience. I contributed to the backend by assisting senior engineers with the development of business logic in .NET, giving me insight into server-side architecture. Simultaneously, I was responsible for building responsive, user-facing components for the front-end using HTML, CSS, and JavaScript.\nThis experience was a deep dive into the software development lifecycle. I learned how to translate business requirements into technical specifications, participated in code reviews, and understood the synergy between front-end and back-end systems. Working in close collaboration with the engineering team on a single, focused product was an excellent opportunity to apply my skills to a real-world project and solidify my understanding of creating robust, scalable web applications.\n","permalink":"http://localhost:1313/experience/blackbox/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eBuilding on my foundational experience, my internship at BlackBox Technologies immersed me in a more complex, project-based environment. I was an integral part of a development team tasked with building a web-based attendance system from the ground up. This role provided me with invaluable hands-on, full-stack experience. I contributed to the backend by assisting senior engineers with the development of business logic in .NET, giving me insight into server-side architecture. Simultaneously, I was responsible for building responsive, user-facing components for the front-end using HTML, CSS, and JavaScript.\u003c/p\u003e","title":"Web Development Intern"},{"content":"Description My journey into professional software development began at Radiant Infotech, my first internship and job in the tech industry. This role was a pivotal transition from academic theory to real-world application. I was entrusted with supporting the full lifecycle of client websites, which provided an immersive learning experience. My primary responsibility was to develop responsive, pixel-perfect front-end interfaces using HTML, CSS, and Bootstrap, translating design files into functional web components. A key part of this process was using Adobe Photoshop to prepare and optimize web graphics, ensuring both aesthetic quality and optimal performance.\nBeyond the initial development, my role extended to managing website content through various CMS platforms and performing rigorous debugging to ensure cross-browser compatibility and a seamless user experience. This foundational internship was crucial in building my confidence and skills in modern web development, teaching me how to collaborate effectively within a team to deliver high-quality digital products for clients.\n","permalink":"http://localhost:1313/experience/radiantinfotech/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eMy journey into professional software development began at Radiant Infotech, my first internship and job in the tech industry. This role was a pivotal transition from academic theory to real-world application. I was entrusted with supporting the full lifecycle of client websites, which provided an immersive learning experience. My primary responsibility was to develop responsive, pixel-perfect front-end interfaces using HTML, CSS, and Bootstrap, translating design files into functional web components. A key part of this process was using Adobe Photoshop to prepare and optimize web graphics, ensuring both aesthetic quality and optimal performance.\u003c/p\u003e","title":"Web Development Intern"},{"content":"Introduction Welcome to my personal portfolio website. I respect your privacy and am committed to protecting it. This policy outlines what information is collected when you visit my site and how that information is used.\nInformation Collection and Use I collect information in two ways: information you provide directly and anonymous data collected by analytics services.\nPersonal Data You Provide When you request a copy of my resume, you are asked to voluntarily provide your email address.\nHow it\u0026rsquo;s collected: This information is collected via an embedded Google Form. Why it\u0026rsquo;s collected: It is used for the sole purpose of sending the requested resume document to you through an automated process managed by Google Apps Script. How it\u0026rsquo;s used: Your email will not be used for marketing purposes, sold, or shared with any third parties. Anonymous Usage Data To improve the user experience and analyze traffic, this website uses the following third-party services:\nUmami (Self-Hosted): This website uses a self-hosted instance of Umami for privacy-focused web analytics. Umami collects anonymous usage data such as page views, referrers, and geographic regions to help me understand website traffic. This service does not use cookies, does not collect any personally identifiable information, and all data is stored on a private server under my control.\nCloudflare Web Analytics: This service collects anonymous traffic data such as page views and country of origin. It does not use cookies or collect personally identifiable information. You can view their privacy policy here.\nService Providers This website relies on the following third-party service providers to function:\nGoogle Workspace (Forms, Sheets, Apps Script): Used to manage and automate resume requests. Cloudflare: Used as a Content Delivery Network (CDN) to improve website performance, as a security firewall to protect against malicious attacks, and for collecting anonymous web analytics. Vercel: Used to host the self-hosted Umami analytics application. Netlify \u0026amp; GitHub: Used for hosting and deploying the website. Changes to This Privacy Policy I may update this Privacy Policy from time to time. I will notify you of any changes by posting the new Privacy Policy on this page. You are advised to review this page periodically for any changes.\nContact Me If you have any questions about this Privacy Policy, please contact me at: prajwolad18@gmail.com\n","permalink":"http://localhost:1313/privacy-policy/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWelcome to my personal portfolio website. I respect your privacy and am committed to protecting it. This policy outlines what information is collected when you visit my site and how that information is used.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"information-collection-and-use\"\u003eInformation Collection and Use\u003c/h2\u003e\n\u003cp\u003eI collect information in two ways: information you provide directly and anonymous data collected by analytics services.\u003c/p\u003e\n\u003ch3 id=\"personal-data-you-provide\"\u003ePersonal Data You Provide\u003c/h3\u003e\n\u003cp\u003eWhen you request a copy of my resume, you are asked to voluntarily provide your email address.\u003c/p\u003e","title":"Privacy Policy"},{"content":"Introduction Welcome to the part 4 of the homelab series! In the previous parts, we built a server, deployed a suite of services, and configured our network. Now, it\u0026rsquo;s time to make it resilient and self-maintaining. A homelab isn\u0026rsquo;t just about setting things up; it\u0026rsquo;s about keeping them running reliably.\nThis guide will show you how to set up the three pillars of modern IT operations: Automated Backups, Automated Updates, and Proactive Alerting. By the end, you\u0026rsquo;ll have a homelab that runs itself, ensures your data is safe, stays up-to-date, and notifies you when something goes wrong.\nChapter 1: The Automated Backup Strategy (at 3 AM) A solid backup strategy is non-negotiable. I implemented a robust system inspired by the \u0026ldquo;3-2-1\u0026rdquo; rule, focusing on redundancy and an off-site copy. My strategy involves maintaining two copies of my data in two separate locations: one local backup on the server itself for fast recovery, and one automated, off-site backup to Google Drive to protect against a local disaster like a fire or hardware failure.\nThis script runs at 3 AM, creates a local backup, uploads it, and then notifies Discord.\nStep 1: Configure rclone for Google Drive First, you need a tool to communicate with Google Drive. We\u0026rsquo;ll use rclone.\nInstall rclone on your Debian server: sudo -v ; curl [https://rclone.org/install.sh](https://rclone.org/install.sh) | sudo bash Run the interactive setup: rclone config Follow the Prompts: n (New remote) * name\u0026gt;: gdrive (You can name it anything) storage\u0026gt;: Find and select drive (Google Drive). client_id\u0026gt; \u0026amp; client_secret\u0026gt;: Press Enter for both to leave blank. scope\u0026gt;: Choose 1 (Full access). Use auto config? y/n\u0026gt;: This is a critical step. Since we are on a headless server, type n and press Enter. Authorize Headless: rclone will give you a command to run on a machine with a web browser (like your main computer). On your main computer (where you have rclone installed), run the rclone authorize \u0026quot;drive\u0026quot; \u0026quot;...\u0026quot; command. This will open your browser, ask you to log in to Google, and grant permission. Your main computer\u0026rsquo;s terminal will then output a block of text (your config_token). Paste Token: Copy the token from your main computer and paste it back into your server\u0026rsquo;s rclone prompt. Finish the prompts, and your connection is complete. Step 2: Create the Backup Script Next, create a shell script to perform the backup.\nCreate the file and make it executable:\nnano ~/backup.sh chmod +x ~/backup.sh Paste in the following script. You must edit the first 7 variables to match your setup.\n#!/bin/bash # --- Configuration --- SOURCE_DIR=\u0026#34;/path/to/your/docker\u0026#34; # \u0026lt;-- Change to your Docker projects directory BACKUP_DIR=\u0026#34;/path/to/your/backups\u0026#34; # \u0026lt;-- Change to your backups folder FILENAME=\u0026#34;homelab-backup-$(date +%Y-%m-%d).tar.gz\u0026#34; LOCAL_RETENTION_DAYS=3 CLOUD_RETENTION_DAYS=3 RCLONE_REMOTE=\u0026#34;gdrive\u0026#34; # \u0026lt;-- Must match your rclone remote name RCLONE_DEST=\u0026#34;Homelab Backups\u0026#34; # \u0026lt;-- Folder name in Google Drive # --- \u0026#34;https://discordapp.com/api/webhooks/141949178941/6Tx6f1yjf26LztQ\u0026#34; --- DISCORD_WEBHOOK_URL=\u0026#34;YOUR_DISCORD_WEBHOOK_URL\u0026#34; # --- Notification Function --- send_notification() { MESSAGE=$1 curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#34;{\\\u0026#34;content\\\u0026#34;: \\\u0026#34;$MESSAGE\\\u0026#34;}\u0026#34; \u0026#34;$DISCORD_WEBHOOK_URL\u0026#34; } # --- Script Logic --- echo \u0026#34;--- Starting Homelab Backup: $(date) ---\u0026#34; send_notification \u0026#34;✅ Starting Homelab Backup...\u0026#34; # 1. Create local backup echo \u0026#34;Creating local backup...\u0026#34; tar -czf \u0026#34;${BACKUP_DIR}/${FILENAME}\u0026#34; -C \u0026#34;${SOURCE_DIR}\u0026#34; . echo \u0026#34;Local backup created at ${BACKUP_DIR}/${FILENAME}\u0026#34; # 2. Upload to Google Drive echo \u0026#34;Uploading backup to ${RCLONE_REMOTE}...\u0026#34; rclone copy \u0026#34;${BACKUP_DIR}/${FILENAME}\u0026#34; \u0026#34;${RCLONE_REMOTE}:${RCLONE_DEST}\u0026#34; echo \u0026#34;Upload complete.\u0026#34; # 3. Clean up local backups echo \u0026#34;Cleaning up local backups older than ${LOCAL_RETENTION_DAYS} days...\u0026#34; find \u0026#34;${BACKUP_DIR}\u0026#34; -type f -name \u0026#34;*.tar.gz\u0026#34; -mtime +${LOCAL_RETENTION_DAYS} -delete echo \u0026#34;Local cleanup complete.\u0026#34; # 4. Clean up cloud backups echo \u0026#34;Cleaning up cloud backups older than ${CLOUD_RETENTION_DAYS} days...\u0026#34; rclone delete \u0026#34;${RCLONE_REMOTE}:${RCLONE_DEST}\u0026#34; --min-age ${CLOUD_RETENTION_DAYS}d echo \u0026#34;Cloud cleanup complete.\u0026#34; echo \u0026#34;Backup process finished.\u0026#34; send_notification \u0026#34;🎉 Homelab backup and cloud upload completed successfully!\u0026#34; Step 3: Automate with Cron To run this script automatically, you must add it to the root user\u0026rsquo;s crontab. This is critical for giving the script permission to read all Docker files.\nOpen the root crontab editor: sudo crontab -e Add the following line to schedule the backup for 3:00 AM every morning: 0 3 * * * /path/to/your/backup.sh You will now get a fresh, onsite and off-site backup every night and a Discord message when it\u0026rsquo;s done. Chapter 2: Automated Updates with Watchtower (at 6 AM) Manually updating every Docker container is tedious. We can automate this by deploying Watchtower.\nStep 1: The Docker Compose File Create a docker-compose.yml for Watchtower. This configuration schedules it to run once a day at 6:00 AM, clean up old images, and send a Discord notification only if it finds an update.\nmkdir -p ~/docker/watchtower\ncd ~/docker/watchtower\nnano docker-compose.yml\nPaste in this configuration:\nservices: watchtower: image: containrrr/watchtower container_name: watchtower restart: unless-stopped volumes: - /var/run/docker.sock:/var/run/docker.sock environment: # Timezone setting TZ: America/Chicago # Discord notification settings WATCHTOWER_NOTIFICATIONS: shoutrrr WATCHTOWER_NOTIFICATION_URL: \u0026#34;discord://YOUR_DISCORD_WEBHOOK_ID_URL\u0026gt; # Notification settings WATCHTOWER_NOTIFICATIONS_LEVEL: info WATCHTOWER_NOTIFICATION_REPORT: \u0026#34;true\u0026#34; WATCHTOWER_NOTIFICATIONS_HOSTNAME: Homelab-Laptop # Update settings WATCHTOWER_CLEANUP: \u0026#34;true\u0026#34; WATCHTOWER_INCLUDE_STOPPED: \u0026#34;false\u0026#34; WATCHTOWER_INCLUDE_RESTARTING: \u0026#34;true\u0026#34; WATCHTOWER_SCHEDULE: \u0026#34;0 0 6 * * *\u0026#34; Note: The WATCHTOWER_NOTIFICATION_URL uses a special shoutrrr format for Discord, which looks like discord://token@webhook-id.\nNow, every morning at 6:00 AM, Watchtower will scan all running containers and update any that have a new image available.\nChapter 3: Proactive Alerting (24/7) The final piece of automation is proactive alerting. This setup ensures you are immediately notified via Discord if something goes wrong.\nStep 1: The Alerting Pipeline The pipeline we\u0026rsquo;ll build is: Prometheus (detects problems) -\u0026gt; Alertmanager (groups and routes alerts) -\u0026gt; Discord (notifies you).\nStep 2: Deploy Alertmanager First, deploy Alertmanager. It must be on the same npm_default network as Prometheus.\nmkdir -p ~/docker/alertmanager\ncd ~/docker/alertmanager\nCreate the alertmanager.yml configuration file:\nnano alertmanager.yml Paste in this configuration. It uses advanced routing to send critical alerts every 2 hours and warning alerts every 12 hours.\nglobal: resolve_timeout: 5m route: group_by: [\u0026#34;alertname\u0026#34;, \u0026#34;severity\u0026#34;] group_wait: 30s group_interval: 10m repeat_interbal: 12h receiver: \u0026#34;discord-notifications\u0026#34; routes: - receiver: \u0026#34;discord-notifications\u0026#34; matchers: - severity=\u0026#34;critical\u0026#34; repeat_interval: 2h - receiver: \u0026#34;discord-notifications\u0026#34; matchers: - severity=\u0026#34;warning\u0026#34; repeat_interval: 12h receivers: - name: \u0026#34;discord-notifications\u0026#34; discord_configs: - webhook_url: \u0026#34;YOUR_DISCORD_WEBHOOK_URL\u0026#34; send_resolved: true Now create the docker-compose.yml for Alertmanager:\nnano docker-compose.yml Paste in the following:\nservices: alertmanager: image: prom/alertmanager:latest container_name: alertmanager restart: unless-stopped volumes: - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml networks: - npm_default networks: npm_default: external: true Launch it: docker compose up -d\nStep 3: Configure Prometheus Finally, tell Prometheus to send alerts to Alertmanager and load your rules.\nCreate your rules file, ~/docker/monitoring/alert_rules.yml, with rules for \u0026ldquo;Instance Down,\u0026rdquo; \u0026ldquo;High CPU,\u0026rdquo; \u0026ldquo;Low Disk Space,\u0026rdquo; etc.\ncd ~/docker/monitoring nano alert_rules.yml Add the alert_rules.yml as a volume in your ~/docker/monitoring/docker-compose.yml.\nvolumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - ./alert_rules.yml:/etc/prometheus/alert_rules.yml - prometheus_data:/prometheus Add the alerting and rule_files blocks to your ~/docker/monitoring/prometheus.yml:\ngroups: -name: Critical System Alerts interval: 30s rules: - alert: InstanceDown expr: up == 0 for: 2m labels: severity: critical annotations: summary: \u0026#34;🔴 Instance {{ $labels.instance }} is DOWN\u0026#34; description: \u0026#34;Service {{ $labels.job }} has been unreachable for 2 minutes.\u0026#34; - alert: LaptopOnBattery expr: node_power_supply_online == 0 for: 5m labels: severity: critical annotations: summary: \u0026#34;🔋 Server running on BATTERY\u0026#34; description: \u0026#34;Homelab has been unplugged for 5 minutes. Check power connection!\u0026#34; - alert: LowBatteryLevel expr: node_power_supply_capacity \u0026lt; 20 and node_power_supply_online == 0 for: 1m labels: severity: critical annotations: summary: \u0026#34;⚠️ CRITICAL: Battery at {{ $value }}%\u0026#34; description: \u0026#34;Battery below 20%. Server may shut down soon!\u0026#34; - alert: DiskAlmostFull expr: (node_filesystem_avail_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;} / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;}) * 100 \u0026lt; 10 for: 5m labels: severity: critical annotations: summary: \u0026#34;💾 Disk space critically low: {{ $value | humanize }}% remaining\u0026#34; description: \u0026#34;Root filesystem has less than 10% free space.\u0026#34; - alert: OutOfMemory expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 \u0026lt; 5 for: 2m labels: severity: critical annotations: summary: \u0026#34;🧠 Memory critically low: {{ $value | humanize }}% available\u0026#34; description: \u0026#34;Less than 5% memory available. System may become unresponsive.\u0026#34; - alert: CriticalCpuTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;coretemp\u0026#34;} \u0026gt; 95 for: 2m labels: severity: critical annotations: summary: \u0026#34;🔥 CRITICAL CPU Temperature: {{ $value }}°C\u0026#34; description: \u0026#34;CPU temperature exceeds 95°C. Thermal throttling or shutdown imminent!\u0026#34; - name: Warning System Alerts interval: 1m rules: - alert: HighCpuUsage expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[5m])) * 100) \u0026gt; 80 for: 5m labels: severity: warning annotations: summary: \u0026#34;⚡ High CPU usage: {{ $value | humanize }}%\u0026#34; description: \u0026#34;CPU usage above 80% for 5 minutes on {{ $labels.instance }}\u0026#34; - alert: HighSystemLoad expr: node_load5 / on(instance) count(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}) by (instance) \u0026gt; 1.5 for: 10m labels: severity: warning annotations: summary: \u0026#34;📊 High system load: {{ $value | humanize }}\u0026#34; description: \u0026#34;5-minute load average is 1.5x CPU cores for 10 minutes.\u0026#34; - alert: HighMemoryUsage expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 \u0026lt; 20 for: 5m labels: severity: warning annotations: summary: \u0026#34;🧠 High memory usage: {{ $value | humanize }}% available\u0026#34; description: \u0026#34;Less than 20% memory available.\u0026#34; - alert: HighCpuTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;coretemp\u0026#34;} \u0026gt; 85 for: 5m labels: severity: warning annotations: summary: \u0026#34;🌡️ High CPU temperature: {{ $value }}°C\u0026#34; description: \u0026#34;CPU temperature above 85°C. Consider improving cooling.\u0026#34; - alert: HighNvmeTemperature expr: node_hwmon_temp_celsius{chip=\u0026#34;nvme\u0026#34;} \u0026gt; 65 for: 10m labels: severity: warning annotations: summary: \u0026#34;💿 High NVMe temperature: {{ $value }}°C\u0026#34; description: \u0026#34;NVMe drive temperature above 65°C for 10 minutes.\u0026#34; - alert: DiskSpaceLow expr: (node_filesystem_avail_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;} / node_filesystem_size_bytes{mountpoint=\u0026#34;/\u0026#34;,fstype!=\u0026#34;tmpfs\u0026#34;}) * 100 \u0026lt; 20 for: 10m labels: severity: warning annotations: summary: \u0026#34;💾 Disk space low: {{ $value | humanize }}% remaining\u0026#34; description: \u0026#34;Root filesystem has less than 20% free space.\u0026#34; - alert: HighSwapUsage expr: ((node_memory_SwapTotal_bytes - node_memory_SwapFree_bytes) / node_memory_SwapTotal_bytes * 100) \u0026gt; 50 for: 10m labels: severity: warning annotations: summary: \u0026#34;💱 High swap usage: {{ $value | humanize }}%\u0026#34; description: \u0026#34;Swap usage above 50%. System may be memory-constrained.\u0026#34; # Monitor your USB-C hub ethernet adapter (enx00) - alert: EthernetInterfaceDown expr: node_network_up{device=\u0026#34;enx00\u0026#34;} == 0 for: 2m labels: severity: warning annotations: summary: \u0026#34;🌐 USB-C Ethernet adapter is DISCONNECTED\u0026#34; description: \u0026#34;Your USB-C hub ethernet connection (enx00) is down. Check cable or hub.\u0026#34; - alert: HighNetworkErrors expr: rate(node_network_receive_errs_total{device=\u0026#34;enx00\u0026#34;}[5m]) \u0026gt; 10 or rate(node_network_transmit_errs_total{device=\u0026#34;enx00\u0026#34;}[5m]) \u0026gt; 10 for: 5m labels: severity: warning annotations: summary: \u0026#34;🌐 High network errors on USB-C ethernet\u0026#34; description: \u0026#34;Your ethernet adapter is experiencing high error rate. Check cable quality.\u0026#34; - name: Docker Container Alerts interval: 1m rules: # Simplified alert - just checks if container exporter is working - alert: ContainerMonitoringDown expr: absent(container_last_seen) for: 2m labels: severity: warning annotations: summary: \u0026#34;🐳 Container monitoring is down\u0026#34; description: \u0026#34;cAdvisor or container metrics are not available. Check if containers are being monitored.\u0026#34; - alert: ContainerRestarting expr: rate(container_start_time_seconds[5m]) \u0026gt; 0.01 for: 2m labels: severity: warning annotations: summary: \u0026#34;🐳 Container {{ $labels.name }} is restarting\u0026#34; description: \u0026#34;Container {{ $labels.name }} has restarted recently.\u0026#34; - alert: ContainerHighCpu expr: rate(container_cpu_usage_seconds_total{name!~\u0026#34;.*POD.*\u0026#34;,name!=\u0026#34;\u0026#34;}[5m]) * 100 \u0026gt; 80 for: 10m labels: severity: warning annotations: summary: \u0026#34;🐳 Container {{ $labels.name }} high CPU: {{ $value | humanize }}%\u0026#34; description: \u0026#34;Container CPU usage above 80% for 10 minutes.\u0026#34; Restart Prometheus to apply the changes:\ncd ~/docker/monitoring docker compose up -d --force-recreate prometheus Now, if any service fails or your server\u0026rsquo;s resources run low, you will get an instant notification in Discord.\nStep 3: The Critical Firewall Fix You may find your alerts are not sending. This is often due to a conflict between Docker and ufw.\nOpen the main ufw configuration file:\nsudo nano /etc/default/ufw Change DEFAULT_FORWARD_POLICY=\u0026quot;DROP\u0026quot; to DEFAULT_FORWARD_POLICY=\u0026quot;ACCEPT\u0026quot;.\nReload the firewall:\nsudo ufw reload Restart your containers that need internet access:\ndocker compose restart Now, if any service fails or your server\u0026rsquo;s resources run low, you will get an instant notification in Discord.\nConclusion Our homelab has now truly come to life. It\u0026rsquo;s no longer just a collection of services but a resilient, self-maintaining platform. With automated backups to Google Drive, daily updates via Watchtower, and proactive alerts with Prometheus and Alertmanager, our server can now run 24/7 with minimal manual intervention. We\u0026rsquo;ve built a solid, reliable, and intelligent system.\nBut there\u0026rsquo;s one critical piece still missing: end-to-end security for our local services.\nRight now, we\u0026rsquo;re accessing our dashboards at addresses like http://grafana.local, which browsers flag as \u0026ldquo;Not Secure.\u0026rdquo; What if we could use a real, public domain name for our internal services and get a valid HTTPS certificate, all without opening a single port on our router?\nIn the next part of this series, I\u0026rsquo;ll show you exactly how to do that. We\u0026rsquo;ll dive into an advanced but powerful setup using Cloudflare and Nginx Proxy Manager to bring trusted, zero-exposure SSL to everything we\u0026rsquo;ve built.\nStay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-4-automation-and-alerting/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to the part 4 of the homelab series! In the previous parts, we built a server, deployed a suite of services, and configured our network. Now, it\u0026rsquo;s time to make it resilient and self-maintaining. A homelab isn\u0026rsquo;t just about setting things up; it\u0026rsquo;s about keeping them running reliably.\u003c/p\u003e\n\u003cp\u003eThis guide will show you how to set up the three pillars of modern IT operations: \u003cstrong\u003eAutomated Backups\u003c/strong\u003e, \u003cstrong\u003eAutomated Updates\u003c/strong\u003e, and \u003cstrong\u003eProactive Alerting\u003c/strong\u003e. By the end, you\u0026rsquo;ll have a homelab that runs itself, ensures your data is safe, stays up-to-date, and notifies you when something goes wrong.\u003c/p\u003e","title":"Part 4: Automating a Homelab with Backups, Updates, and Alerts"},{"content":"My New Weekend Project: Building a Personal Ad-Blocking Server in the Cloud! Hey everyone, Prajwol here.\nLike a lot of you, I spend a good chunk of my day online. And lately, it\u0026rsquo;s felt like I\u0026rsquo;m in a constant battle with pop-ups, trackers, and auto-playing video ads. I\u0026rsquo;ve used browser extensions for years, but I wanted a more powerful solution—something that would protect my entire home network, including my phone and smart TV, without needing to install software everywhere.\nSo, I decided to take on a new project: building my very own ad-blocking DNS server in the cloud.\nI\u0026rsquo;d done something similar a while back with Linode, but this time I wanted to dive into the world of Amazon Web Services (AWS) and see if I could build a reliable, secure, and cost-effective setup from scratch. It turned into quite the adventure, involving a late-night session of launching a virtual server, wrestling with firewalls, and securing my own private domain with an SSL certificate.\nThe end result? It\u0026rsquo;s been fantastic. My web pages load noticeably faster, and the general online experience feels so much cleaner and less intrusive. Plus, knowing that I have full control over my own corner of the internet is incredibly satisfying. It\u0026rsquo;s a great feeling to see the query logs fill up with blocked requests for domains I\u0026rsquo;ve never even heard of!\nI documented every single step of my journey, from the first click in the AWS console to the final configuration on my home router. If you\u0026rsquo;re curious about how to build one for yourself, I\u0026rsquo;ve written up a complete, step-by-step guide.\nYou can check out the full project guide here! It was a challenging but really rewarding project. Let me know what you think!\nPublished: Friday, August 22, 2025\n","permalink":"http://localhost:1313/blog/adguard-aws/","summary":"\u003ch1 id=\"my-new-weekend-project-building-a-personal-ad-blocking-server-in-the-cloud\"\u003eMy New Weekend Project: Building a Personal Ad-Blocking Server in the Cloud!\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eLike a lot of you, I spend a good chunk of my day online. And lately, it\u0026rsquo;s felt like I\u0026rsquo;m in a constant battle with pop-ups, trackers, and auto-playing video ads. I\u0026rsquo;ve used browser extensions for years, but I wanted a more powerful solution—something that would protect my entire home network, including my phone and smart TV, without needing to install software everywhere.\u003c/p\u003e","title":"Building a Personal Ad-Blocking Server in the Cloud!"},{"content":"Introduction Welcome to Part 3 of my homelab series! In the previous parts, I built my server and deployed a suite of management and monitoring tools. Now, it\u0026rsquo;s time to build the brain of my network: a robust, redundant, and high-availability DNS system using AdGuard Home that works both at home and on the go.\nIn this detailed guide, I\u0026rsquo;ll walk you through how I deployed a total of three AdGuard Home instances, each with its own unique IP address. I set up a primary resolver on my homelab, a secondary failover resolver in the cloud for my mobile devices, and a tertiary resolver on a separate virtual network for local redundancy.\nChapter 1: The Local Workhorse (Primary DNS) I started by deploying my main, day-to-day DNS resolver on my homelab server.\nStep 1: Deploying AdGuard Home with Docker Compose First, I SSHed into my server, created a directory for the project, and a docker-compose.yml file to define the service.\nmkdir -p ~/docker/adguard-primary cd ~/docker/adguard-primary nano docker-compose.yml I pasted in the following configuration. This runs the AdGuard Home container, maps all the necessary ports for DNS and the web UI, and connects it to the shared npm_default network I set up in Part 2.\nservices: adguardhome: image: adguard/adguardhome:latest container_name: adguard-primary restart: unless-stopped ports: - \u0026#34;53:53/tcp\u0026#34; - \u0026#34;53:53/udp\u0026#34; - \u0026#34;8080:80/tcp\u0026#34; # Web UI - \u0026#34;853:853/tcp\u0026#34; # DNS-over-TLS volumes: - ./workdir:/opt/adguardhome/work - ./confdir:/opt/adguardhome/conf networks: - npm_default networks: npm_default: external: true I then launched the container by running:\ndocker compose up -d Step 2: Initial AdGuard Home Setup Wizard I navigated to http://\u0026lt;your-server-ip\u0026gt;:3000 in my web browser to start the setup wizard.\nI clicked \u0026ldquo;Get Started.\u0026rdquo;\nOn the \u0026ldquo;Admin Web Interface\u0026rdquo; screen, I changed the \u0026ldquo;Listen Interface\u0026rdquo; to All interfaces and the port to 80.\nOn the \u0026ldquo;DNS server\u0026rdquo; screen, I changed the \u0026ldquo;Listen Interface\u0026rdquo; to All interfaces and left the port as 53.\nI followed the prompts to create my admin username and password.\nOnce the setup was complete, I was redirected to my main dashboard, now available at http://\u0026lt;your-server-ip\u0026gt;:8080.\nStep 3: Configure My Home Router To make all my devices use AdGuard automatically, I logged into my home router\u0026rsquo;s admin panel, found the DHCP Server settings, and changed the Primary DNS Server to my homelab\u0026rsquo;s static IP address (e.g., 192.168.1.10).\nChapter 2: The Cloud Failover (Secondary DNS on Oracle Cloud) An off-site DNS server ensures I have ad-blocking on my mobile devices and acts as a backup.\nWhy I Chose Oracle Cloud After testing the free tiers of both AWS and Linode, I chose Oracle Cloud Infrastructure (OCI). In my experience, OCI\u0026rsquo;s \u0026ldquo;Always Free\u0026rdquo; tier is far more generous with its resources. It provides powerful Ampere A1 Compute instances with up to 4 CPU cores and 24 GB of RAM, plus 200 GB of storage and significant bandwidth, all for free. This was ideal for running my service 24/7 without the strict limitations or eventual costs associated with other providers.\nStep 1: Launching the Oracle Cloud VM Sign Up: I created my account on the Oracle Cloud website.\nCreate VM Instance: In the OCI console, I navigated to Compute \u0026gt; Instances and clicked \u0026ldquo;Create instance\u0026rdquo;.\nConfigure Instance:\nName: I gave it a name like AdGuard-Cloud.\nImage and Shape: I clicked \u0026ldquo;Edit\u0026rdquo;. For the image, I selected Ubuntu. For the shape, I selected \u0026ldquo;Ampere\u0026rdquo; and chose the VM.Standard.A1.Flex shape (it\u0026rsquo;s \u0026ldquo;Always Free-eligible\u0026rdquo;).\nNetworking: I used the default VCN and made sure \u0026ldquo;Assign a public IPv4 address\u0026rdquo; was checked.\nSSH Keys: I added my SSH public key.\nI clicked Create. Once the instance was running, I took note of its Public IP Address.\nStep 2: Configuring the Cloud Firewall For maximum security, I locked down the administrative ports to only my home IP address.\nFind My Public IP: I went to a site like whatismyip.com and copied my home\u0026rsquo;s public IP address.\nEdit Security List: I navigated to my instance\u0026rsquo;s details page, clicked the subnet link, then clicked the \u0026ldquo;Security List\u0026rdquo; link.\nI clicked \u0026ldquo;Add Ingress Rules\u0026rdquo; and added the following rules:\nFor SSH (Port 22): I set the Source to my home\u0026rsquo;s public IP, followed by /32 (e.g., 203.0.113.55/32). This is a critical security step.\nFor AdGuard Setup (Port 3000): I also set the Source to my home\u0026rsquo;s public IP with /32.\nFor AdGuard Web UI (Port 80/443): I set the Source to my home\u0026rsquo;s public IP with /32 as well.\nFor Public DNS (Port 53, 853, etc.): I set the Source to 0.0.0.0/0 (Anywhere) to allow all my devices to connect from any network.\nStep 3: Installing AdGuard Home \u0026amp; Configuring SSL Connect via SSH: I used the public IP and my SSH key to connect to the VM.\nRun Install Script: I chose to install AdGuard Home directly on the OS for this instance.\ncurl -s -S -L [https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh](https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh) | sh -s -- -v Get a Hostname: I went to No-IP.com, created a free hostname (e.g., my-cloud-dns.ddns.net), and pointed it to my cloud VM\u0026rsquo;s public IP.\nEnable Encryption: In my cloud AdGuard\u0026rsquo;s dashboard (Settings \u0026gt; Encryption settings), I enabled encryption, entered my No-IP hostname, and used the built-in function to request a Let\u0026rsquo;s Encrypt certificate.\nStep 4: Creating a Cloud Backup (Snapshot) A critical final step for any cloud service is creating a backup. Here is how I did it in OCI:\nIn the OCI Console, I navigated to the details page for my AdGuard-Cloud instance.\nUnder the \u0026ldquo;Resources\u0026rdquo; menu on the left, I clicked on \u0026ldquo;Boot volume\u0026rdquo;.\nOn the Boot Volume details page, under \u0026ldquo;Resources,\u0026rdquo; I clicked \u0026ldquo;Boot volume backups\u0026rdquo;.\nI clicked the \u0026ldquo;Create boot volume backup\u0026rdquo; button.\nI gave the backup a descriptive name (e.g., AdGuard-Cloud-Backup-YYYY-MM-DD) and clicked the create button. This creates a full snapshot of my server that I can use to restore it in minutes.\nStep 5: ### How to Use Your Cloud DNS on Mobile Devices The main benefit of the cloud server is having ad-blocking on the go. Here’s how I set it up on my mobile phone using secure, encrypted DNS.\nFor Android (Version 9+): Modern Android has a built-in feature called \u0026ldquo;Private DNS\u0026rdquo; that uses DNS-over-TLS (DoT), which is perfect for this.\nOpen Settings on your Android device.\nTap on \u0026ldquo;Network \u0026amp; internet\u0026rdquo; (this may be called \u0026ldquo;Connections\u0026rdquo; on some devices).\nFind and tap on \u0026ldquo;Private DNS\u0026rdquo;. You may need to look under an \u0026ldquo;Advanced\u0026rdquo; section.\nSelect the option labeled \u0026ldquo;Private DNS provider hostname\u0026rdquo;.\nIn the text box, enter the No-IP hostname you created for your Oracle Cloud server (e.g., my-cloud-dns.ddns.net).\nTap Save.\nYour phone will now send all its DNS queries through an encrypted tunnel to your personal AdGuard Home server in the cloud, giving you ad-blocking on both Wi-Fi and cellular data.\nFor iOS (iPhone/iPad): On iOS, the easiest way to set up encrypted DNS is by installing a configuration profile.\nOn your iPhone or iPad, open Safari.\nGo to a DNS profile generator site, like the one provided by AdGuard.\nWhen prompted, enter the DNS-over-HTTPS (DoH) address for your cloud server. It will be your No-IP hostname with /dns-query at the end (e.g., https://my-cloud-dns.ddns.net/dns-query).\nDownload the generated configuration profile.\nGo to your device\u0026rsquo;s Settings app. You will see a new \u0026ldquo;Profile Downloaded\u0026rdquo; item near the top. Tap on it.\nFollow the on-screen prompts to Install the profile. You may need to enter your device passcode.\nOnce installed, your iOS device will also route its DNS traffic through your secure cloud server.\nChapter 3: Ultimate Local Redundancy (Tertiary DNS with Macvlan) For an extra layer of redundancy within my homelab, I created a third AdGuard instance. By using an advanced Docker network mode called macvlan, this container gets its own unique IP address on my home network, making it a truly independent resolver.\nCreate Macvlan Network: First, I created the macvlan network, telling it which of my physical network cards to use (eth0 in my case).\ndocker network create -d macvlan \\ --subnet=192.168.1.0/24 \\ --gateway=192.168.1.1 \\ -o parent=eth0 homelab_net Deploy Tertiary Instance: I created a new folder (~/docker/adguard-tertiary) and this docker-compose.yml. Notice there are no ports since the container gets its own IP.\nservices: adguardhome2: image: adguard/adguardhome:latest container_name: adguardhome2 volumes: - \u0026#34;./work:/opt/adguardhome/work\u0026#34; - \u0026#34;./conf:/opt/adguardhome/conf\u0026#34; networks: homelab_net: ipv4_address: 192.168.1.11 # The new, unique IP for this container restart: unless-stopped networks: homelab_net: external: true Configure Router for Local Failover: To complete the local redundancy, I went back into my router\u0026rsquo;s DHCP settings.\nIn the Primary DNS field, I have the IP of my main homelab server (e.g., 192.168.1.10).\nIn the Secondary DNS field, I entered the unique IP address I assigned to my macvlan container (e.g., 192.168.1.11).\nNow, if my primary AdGuard container has an issue, all devices on my network will automatically fail over to the tertiary instance.\nChapter 4: Fine-Tuning and Integration Finally, I implemented some best practices on my primary AdGuard Home instance.\nUpstream DNS Servers: Under Settings \u0026gt; DNS Settings, I configured AdGuard to send requests to multiple resolvers in parallel for speed and reliability, using Cloudflare (1.1.1.1), Google (8.8.8.8), and Quad9 (9.9.9.9).\nEnable DNSSEC: In the same settings page, I enabled DNSSEC to verify the integrity of DNS responses.\nDNS Blocklists: I added several popular lists from the \u0026ldquo;Filters \u0026gt; DNS blocklists\u0026rdquo; page, including the AdGuard DNS filter and the OISD Blocklist, for robust protection.\nDNS Rewrites for Local Services: This is the key to a clean homelab experience. For each service, I performed a detailed two-step process:\nCreate the Proxy Host in Nginx Proxy Manager: I logged into my NPM admin panel, went to Hosts \u0026gt; Proxy Hosts, and clicked \u0026ldquo;Add Proxy Host\u0026rdquo;. For my Homer dashboard, I set the Forward Hostname to homer (the container name) and the Forward Port to 8080 (its internal port), using homer.local as the domain name.\nCreate the DNS Rewrite in AdGuard Home: I logged into my primary AdGuard dashboard, went to Filters \u0026gt; DNS Rewrites, and clicked \u0026ldquo;Add DNS rewrite\u0026rdquo;. I entered homer.local as the domain and the IP address of my Nginx Proxy Manager server as the answer.\nConclusion I\u0026rsquo;ve now built an incredibly robust, multi-layered DNS infrastructure. My home devices use the primary local server, which is backed up by a second, independent local server, and my mobile devices use a completely separate cloud instance for on-the-go protection. This provides a resilient, secure, and ad-free internet experience.\nIn the final part of this series, we\u0026rsquo;ll shift our focus from deploying services to maintaining them. I\u0026rsquo;ll show you how I set up a fully automated operations pipeline for my homelab, including daily off-site backups, automatic container updates with Watchtower, and proactive alerting with Prometheus. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-3-high-availability-dns/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to Part 3 of my homelab series! In the previous parts, I built my server and deployed a suite of management and monitoring tools. Now, it\u0026rsquo;s time to build the brain of my network: a robust, redundant, and high-availability DNS system using \u003cstrong\u003eAdGuard Home\u003c/strong\u003e that works both at home and on the go.\u003c/p\u003e\n\u003cp\u003eIn this detailed guide, I\u0026rsquo;ll walk you through how I deployed a total of \u003cstrong\u003ethree\u003c/strong\u003e AdGuard Home instances, each with its own unique IP address. I set up a primary resolver on my homelab, a secondary failover resolver in the cloud for my mobile devices, and a tertiary resolver on a separate virtual network for local redundancy.\u003c/p\u003e","title":"Part 3: A High-Availability DNS Network with AdGuard Home"},{"content":"New Project Alert: Running a Powerful AI Locally with Docker! Hey everyone, Prajwol here.\nI\u0026rsquo;ve always been fascinated by the incredible advancements in AI and large language models. While cloud-based models are powerful, I was really curious about what it would take to run a high-performance model right on my own machine. This gives you ultimate privacy, control, and the ability to experiment without limits.\nSo, for my latest project, I decided to dive in and get the DeepSeek-R1 model, a powerful AI, up and running locally using Docker.\nDocker is an amazing tool that lets you package up applications and all their dependencies into a neat little box called a container. This means you can run complex software without the headache of complicated installations or conflicts with other programs on your system. It was the perfect way to tame this powerful AI and get it running smoothly on my Ubuntu machine.\nThe process was a fantastic learning experience, covering everything from setting up Docker to pulling the model and interacting with the AI. It’s amazing to have that kind of power running on your own hardware.\nI’ve documented my entire process in a detailed, step-by-step guide. If you’re interested in local AI and want to see how you can run a powerful model yourself, be sure to check it out!\nYou can find the full project guide right here! Let me know what you think of this one!\nPublished: February, 2025\n","permalink":"http://localhost:1313/blog/running-deepseek-r1-on-docker-container-on-ubuntu/","summary":"\u003ch1 id=\"new-project-alert-running-a-powerful-ai-locally-with-docker\"\u003eNew Project Alert: Running a Powerful AI Locally with Docker!\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve always been fascinated by the incredible advancements in AI and large language models. While cloud-based models are powerful, I was really curious about what it would take to run a high-performance model right on my own machine. This gives you ultimate privacy, control, and the ability to experiment without limits.\u003c/p\u003e\n\u003cp\u003eSo, for my latest project, I decided to dive in and get the \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e model, a powerful AI, up and running locally using \u003cstrong\u003eDocker\u003c/strong\u003e.\u003c/p\u003e","title":"Running a Powerful AI Locally with Docker!"},{"content":"Introduction Welcome to Part 2 of my homelab series! In Part 1, we built a solid foundation by turning an old laptop into a hardened Debian server with Docker. Now that our server is running, we need to deploy services to manage, monitor, and easily access our projects.\nIn this guide, we\u0026rsquo;ll deploy three essential stacks. First, Nginx Proxy Manager (NPM) will act as our server\u0026rsquo;s front door and create a shared network for our containers. Second, we\u0026rsquo;ll set up a professional-grade monitoring stack with Prometheus and Grafana. Finally, we\u0026rsquo;ll deploy a Homer dashboard to create a beautiful and convenient launchpad for all our services.\n1. The Management Layer: Nginx Proxy Manager (NPM) 🌐 Before we can deploy our other services, we need a way to manage connections between them. NPM will act as our reverse proxy and, crucially, will create the shared Docker network that all our other services will connect to.\nA. Deploy Nginx Proxy Manager First, let\u0026rsquo;s create a directory and the docker-compose.yml file for NPM.\n# Create the directory mkdir -p ~/docker/npm cd ~/docker/npm # Create the docker-compose.yml nano docker-compose.yml Paste in the following configuration. This file defines the NPM service and creates a network named npm_default.\nservices: app: image: \u0026#39;jc21/nginx-proxy-manager:latest\u0026#39; container_name: npm-app-1 restart: unless-stopped ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;81:81\u0026#39; volumes: - ./data:/data - ./letsencrypt:/etc/letsencrypt networks: default: name: npm_default Launch it with\ndocker compose up -d You can now log in to the admin UI at http://\u0026lt;your-server-ip\u0026gt;:81.\n2. The Monitoring Stack 📊 With our shared network in place, we can now deploy our monitoring stack.\nPrometheus: Collects all the metrics.\nNode Exporter: Exposes the server\u0026rsquo;s hardware metrics.\ncAdvisor: Exposes Docker container metrics.\nGrafana: Visualizes all the data in beautiful dashboards.\nA. Create the Prometheus Configuration Prometheus needs a config file to know what to monitor.\n# Create the project directory mkdir -p ~/docker/monitoring cd ~/docker/monitoring # Create the prometheus.yml file nano prometheus.yml Paste in the following configuration:\nglobal: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;node-exporter\u0026#39; static_configs: - targets: [\u0026#39;node-exporter:9100\u0026#39;] - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] B. Deploy the Stack with Docker Compose Next, create the docker-compose.yml file in the same ~/docker/monitoring directory.\nnano docker-compose.yml This file defines all four monitoring services and tells them to connect to the npm_default network we created earlier.\nservices: prometheus: image: prom/prometheus:latest container_name: prometheus restart: unless-stopped ports: - \u0026#34;9090:9090\u0026#34; volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - prometheus_data:/prometheus networks: - default grafana: image: grafana/grafana:latest container_name: grafana restart: unless-stopped ports: - \u0026#34;3001:3000\u0026#34; volumes: - grafana_data:/var/lib/grafana networks: - default node-exporter: image: prom/node-exporter:latest container_name: node-exporter restart: unless-stopped volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - \u0026#39;--path.procfs=/host/proc\u0026#39; - \u0026#39;--path.sysfs=/host/sys\u0026#39; - \u0026#39;--path.rootfs=/rootfs\u0026#39; - \u0026#39;--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)\u0026#39; networks: - default cadvisor: image: gcr.io/cadvisor/cadvisor:latest container_name: cadvisor restart: unless-stopped ports: - \u0026#34;8081:8080\u0026#34; volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro networks: - default volumes: prometheus_data: grafana_data: networks: default: name: npm_default external: true Now, launch the stack:\ndocker compose up -d C. Configure Grafana Log in to Grafana at http://\u0026lt;your-server-ip\u0026gt;:3001 (default: admin/admin).\nAdd Data Source: Go to Connections \u0026gt; Data Sources, add a Prometheus source, and set the URL to http://prometheus:9090.\nImport Dashboards: Go to Dashboards \u0026gt; New \u0026gt; Import and add these dashboards by ID:\nNode Exporter Full (ID: 1860)\nDocker Host/Container Metrics (ID: 193)\n3. The Homer Launchpad Dashboard 🚀 Finally, let\u0026rsquo;s deploy Homer as our beautiful start page with custom icons.\nCreate Directories \u0026amp; Download Icons: First, create a directory for Homer and an assets subdirectory. Then, cd into the assets folder and download the icons.\nmkdir -p ~/docker/homer/assets cd ~/docker/homer/assets wget -O grafana.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/grafana.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/grafana.png) wget -O prometheus.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/prometheus.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/prometheus.png) wget -O cadvisor.png [https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/cadvisor.png](https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/png/cadvisor.png) wget -O npm.png [https://nginxproxymanager.com/icon.png](https://nginxproxymanager.com/icon.png) Create Configuration: Go back to the main homer directory and create the config.yml file.\ncd ~/docker/homer nano config.yml Paste in the following configuration. The logo: lines point to the icons we just downloaded.\n--- title: \u0026#34;Homelab Dashboard\u0026#34; subtitle: \u0026#34;Server Management\u0026#34; theme: \u0026#34;dark\u0026#34; services: - name: \u0026#34;Management\u0026#34; icon: \u0026#34;fas fa-server\u0026#34; items: - name: \u0026#34;Nginx Proxy Manager\u0026#34; logo: \u0026#34;assets/tools/npm.png\u0026#34; subtitle: \u0026#34;Reverse Proxy Admin\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:81\u0026#34; - name: \u0026#34;Monitoring\u0026#34; icon: \u0026#34;fas fa-chart-bar\u0026#34; items: - name: \u0026#34;Grafana\u0026#34; logo: \u0026#34;assets/tools/grafana.png\u0026#34; subtitle: \u0026#34;Metrics Dashboard\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:3001\u0026#34; - name: \u0026#34;Prometheus\u0026#34; logo: \u0026#34;assets/tools/prometheus.png\u0026#34; subtitle: \u0026#34;Metrics Database\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:9090\u0026#34; - name: \u0026#34;cAdvisor\u0026#34; logo: \u0026#34;assets/tools/cadvisor.png\u0026#34; subtitle: \u0026#34;Container Metrics\u0026#34; url: \u0026#34;http://\u0026lt;your-server-ip\u0026gt;:8081\u0026#34; Create Docker Compose File: Finally, create the docker-compose.yml file.\nnano docker-compose.yml This configuration connects Homer to our shared network.\nservices: homer: image: b4bz/homer container_name: homer volumes: - ./config.yml:/www/assets/config.yml - ./assets:/www/assets/tools ports: - \u0026#34;8090:8080\u0026#34; restart: unless-stopped networks: - npm_default networks: npm_default: external: true Launch: Run docker compose up -d. You can now access your new dashboard with custom icons at http://\u0026lt;your-server-ip\u0026gt;:8090.\nConclusion Our homelab now has a powerful management and monitoring foundation. Nginx Proxy Manager is ready to direct traffic, Grafana is visualizing our server\u0026rsquo;s health, and Homer provides a central launchpad.\nIn the next part of the series, we\u0026rsquo;ll deploy our core network service, AdGuard Home, and use NPM to create clean, memorable local domains for all the applications we set up today. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-2-monitoring-and-management/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to Part 2 of my homelab series! In \u003ca href=\"/projects/homelab-series-part-1-debian-docker-foundation/\"\u003ePart 1\u003c/a\u003e, we built a solid foundation by turning an old laptop into a hardened Debian server with Docker. Now that our server is running, we need to deploy services to manage, monitor, and easily access our projects.\u003c/p\u003e\n\u003cp\u003eIn this guide, we\u0026rsquo;ll deploy three essential stacks. First, \u003cstrong\u003eNginx Proxy Manager (NPM)\u003c/strong\u003e will act as our server\u0026rsquo;s front door and create a shared network for our containers. Second, we\u0026rsquo;ll set up a professional-grade monitoring stack with \u003cstrong\u003ePrometheus\u003c/strong\u003e and \u003cstrong\u003eGrafana\u003c/strong\u003e. Finally, we\u0026rsquo;ll deploy a \u003cstrong\u003eHomer\u003c/strong\u003e dashboard to create a beautiful and convenient launchpad for all our services.\u003c/p\u003e","title":"Part 2: Homelab Management \u0026 Monitoring"},{"content":"My First Cloud Ad-Blocker: A Look Back at AdGuard Home on Linode Hey everyone, Prajwol here.\nAs I continue to explore different cloud projects, I often think back to the ones that had the biggest impact on my day-to-day life. One of the very first projects that truly changed my internet experience was setting up my own ad-blocking DNS server using AdGuard Home on a Linode instance.\nMy goal was to find a simple, cost-effective way to block ads and trackers across my entire home network. I wanted a \u0026ldquo;set it and forget it\u0026rdquo; solution that would cover every device—from my phone to my smart TV—without needing to install an app on each one. Linode (now Akamai) was the perfect platform for this: straightforward, powerful, and great for hosting a lightweight service like AdGuard Home.\nThe process of spinning up a small server, running a single installation script, and then seeing the query logs light up with blocked requests was incredibly satisfying. It felt like I had taken back a real measure of control over my own network.\nThis project remains a fantastic entry point for anyone wanting to get started with self-hosting and network privacy. I\u0026rsquo;ve kept the original, detailed guide for anyone who wants to follow along.\nYou can find the full step-by-step project guide here! It’s a rewarding project that delivers tangible results almost immediately. Let me know if you give it a try!\nPublished: Friday, August 22, 2025\n","permalink":"http://localhost:1313/blog/adguard-home-on-cloud/","summary":"\u003ch1 id=\"my-first-cloud-ad-blocker-a-look-back-at-adguard-home-on-linode\"\u003eMy First Cloud Ad-Blocker: A Look Back at AdGuard Home on Linode\u003c/h1\u003e\n\u003cp\u003eHey everyone, Prajwol here.\u003c/p\u003e\n\u003cp\u003eAs I continue to explore different cloud projects, I often think back to the ones that had the biggest impact on my day-to-day life. One of the very first projects that truly changed my internet experience was setting up my own ad-blocking DNS server using AdGuard Home on a Linode instance.\u003c/p\u003e\n\u003cp\u003eMy goal was to find a simple, cost-effective way to block ads and trackers across my entire home network. I wanted a \u0026ldquo;set it and forget it\u0026rdquo; solution that would cover every device—from my phone to my smart TV—without needing to install an app on each one. Linode (now Akamai) was the perfect platform for this: straightforward, powerful, and great for hosting a lightweight service like AdGuard Home.\u003c/p\u003e","title":"My First Cloud Ad-Blocker - A Look Back at AdGuard Home on Linode"},{"content":"Introduction Welcome to the first post in my new homelab series! I\u0026rsquo;ve always been fascinated by self-hosting and DevOps, and I believe the best way to learn is by doing. In this series, I\u0026rsquo;ll document my journey of turning an old, unused laptop into a powerful, efficient, and secure bare-metal server for hosting a variety of network services.\nThe goal for this first part is to lay a solid foundation. We\u0026rsquo;ll take an old laptop, install a minimal and stable Linux operating system, perform some initial security hardening, and set up Docker as our containerization engine. By the end of this post, we\u0026rsquo;ll have a perfect blank canvas ready for the exciting services we\u0026rsquo;ll deploy in the upcoming parts.\n1. Choosing the Hardware \u0026amp; OS Why an Old Laptop? Before diving in, why use an old laptop instead of a Raspberry Pi or a dedicated server? For a starter homelab, a laptop has three huge advantages:\nCost-Effective: It\u0026rsquo;s free if you have one lying around! Built-in UPS: The battery acts as a built-in Uninterruptible Power Supply (UPS), keeping the server running through short power outages. Low Power Consumption: Laptop hardware is designed to be power-efficient, which is great for a device that will be running 24/7. Why Debian 13 \u0026ldquo;Trixie\u0026rdquo;? For the operating system, I chose Debian. It\u0026rsquo;s renowned for its stability, security, and massive package repository. It’s the bedrock of many other distributions (like Ubuntu) and is perfect for a server because it\u0026rsquo;s lightweight and doesn\u0026rsquo;t include unnecessary software. We\u0026rsquo;ll be using the minimal \u0026ldquo;net-install\u0026rdquo; to ensure we only install what we absolutely need.\n2. Installation and Network Configuration The installation process is straightforward, but the network setup is key to a reliable server.\nMinimal Installation Create a Bootable USB: I downloaded the Debian 13 \u0026ldquo;netinst\u0026rdquo; ISO from the official website and used Rufus on Windows to create a bootable USB drive. Boot from USB: I plugged the USB into the laptop and booted from it (usually pressing F12, F2, or Esc during startup to select the USB device). Language, Location, and Keyboard: Selected English, United States, and the default keyboard layout. Network Setup: Connected the laptop to my home network (Ethernet preferred for stability). Hostname \u0026amp; Domain: Entered a short, memorable hostname for the server (e.g., homelab) and left the domain blank. User Accounts: Set a root password. Created a non-root regular user (this will be used for daily management). Partition Disks: Chose Guided – use entire disk with separate /home partition. This is simpler for a server setup. Software Selection: At the “Software selection” screen: Unchecked “Debian desktop environment” Checked “SSH server” and “standard system utilities” This ensures a clean command-line system that can be accessed remotely. GRUB Bootloader: Installed GRUB on the primary drive (so the system boots correctly). Finish Installation: Removed the USB drive when prompted and rebooted into the fresh Debian install. Setting a Static IP A server needs a permanent, unchanging IP address. The best way to do this is with DHCP Reservation on your router. This tells your router to always assign the same IP address to your server\u0026rsquo;s unique MAC address.\nFirst, find your laptop’s current IP address and network interface name by running:\nip a You’ll see output similar to:\n2: enp3s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\rinet 192.168.0.45/24 brd 192.168.0.255 scope global dynamic enp3s0\rvalid_lft 86396sec preferred_lft 86396sec In this example:\nInterface name: enp3s0 Current IP: 192.168.0.45 MAC address: shown under link/ether in the same section. With this info, log into your router’s admin panel, find the \u0026ldquo;DHCP Reservation\u0026rdquo; or \u0026ldquo;Static Leases\u0026rdquo; section, and assign a memorable IP address (e.g., 192.168.0.45) to your server’s MAC address.\nThis ensures the server always gets the same IP from your router, making it easy to find on your network.\nConnecting Remotely with SSH With a static IP set, all future management will be done remotely using an SSH client. For Windows, I highly recommend Solar-PuTTY. I created a new session, entered the server\u0026rsquo;s static IP address, my username, and password, and connected.\n3. Initial Server Hardening With a remote SSH session active, the first thing to do is secure the server and configure it for its headless role.\nUpdate the System First, let\u0026rsquo;s make sure all packages are up to date.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y Configure the Firewall ufw (Uncomplicated Firewall) is perfect for a simple setup. We\u0026rsquo;ll set it to deny all incoming traffic by default and only allow SSH connections.\n# Install UFW sudo apt install ufw -y # Allow SSH connections sudo ufw allow ssh # Enable the firewall sudo ufw enable Configure Lid-Close Action To ensure the laptop keeps running when the lid is closed, we edit the logind.conf file.\nsudo nano /etc/systemd/logind.conf Uncomment the line:\nHandleLidSwitch=ignore Save the file, then restart the service:\nsudo systemctl restart systemd-logind.service 4. Installing the Containerization Engine: Docker Instead of installing applications directly on our host, we\u0026rsquo;ll use Docker to keep the system clean and make management easier.\nInstall Docker Engine The official convenience script is the easiest way to get the latest version.\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Add User to Docker Group To run docker commands without sudo, add your user to the docker group. The $USER variable automatically uses the currently logged-in user.\nsudo usermod -aG docker $USER After this, log out and log back in for the change to take effect.\nInstall Docker Compose Docker Compose is essential for managing multi-container applications with a simple YAML file.\nsudo apt install docker-compose-plugin -y To verify the installation:\ndocker compose version Conclusion And that\u0026rsquo;s it for Part 1! We\u0026rsquo;ve successfully turned an old piece of hardware into a hardened, modern server running Debian and Docker with a reliable network configuration. We have a solid and secure foundation to build upon.\nIn the next part of the series, we\u0026rsquo;ll deploy our first critical service: a local, network-wide ad-blocking DNS resolver using AdGuard Home. Stay tuned!\n","permalink":"http://localhost:1313/projects/homelab-series-part-1-debian-docker-foundation/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eWelcome to the first post in my new homelab series! I\u0026rsquo;ve always been fascinated by self-hosting and DevOps, and I believe the best way to learn is by doing. In this series, I\u0026rsquo;ll document my journey of turning an old, unused laptop into a powerful, efficient, and secure bare-metal server for hosting a variety of network services.\u003c/p\u003e\n\u003cp\u003eThe goal for this first part is to lay a solid foundation. We\u0026rsquo;ll take an old laptop, install a minimal and stable Linux operating system, perform some initial security hardening, and set up Docker as our containerization engine. By the end of this post, we\u0026rsquo;ll have a perfect blank canvas ready for the exciting services we\u0026rsquo;ll deploy in the upcoming parts.\u003c/p\u003e","title":"Part 1: Reviving an Old Laptop with Debian \u0026 Docker"},{"content":"Your Personal Internet Guardian: How to Build a FREE Ad-Blocker in the Cloud! 🚀 Hey everyone! A while back, I wrote a guide on setting up AdGuard Home on Linode. The world of tech moves fast, and it\u0026rsquo;s time for an upgrade! Today, we\u0026rsquo;re going to build our own powerful, network-wide ad-blocker using Amazon Web Services (AWS), and we\u0026rsquo;ll make it secure with our own domain and SSL certificate.\nThink of this as building a digital gatekeeper for your internet. Before any ads, trackers, or malicious sites can reach your devices, our AdGuard Home server will slam the door shut. The best part? This works on your phone, laptop, smart TV—anything on your network—without installing a single app on them.\nThis guide is for everyone, from seasoned tech wizards to curious beginners. We\u0026rsquo;ll break down every step in simple terms, so grab a coffee, and let\u0026rsquo;s build something awesome!\n## Chapter 1: Building Our Home in the AWS Cloud ☁️ First, we need a server. We\u0026rsquo;ll use an Amazon EC2 instance, which is just a fancy name for a virtual computer that you rent.\nSign Up for AWS: If you don\u0026rsquo;t have an account, head to the AWS website and sign up. You\u0026rsquo;ll need a credit card for verification, but for this guide, we can often stay within the Free Tier.\nLaunch Your EC2 Instance:\nLog in to your AWS Console and search for EC2. Click \u0026ldquo;Launch instance\u0026rdquo;. Name: Give your server a cool name, like AdGuard-Server. Application and OS Images: In the search bar, type Debian and select the latest version (e.g., Debian 12). Make sure it\u0026rsquo;s marked \u0026ldquo;Free tier eligible\u0026rdquo;. Instance Type: Choose t2.micro. This is your free, trusty little server. Key Pair (for login): This is your digital key to the server\u0026rsquo;s front door. Click \u0026ldquo;Create a new key pair\u0026rdquo;, name it something like my-adguard-key, and download the .pem file. Keep this file secret and safe! Network settings (The Firewall): This is crucial. We need to tell our server which doors to open. Click \u0026ldquo;Edit\u0026rdquo;. Check the box for \u0026ldquo;Allow SSH traffic from\u0026rdquo; and select My IP. This lets you securely log in. Check \u0026ldquo;Allow HTTPS traffic from the internet\u0026rdquo; and \u0026ldquo;Allow HTTP traffic from the internet\u0026rdquo;. We\u0026rsquo;ll need these for our secure dashboard later. Launch It! Hit the \u0026ldquo;Launch instance\u0026rdquo; button and watch as your new cloud server comes to life.\nGive Your Server a Permanent Address (Elastic IP):\nBy default, your server\u0026rsquo;s public IP address will change every time it reboots. Let\u0026rsquo;s make it permanent! In the EC2 menu on the left, go to \u0026ldquo;Elastic IPs\u0026rdquo;. Click \u0026ldquo;Allocate Elastic IP address\u0026rdquo; and then \u0026ldquo;Allocate\u0026rdquo;. Select the new IP address from the list, click \u0026ldquo;Actions\u0026rdquo;, and then \u0026ldquo;Associate Elastic IP address\u0026rdquo;. Choose your AdGuard-Server instance from the list and click \u0026ldquo;Associate\u0026rdquo;. Your server now has a static IP address that will never change! Make a note of this new IP. ## Chapter 2: Opening the Doors (Configuring the Firewall) 🚪 Our server is running, but for maximum security, we want to ensure only you can access the administrative parts of it. We\u0026rsquo;ll open the public DNS ports to everyone, but lock down the management ports to your home IP address.\nFind Your Public IP Address: Open a new browser tab and go to a site like WhatIsMyIP.com. It will display your home\u0026rsquo;s public IP address. Copy this IP address (it will look something like 203.0.113.55).\nEdit the Firewall Rules: Go to your EC2 Instance details, click the \u0026ldquo;Security\u0026rdquo; tab, and click on the Security Group name.\nClick \u0026ldquo;Edit inbound rules\u0026rdquo; and \u0026ldquo;Add rule\u0026rdquo; for each of the following. This makes sure your DNS is publicly available but the setup panel is locked down to your IP only.\nRule for AdGuard Setup (Port 3000):\nType: Custom TCP Port range: 3000 Source: Paste your IP address here, and add /32 to the end (e.g., 203.0.113.55/32). The /32 tells AWS it\u0026rsquo;s a single, specific IP address. Rule for DNS (Port 53):\nType: Custom UDP and Custom TCP (you will add two separate rules for this port) Port range: 53 Source: Anywhere-IPv4 Rule for DNS-over-TLS (Port 853):\nType: Custom TCP Port range: 853 Source: Anywhere-IPv4 Click \u0026ldquo;Save rules\u0026rdquo;. Your firewall is now configured to allow public DNS requests while keeping your management panel secure.\n## Chapter 3: Installing AdGuard Home 🛡️ Now, let\u0026rsquo;s connect to our server and install the magic software.\nConnect via SSH: Open a terminal (PowerShell on Windows, Terminal on Mac/Linux) and use the key you downloaded to connect. Use your new Elastic IP address! # Replace the path and Elastic IP with your own ssh -i \u0026#34;path/to/my-adguard-key.pem\u0026#34; admin@YOUR_ELASTIC_IP Install AdGuard Home: Run this one simple command. It downloads and installs everything for you. curl -s -S -L [https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh](https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh) | sh -s -- -v Run the Setup Wizard: The script will give you a link, like http://YOUR_ELASTIC_IP:3000. Open this in your browser. Follow the on-screen steps to create your admin username and password. ## Chapter 4: Teaching Your Guardian Who to Trust and What to Block With AdGuard Home installed, the next step is to configure its core brain: the DNS servers it gets its answers from and the blocklists it uses to protect your network.\n1. Setting Up Upstream DNS Servers Think of \u0026ldquo;Upstream DNS Servers\u0026rdquo; as the giant, public phonebooks of the internet. When your AdGuard server doesn\u0026rsquo;t know an address (and it\u0026rsquo;s not on a blocklist), it asks one of these upstreams. It\u0026rsquo;s recommended to use a mix of the best encrypted DNS providers for security, privacy, and speed.\nIn the AdGuard dashboard, go to Settings -\u0026gt; DNS settings. In the \u0026ldquo;Upstream DNS servers\u0026rdquo; box, enter the following, one per line:\nhttps://dns.quad9.net/dns-query https://dns.google/dns-query https://dns.cloudflare.com/dns-query Quad9: Focuses heavily on security, blocking malicious domains. Google: Known for being very fast. Cloudflare: A great all-around choice with a strong focus on privacy. 2. Optimizing DNS Performance Still in the DNS settings page, scroll down to optimize how your server queries the upstreams.\nParallel requests: Select this option. This is the fastest and most resilient mode. It sends your DNS query to all three of your upstream servers at the same time and uses the answer from the very first one that responds. This ensures you always get the quickest possible result.\nEnable EDNS client subnet (ECS): Check this box. This is very important for services like Netflix, YouTube, and other content delivery networks (CDNs). It helps them give you content from a server that is geographically closest to you, resulting in faster speeds and a better experience.\n3. Enabling DNSSEC Right below the upstream servers, there\u0026rsquo;s a checkbox for \u0026ldquo;Enable DNSSEC\u0026rdquo;. You should check this box. DNSSEC is like a digital wax seal on a letter; it verifies that the DNS answers you\u0026rsquo;re getting are authentic and haven\u0026rsquo;t been tampered with. It\u0026rsquo;s a simple, one-click security boost.\n4. Choosing Your Blocklists This is the fun part—the actual ad-blocking! Go to Filters -\u0026gt; DNS blocklists. For a \u0026ldquo;Balanced \u0026amp; Powerful\u0026rdquo; setup that blocks aggressively without a high risk of breaking websites, enable the following lists:\nAdGuard DNS filter: A great, well-maintained baseline. OISD Blocklist Big: Widely considered one of the best all-in-one lists for blocking ads, trackers, and malware. HaGeZi\u0026rsquo;s Pro Blocklist: A fantastic list that adds another layer of aggressive blocking for privacy. HaGeZi\u0026rsquo;s Threat Intelligence Feed: A crucial security-only list that focuses on protecting against active threats like phishing and malware. This combination will give you robust protection against both annoyances and real dangers.\n## Chapter 5: Giving Your Server a Name (Free Domain with No-IP) 📛 An IP address is hard to remember. Let\u0026rsquo;s get a free, memorable name for our server.\nSign Up at No-IP: Go to No-IP.com, create a free account, and create a hostname (e.g., my-dns.ddns.net). Point it to Your Server: When creating the hostname, enter your server\u0026rsquo;s permanent Elastic IP address. Confirm your account via email. ## Chapter 6: Making It Secure with SSL/TLS 🔐 We\u0026rsquo;ll use Let\u0026rsquo;s Encrypt and Certbot to get a free SSL certificate, which lets us use secure https:// and encrypted DNS.\nInstall Certbot: In your SSH session, run these commands:\nsudo apt update sudo apt install certbot -y Get the Certificate: Run this command, replacing the email and domain with your own.\n# This command will temporarily stop any service on port 80, get the certificate, and then finish. sudo certbot certonly --standalone --agree-tos --email YOUR_EMAIL@example.com -d your-no-ip-hostname.ddns.net If it\u0026rsquo;s successful, it will tell you where your certificate files are saved (usually in /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/).\nConfigure AdGuard Home Encryption:\nGo to your AdGuard Home dashboard (Settings -\u0026gt; Encryption settings). Check \u0026ldquo;Enable encryption\u0026rdquo;. In the \u0026ldquo;Server name\u0026rdquo; field, enter your No-IP hostname. Under \u0026ldquo;Certificates\u0026rdquo;, choose \u0026ldquo;Set a certificates file path\u0026rdquo;. Certificate path: /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/fullchain.pem Private key path: /etc/letsencrypt/live/your-no-ip-hostname.ddns.net/privkey.pem Click \u0026ldquo;Save configuration\u0026rdquo;. The page will reload on a secure https:// connection! ## Chapter 7: Automating SSL Renewal (Cron Job Magic) ✨ Let\u0026rsquo;s Encrypt certificates last for 90 days. We can tell our server to automatically renew them.\nOpen the Cron Editor: In SSH, run sudo crontab -e and choose nano as your editor. Add the Renewal Job: Add this line to the bottom of the file. It tells the server to try renewing the certificate every day at 2:30 AM. 30 2 * * * systemctl stop AdGuardHome.service \u0026amp;\u0026amp; certbot renew --quiet \u0026amp;\u0026amp; systemctl start AdGuardHome.service Save and exit (Ctrl+X, then Y, then Enter). Your server will now keep its certificate fresh forever! ## Chapter 8: Testing Your New Superpowers (DoH \u0026amp; DoT) 🧪 For a direct confirmation, I used these commands on my computer:\nDNS-over-HTTPS (DoH) Test: This test checks if the secure web endpoint for DNS is alive.\ncurl -v [https://your-no-ip-hostname.ddns.net/dns-query](https://your-no-ip-hostname.ddns.net/dns-query) I got a \u0026ldquo;405 Method Not Allowed\u0026rdquo; error, which sounds bad but is actually great news. It means I successfully connected to the server, which correctly told me I didn\u0026rsquo;t send a real query. The connection works!\nDNS-over-TLS (DoT) Test: This checks the dedicated secure port for DNS. I used a tool called kdig.\n# I had to install it first with: sudo apt install knot-dnsutils kdig @your-no-ip-hostname.ddns.net +tls-ca +tls-host=your-no-ip-hostname.ddns.net example.com The command returned a perfect DNS answer for example.com, confirming the secure tunnel was working.\n## Chapter 9: Protecting Your Kingdom (Router \u0026amp; Phone Setup) 🏰 Now, let\u0026rsquo;s point your devices to their new guardian.\nOn Your Home Router: Log in to your router\u0026rsquo;s admin page, find the DNS settings, and enter your server\u0026rsquo;s Elastic IP as the primary DNS server. Leave the secondary field blank! This forces all devices on your Wi-Fi to be protected. Then, restart your router. On Your Mobile Phone: Android: Go to Settings -\u0026gt; Network -\u0026gt; Private DNS. Choose \u0026ldquo;Private DNS provider hostname\u0026rdquo; and enter your No-IP hostname (my-dns.ddns.net). This gives you ad-blocking everywhere, even on cellular data! iOS: You can use a profile to configure DoH. A simple way is to use a site like AdGuard\u0026rsquo;s DNS profile generator, but enter your own server\u0026rsquo;s DoH address (https://my-dns.ddns.net/dns-query). ## Chapter 10: The Ultimate Safety Net (Creating a Snapshot) 📸 Finally, let\u0026rsquo;s back up our perfect setup.\nIn the EC2 Console, go to your instance details. Click the \u0026ldquo;Storage\u0026rdquo; tab and click the \u0026ldquo;Volume ID\u0026rdquo;. Click \u0026ldquo;Actions\u0026rdquo; -\u0026gt; \u0026ldquo;Create snapshot\u0026rdquo;. Give it a description, like AdGuard-Working-Setup-Backup. If you ever mess something up, you can use this snapshot to restore your server to this exact working state in minutes.\n## Bonus Chapter: Common Troubleshooting Tips If things aren\u0026rsquo;t working, here are a few common pitfalls to check:\nBrowser Overrides Everything: If one device isn\u0026rsquo;t blocking ads, check its browser settings! Modern browsers like Chrome have a \u0026ldquo;Secure DNS\u0026rdquo; feature that can bypass your custom setup. You may need to turn this off. Check Your Laptop\u0026rsquo;s DNS: Make sure your computer\u0026rsquo;s network settings are set to \u0026ldquo;Obtain DNS automatically\u0026rdquo; so it listens to the router. A manually set DNS on your PC will ignore the router\u0026rsquo;s settings. Beware of IPv6: If you run into trouble on one device, try disabling IPv6 in that device\u0026rsquo;s Wi-Fi adapter properties to force it to use your working IPv4 setup. ## It’s a Wrap! And there you have it! You\u0026rsquo;ve successfully built a personal, secure, ad-blocking DNS server in the cloud. You\u0026rsquo;ve learned about cloud computing, firewalls, DNS, SSL, and automation. Go enjoy a faster, cleaner, and more private internet experience.\n","permalink":"http://localhost:1313/projects/adguard-updated/","summary":"\u003ch1 id=\"your-personal-internet-guardian-how-to-build-a-free-ad-blocker-in-the-cloud-\"\u003eYour Personal Internet Guardian: How to Build a FREE Ad-Blocker in the Cloud! 🚀\u003c/h1\u003e\n\u003cp\u003eHey everyone! A while back, I wrote a guide on setting up AdGuard Home on Linode. The world of tech moves fast, and it\u0026rsquo;s time for an upgrade! Today, we\u0026rsquo;re going to build our own powerful, network-wide ad-blocker using \u003cstrong\u003eAmazon Web Services (AWS)\u003c/strong\u003e, and we\u0026rsquo;ll make it secure with our own domain and SSL certificate.\u003c/p\u003e\n\u003cp\u003eThink of this as building a digital gatekeeper for your internet. Before any ads, trackers, or malicious sites can reach your devices, our AdGuard Home server will slam the door shut. The best part? This works on your phone, laptop, smart TV—anything on your network—without installing a single app on them.\u003c/p\u003e","title":"How I Built My Own Ad-Blocking DNS Server in the Cloud (2025 Updated Edition!)"},{"content":"What\u0026rsquo;s the buzz about AdGuard Home? Imagine AdGuard Home as your personal internet guardian. This versatile tool blocks ads, trackers, and other online nuisances across all devices connected to your network. Whether you\u0026rsquo;re browsing on your phone, tablet, or computer, AdGuard Home has your back.\nIn today\u0026rsquo;s digital landscape, robust security measures are paramount. Protecting each device shields your family from accidental clicks and malicious attacks, ensuring peace of mind and a secure online environment.\nWhy on the Cloud? While setting up AdGuard Home on your home network is great, installing it on a cloud server like Linode takes things up a notch. Here\u0026rsquo;s why:\nOn-the-Go Protection: Your devices stay protected from ads and trackers, no matter where you are, you can even share it with your family. Centralized Control: Manage and customize your ad-blocking settings from a single dashboard. Enhanced Privacy: Keep your browsing data away from prying eyes. Ready to embark on this ad-free adventure? Let\u0026rsquo;s get started!\nSetting Up The Environment Step 1: Create a Linode Cloud Account Why choose Linode? Through NetworkChuck\u0026rsquo;s referral link, you receive a generous $100 cloud credit - a fantastic start!\nSign Up: Navigate to Linode\u0026rsquo;s signup page and register. Access the Dashboard: Log in and select \u0026lsquo;Linodes\u0026rsquo; from the left-side menu. Create a Linode: Click \u0026lsquo;Create Linode,\u0026rsquo; choose your preferred region, and select an operating system (Debian 11 is a solid choice). Choose a Plan: The Shared 1GB Nanode instance is sufficient for AdGuard Home. Label and Secure: Assign a label to your Linode and set a strong root password. Deploy: Click \u0026lsquo;Create Linode\u0026rsquo; and wait for it to initialize. Once your Linode is up and running, access it via the LISH Console or SSH. (use root as localhost login)\nStep 2: Installing AdGuard Home on Linode Yes, we\u0026rsquo;re already into setting up at this point.\nLog In: Access your Linode using SSH or the LISH Console with your root credentials. Update the system: sudo apt update \u0026amp;\u0026amp; apt upgrade -y Go ahead and copy this command to Install Adguard Home: curl -s -S -L https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh | sh -s -- -v AdGuard Home is installed and running. You can use CTRL+Shift+V to paste into the terminal.\nStep 3: Configure AdGuard Home Post-installation, you\u0026rsquo;ll see a list of IP addresses with port :3000. Access the Web Interface: Open your browser and navigate to the IP address followed by :3000. If you encounter a security warning, proceed by clicking \u0026ldquo;Continue to site.\u0026rdquo; Initial Setup: Click \u0026lsquo;Get Started\u0026rsquo; and follow the prompts. When uncertain, default settings are typically fine. Set Credentials: Set up the Username and Password. Step 4: Integrate AdGuard Home with Your Router After this, your AdGuard Home is running, but in order to use it on your devices you need to set up inside your home router for all your devices to be protected. For that, I can\u0026rsquo;t walk you through each and every router\u0026rsquo;s settings, but the steps are pretty similar.\nFind your router IP address, you should be able to find it on the back of your router (commonly 192.168.0.1 or 192.169.1.1) enter it into your browser. Login into your router using the credentials mentioned in the back of your router; the default is often admin for both username and password. I suggest you change your default password. Configure DNS Settings: Enable DHCP Server: Ensure your router\u0026rsquo;s DHCP server is active. Set DNS Addresses: Input your AdGuard Home server\u0026rsquo;s IP as the primary DNS (mine was 96.126.113.207). For secondary DNS, options like 1.1.1.1 (Cloudflare), 9.9.9.9 (Quad9), or 8.8.8.8 (Google) are reliable. Save and apply the changes. Fine-Tuning AdGuard Home If you\u0026rsquo;ve done everything till here you should be good, but for those who enjoy customizations, AdGuard Home offers a plethora of settings. Some of the customization I did are:\nSettings Go to Settings -\u0026gt; General Settings: You can enable Parental Control and Safe Search. You can also make your Statistics last longer than 24hrs which is default. Now on Settings -\u0026gt; DNS Settings By default, it uses DNS from quad9 which is pretty good but I suggest you add more. You can click on the list of known DNS providers, which you can choose from. I used: https://dns.quad9.net/dns-query https://dns.google/dns-query https://dns.cloudflare.com/dns-query Enable \u0026lsquo;Load Balancing\u0026rsquo; to distribute queries evenly. Scroll down to \u0026lsquo;DNS server configuration\u0026rsquo; and enable DNSSEC for enhanced security. Click on Save. Filters DNS blocklists Go to Filters -\u0026gt; DNS blocklists, here you can add a blocklist that people have created and use it to block even more things. By default, AdGuard uses the AdGuard DNS filter, and you can add more.\nClick on Add blocklist -\u0026gt; Choose from the list Don\u0026rsquo;t choose too many from the list cause it may slow your internet requests. These are the blocklists I added. And just like that you are blocking more and more things. DNS rewrites Go to Filters -\u0026gt; DNS rewrites, here you can add your own DNS entries, so I added AdGuard here.\nClick on Add DNS rewrite Type in domain adguardforme.local and your IP address for AdGuard Home. And save it. Now, when I want to go on the AdGuard Home dashboard I just type in adguardforme.local and I\u0026rsquo;m into AdGuard, I don\u0026rsquo;t have to remember the IP address.\nCustom filtering rules Go to Filters -\u0026gt; Custom filtering rules. For some reason when I use Facebook on mobile device stories and videos did not load up, so I added custom filtering rules.\n@@||graph.facebook.com^$important ","permalink":"http://localhost:1313/projects/adguard-home-on-cloud/","summary":"\u003ch1 id=\"whats-the-buzz-about-adguard-home\"\u003eWhat\u0026rsquo;s the buzz about AdGuard Home?\u003c/h1\u003e\n\u003cp\u003eImagine AdGuard Home as your personal internet guardian. This versatile tool blocks ads, trackers, and other online nuisances across all devices connected to your network. Whether you\u0026rsquo;re browsing on your phone, tablet, or computer, AdGuard Home has your back.\u003c/p\u003e\n\u003cp\u003eIn today\u0026rsquo;s digital landscape, robust security measures are paramount. Protecting each device shields your family from accidental clicks and malicious attacks, ensuring peace of mind and a secure online environment.\u003c/p\u003e","title":"Running Private Adguard Server on Cloud (Linode)"},{"content":"What\u0026rsquo;s a Docker Container? Before we dive into setting up DeepSeek-R1, let me explain what a Docker container is. Imagine you have a toy that works perfectly on your birthday but gets broken if you move it to another room. A Docker container is like a magic box that keeps your AI model (the toy) in perfect condition wherever you take it, whether it\u0026rsquo;s running as a background task, on a web server, or even in the cloud.\nDocker containers encapsulate everything required to run an application: the code, dependencies, and environment settings. This ensures consistency across different machines, which is super important for AI models that rely on precise configurations.\nSetting Up The Environment Step 1: Install Ubuntu on Windows (If You Haven\u0026rsquo;t Already) If you\u0026rsquo;re using Windows, the easiest way to get an Ubuntu environment is through the Microsoft Store. Here\u0026rsquo;s how:\nOpen the Microsoft Store and search for Ubuntu. Click Get and let it install. Once installed, open Ubuntu from the Start menu and follow the setup instructions. Update the system: sudo apt update \u0026amp;\u0026amp; sudo apt upgrade Now, you have an Ubuntu terminal running on Windows!\nStep 2: Install Docker (If You Haven\u0026rsquo;t Already) First, let\u0026rsquo;s check if you have Docker installed. Open a terminal and run:\ndocker --version If that returns a version number, congrats! If not, install Docker:\nsudo apt update \u0026amp;\u0026amp; sudo apt install docker.io -y sudo systemctl enable --now docker Step 3: Prerequisites for NVIDIA GPU Install NVIDIA Container Toolkit:\nConfiguring the production repository: curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list Update the package list: sudo apt-get update Install the NVIDIA Container Toolkit: sudo apt-get install -y nvidia-container-toolkit Running Ollama Inside Docker Run these commands(P.S. shoutout to NetworkChuck):\ndocker run -d \\ --gpus all \\ -v ollama:/root/.ollama \\ -p 11434:11434 \\ --security-opt=no-new-privileges \\ --cap-drop=ALL \\ --cap-add=SYS_NICE \\ --memory=8g \\ --memory-swap=8g \\ --cpus=4 \\ --read-only \\ --name ollama \\ ollama/ollama Running DeepSeek-R1 Locally Time to bring DeepSeek-R1 to life locally and containerized:\ndocker exec -it ollama ollama run deepseek-r1 or you can run other versions of deepseek-r1 just by typing in the version at the end after a colon(:)\ndocker exec -it ollama ollama run deepseek-r1:7b After this, play around with the AI, if you wanna exit just type:\n/bye Starting Deepseek-R1 To Start Deepseek-R1 from next time go to Ubuntu and type:\ndocker start ollama this will start ollama docker container; then type:\ndocker exec -it ollama ollama run deepseek-r1:7b ","permalink":"http://localhost:1313/projects/running-deepseek-r1-on-docker-container-on-ubuntu/","summary":"\u003ch1 id=\"whats-a-docker-container\"\u003eWhat\u0026rsquo;s a Docker Container?\u003c/h1\u003e\n\u003cp\u003eBefore we dive into setting up DeepSeek-R1, let me explain what a Docker container is. Imagine you have a toy that works perfectly on your birthday but gets broken if you move it to another room. A Docker container is like a magic box that keeps your AI model (the toy) in perfect condition wherever you take it, whether it\u0026rsquo;s running as a background task, on a web server, or even in the cloud.\u003c/p\u003e","title":"Dive into AI Fun: Running DeepSeek-R1 on a Docker Container on Ubuntu"},{"content":"Description I joined AbbVie initially as a contractor and quickly demonstrated the skills and dedication that led to my conversion to a full-time position. In my role, I stepped into a high-stakes production environment where precision and operational stability are paramount. My work centered on the optimization and maintenance of sophisticated, machine learning-based visual inspection systems. I was responsible for fine-tuning these models, analyzing their performance data, and troubleshooting complex technical issues across both hardware and software, including the POM and PCE systems.\nThis wasn\u0026rsquo;t just about keeping machines running; it was about enhancing them. By applying a systematic, data-driven approach, I contributed to a 30% reduction in product waste, a metric that translates directly to improved efficiency and sustainability. Working within strict GMPs and utilizing systems like SAP for material tracking, I learned to balance technical problem-solving with rigorous compliance, ensuring that every action contributed to the stability and reliability of mission-critical operations.\n","permalink":"http://localhost:1313/experience/abbvie/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eI joined AbbVie initially as a contractor and quickly demonstrated the skills and dedication that led to my conversion to a full-time position. In my role, I stepped into a high-stakes production environment where precision and operational stability are paramount. My work centered on the optimization and maintenance of sophisticated, machine learning-based visual inspection systems. I was responsible for fine-tuning these models, analyzing their performance data, and troubleshooting complex technical issues across both hardware and software, including the POM and PCE systems.\u003c/p\u003e","title":"Operator III"},{"content":"Description As my first professional role after moving to the United States, my position at FedEx was a crucial step in adapting my technical skills to a new corporate environment. My journey began as a contractor, where my performance and analytical skills in a fast-paced setting led to my transition to a full-time Associate role. At the device testing center, I was on the front lines of quality assurance for a wide array of consumer electronics. I conducted comprehensive, systematic testing on mobile devices, smartwatches, and routers, executing detailed test plans to identify hardware vulnerabilities, software bugs, and non-compliance with network standards.\nMy responsibilities included meticulously documenting my findings, reproducing bugs to assist developers, and providing clear, actionable reports to engineering teams. This collaborative process was crucial in accelerating the repair cycle and ensuring that products met the highest standards of quality and security before reaching the market. The role sharpened my analytical skills and gave me a deep appreciation for the importance of rigorous testing in the software development lifecycle.\n","permalink":"http://localhost:1313/experience/fedex/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs my first professional role after moving to the United States, my position at FedEx was a crucial step in adapting my technical skills to a new corporate environment. My journey began as a contractor, where my performance and analytical skills in a fast-paced setting led to my transition to a full-time Associate role. At the device testing center, I was on the front lines of quality assurance for a wide array of consumer electronics. I conducted comprehensive, systematic testing on mobile devices, smartwatches, and routers, executing detailed test plans to identify hardware vulnerabilities, software bugs, and non-compliance with network standards.\u003c/p\u003e","title":"Product Testing Associate"},{"content":"Description As the IT Support Specialist for a bustling international college with over 2,500 students and staff, I was at the heart of the campus\u0026rsquo;s technical operations. My role was dynamic and comprehensive, involving end-to-end technical support across a diverse, multi-building campus. I managed the entire user lifecycle, from onboarding new accounts to ensuring smooth system setups across Windows, Linux, and Mac environments. I was the primary point of contact for all technical challenges, resolving Tier 1 and 2 support tickets with a 90% SLA adherence and troubleshooting complex OS issues to minimize downtime.\nMy tenure was marked by significant growth and adaptation. I led the complete technical setup of eight new computer labs, managing everything from hardware deployment and network cabling to software installation and configuration. When the COVID-19 pandemic hit, I was instrumental in transitioning the campus to a hybrid learning model, my first professional experience navigating such a large-scale shift. This required rapidly scaling our remote support capabilities and ensuring both students and faculty could operate effectively from anywhere.\nA cornerstone project of my time was the complete technical overhaul of the newly acquired Kumari Film Hall. I was deeply involved in the project to transform the old cinema into modern lecture halls, which included designing and deploying the entire network infrastructure, setting up AV systems, and ensuring seamless integration with the main campus network.\nTo support these expanding operations, I took the lead in deploying a new UVDesk help desk ticketing system on a CentOS server and embraced automation, utilizing tools like OK Goldy to streamline user creation in Google Workspace. These initiatives standardized processes, improved efficiency, and allowed our team to successfully manage the college\u0026rsquo;s ambitious growth.\n","permalink":"http://localhost:1313/experience/islingtoncollege/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eAs the IT Support Specialist for a bustling international college with over 2,500 students and staff, I was at the heart of the campus\u0026rsquo;s technical operations. My role was dynamic and comprehensive, involving end-to-end technical support across a diverse, multi-building campus. I managed the entire user lifecycle, from onboarding new accounts to ensuring smooth system setups across Windows, Linux, and Mac environments. I was the primary point of contact for all technical challenges, resolving Tier 1 and 2 support tickets with a 90% SLA adherence and troubleshooting complex OS issues to minimize downtime.\u003c/p\u003e","title":"IT Support Specialist"},{"content":"Description Building on my foundational experience, my internship at BlackBox Technologies immersed me in a more complex, project-based environment. I was an integral part of a development team tasked with building a web-based attendance system from the ground up. This role provided me with invaluable hands-on, full-stack experience. I contributed to the backend by assisting senior engineers with the development of business logic in .NET, giving me insight into server-side architecture. Simultaneously, I was responsible for building responsive, user-facing components for the front-end using HTML, CSS, and JavaScript.\nThis experience was a deep dive into the software development lifecycle. I learned how to translate business requirements into technical specifications, participated in code reviews, and understood the synergy between front-end and back-end systems. Working in close collaboration with the engineering team on a single, focused product was an excellent opportunity to apply my skills to a real-world project and solidify my understanding of creating robust, scalable web applications.\n","permalink":"http://localhost:1313/experience/blackbox/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eBuilding on my foundational experience, my internship at BlackBox Technologies immersed me in a more complex, project-based environment. I was an integral part of a development team tasked with building a web-based attendance system from the ground up. This role provided me with invaluable hands-on, full-stack experience. I contributed to the backend by assisting senior engineers with the development of business logic in .NET, giving me insight into server-side architecture. Simultaneously, I was responsible for building responsive, user-facing components for the front-end using HTML, CSS, and JavaScript.\u003c/p\u003e","title":"Web Development Intern"},{"content":"Description My journey into professional software development began at Radiant Infotech, my first internship and job in the tech industry. This role was a pivotal transition from academic theory to real-world application. I was entrusted with supporting the full lifecycle of client websites, which provided an immersive learning experience. My primary responsibility was to develop responsive, pixel-perfect front-end interfaces using HTML, CSS, and Bootstrap, translating design files into functional web components. A key part of this process was using Adobe Photoshop to prepare and optimize web graphics, ensuring both aesthetic quality and optimal performance.\nBeyond the initial development, my role extended to managing website content through various CMS platforms and performing rigorous debugging to ensure cross-browser compatibility and a seamless user experience. This foundational internship was crucial in building my confidence and skills in modern web development, teaching me how to collaborate effectively within a team to deliver high-quality digital products for clients.\n","permalink":"http://localhost:1313/experience/radiantinfotech/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eMy journey into professional software development began at Radiant Infotech, my first internship and job in the tech industry. This role was a pivotal transition from academic theory to real-world application. I was entrusted with supporting the full lifecycle of client websites, which provided an immersive learning experience. My primary responsibility was to develop responsive, pixel-perfect front-end interfaces using HTML, CSS, and Bootstrap, translating design files into functional web components. A key part of this process was using Adobe Photoshop to prepare and optimize web graphics, ensuring both aesthetic quality and optimal performance.\u003c/p\u003e","title":"Web Development Intern"},{"content":"Introduction Welcome to my personal portfolio website. I respect your privacy and am committed to protecting it. This policy outlines what information is collected when you visit my site and how that information is used.\nInformation Collection and Use I collect information in two ways: information you provide directly and anonymous data collected by analytics services.\nPersonal Data You Provide When you request a copy of my resume, you are asked to voluntarily provide your email address.\nHow it\u0026rsquo;s collected: This information is collected via an embedded Google Form. Why it\u0026rsquo;s collected: It is used for the sole purpose of sending the requested resume document to you through an automated process managed by Google Apps Script. How it\u0026rsquo;s used: Your email will not be used for marketing purposes, sold, or shared with any third parties. Anonymous Usage Data To improve the user experience and analyze traffic, this website uses the following third-party services:\nUmami (Self-Hosted): This website uses a self-hosted instance of Umami for privacy-focused web analytics. Umami collects anonymous usage data such as page views, referrers, and geographic regions to help me understand website traffic. This service does not use cookies, does not collect any personally identifiable information, and all data is stored on a private server under my control.\nCloudflare Web Analytics: This service collects anonymous traffic data such as page views and country of origin. It does not use cookies or collect personally identifiable information. You can view their privacy policy here.\nService Providers This website relies on the following third-party service providers to function:\nGoogle Workspace (Forms, Sheets, Apps Script): Used to manage and automate resume requests. Cloudflare: Used as a Content Delivery Network (CDN) to improve website performance, as a security firewall to protect against malicious attacks, and for collecting anonymous web analytics. Vercel: Used to host the self-hosted Umami analytics application. Netlify \u0026amp; GitHub: Used for hosting and deploying the website. Changes to This Privacy Policy I may update this Privacy Policy from time to time. I will notify you of any changes by posting the new Privacy Policy on this page. You are advised to review this page periodically for any changes.\nContact Me If you have any questions about this Privacy Policy, please contact me at: prajwolad18@gmail.com\n","permalink":"http://localhost:1313/privacy-policy/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWelcome to my personal portfolio website. I respect your privacy and am committed to protecting it. This policy outlines what information is collected when you visit my site and how that information is used.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"information-collection-and-use\"\u003eInformation Collection and Use\u003c/h2\u003e\n\u003cp\u003eI collect information in two ways: information you provide directly and anonymous data collected by analytics services.\u003c/p\u003e\n\u003ch3 id=\"personal-data-you-provide\"\u003ePersonal Data You Provide\u003c/h3\u003e\n\u003cp\u003eWhen you request a copy of my resume, you are asked to voluntarily provide your email address.\u003c/p\u003e","title":"Privacy Policy"}]